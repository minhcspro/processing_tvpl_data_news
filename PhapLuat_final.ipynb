{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "import json\n",
    "from bs4 import BeautifulSoup, NavigableString,Tag\n",
    "from tabulate import tabulate\n",
    "import unicodedata\n",
    "import time\n",
    "import requests\n",
    "from markdownify import markdownify as md\n",
    "import base64\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test ccpl mới"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format các dạng bảng\n",
    "def table_to_markdown(table):\n",
    "    if table is None:\n",
    "        return \"\"\n",
    "    rows = []\n",
    "    headers = []\n",
    "\n",
    "    for row in table.find_all('tr'):\n",
    "        row_data = []\n",
    "        for cell in row.find_all(['th', 'td']):\n",
    "            cell_text = cell.get_text(strip=True)\n",
    "            row_data.append(cell_text)\n",
    "        if not headers:\n",
    "            headers = row_data\n",
    "        else:\n",
    "            rows.append(row_data)\n",
    "\n",
    "    return tabulate(rows, headers, tablefmt='pipe', disable_numparse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ Điều 1, 2, 3 + replace\n",
    "import re\n",
    "\n",
    "def split_by_dieu_new1(text):\n",
    "    # def extract_dieu_positions(text):\n",
    "    if text is None:\n",
    "        return[text]\n",
    "\n",
    "        \n",
    "    dieu_positions = [(match.start(), match.end()) for match in re.finditer(r'Điều\\s+\\d+[a-zA-Z0-9đĐ]*(?:,\\s*\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*)*(?:\\s+và\\s+\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*)?', text, re.IGNORECASE)]\n",
    "    if not dieu_positions:\n",
    "        return [text]\n",
    "\n",
    "    split_text = []\n",
    "    current_position = 0\n",
    "\n",
    "    for start, end in dieu_positions:\n",
    "        if current_position < start:\n",
    "            split_text.append(text[current_position:start].strip())\n",
    "        split_text.append(text[start:end].strip())\n",
    "        current_position = end\n",
    "\n",
    "    if current_position < len(text):\n",
    "        split_text.append(text[current_position:].strip())\n",
    "\n",
    "    return split_text\n",
    "################\n",
    "def extract_dieu_err(text):\n",
    "    # Define the pattern to match \"Điều\" followed by numbers and possibly letters or other characters\n",
    "    pattern = r'Điều\\s+\\d+[a-zA-Z0-9đĐ]*(?:,\\s*\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*)*(?:\\s+và\\s+\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*)?'\n",
    "\n",
    "    # Find all matches of the pattern in the text\n",
    "    matches = list(re.finditer(pattern, text, re.IGNORECASE))\n",
    "    matched_phrases = [match.group(0) for match in matches]\n",
    "    check_phrases = []\n",
    "\n",
    "    # Loop through each phrase in matched_phrases\n",
    "    for phrase in matched_phrases:\n",
    "        if ',' in phrase or ' và ' in phrase:\n",
    "            check_phrases.append(phrase)\n",
    "    return check_phrases\n",
    "##############\n",
    "def process_dieu_parts(text):\n",
    "    dieu_final = []\n",
    "    for match in text:\n",
    "        if ',' in match or ' và ' in match:\n",
    "            splitted = re.split(r',| và ', match)\n",
    "            for temp in splitted:\n",
    "                if re.match(r'(?i)Điều\\s+\\d+', temp):\n",
    "                    dieu_final.append(temp)\n",
    "                else:\n",
    "                    dieu_final.append('Điều ' + temp)\n",
    "\n",
    "    return dieu_final\n",
    "###############\n",
    "def replace_dieu_in_text(text):\n",
    "    if text is None:\n",
    "        return[text]\n",
    "    split_text = split_by_dieu_new1(text)\n",
    "    matches = extract_dieu_err(text)\n",
    "    if not matches:\n",
    "        return text\n",
    "    dieu_final = process_dieu_parts(matches)\n",
    "    replaced_text = []\n",
    "    for part in split_text:\n",
    "        if re.match(r'(?i)Điều\\s+\\d+', part) and \" và \" in part:\n",
    "            for dieu in dieu_final:\n",
    "                replaced_text.append(dieu)\n",
    "        else:\n",
    "            replaced_text.append(part)\n",
    "\n",
    "    return ' '.join(replaced_text)\n",
    "\n",
    "#############\n",
    "#############\n",
    "########### khoản 1, 2,3  và .... + replace\n",
    "\n",
    "\n",
    "import re\n",
    "\n",
    "def split_by_khoan_new1(text):\n",
    "    if text is None:\n",
    "        return[text]       \n",
    "    khoan_positions = [(match.start(), match.end()) for match in re.finditer(r'khoản\\s+((?:[a-zA-Z0-9đĐ]+(?:\\.\\d+)?(?:,\\s*| và |, và |))*[a-zA-Z0-9đĐ]+(?:\\.\\d+)?)(?=\\s|,|\\.|;|$)', text, re.IGNORECASE)]\n",
    "    if not khoan_positions:\n",
    "        return [text]\n",
    "\n",
    "    split_text = []\n",
    "    current_position = 0\n",
    "\n",
    "    for start, end in khoan_positions:\n",
    "        if current_position < start:\n",
    "            split_text.append(text[current_position:start].strip())\n",
    "        split_text.append(text[start:end].strip())\n",
    "        current_position = end\n",
    "\n",
    "    if current_position < len(text):\n",
    "        split_text.append(text[current_position:].strip())\n",
    "\n",
    "    return split_text\n",
    "\n",
    "#####################\n",
    "def extract_khoan_err(text):\n",
    "    # Define the pattern to match \"khoản\" followed by numbers and possibly letters or other characters\n",
    "    pattern = r'khoản\\s+((?:[a-zA-Z0-9đĐ]+(?:\\.\\d+)?(?:,\\s*| và |, và |))*[a-zA-Z0-9đĐ]+(?:\\.\\d+)?)(?=\\s|,|\\.|;|$)'\n",
    "\n",
    "    # Find all matches of the pattern in the text\n",
    "    matches = list(re.finditer(pattern, text, re.IGNORECASE))\n",
    "    matched_phrases = [match.group(0) for match in matches]\n",
    "    check_phrases = []\n",
    "\n",
    "    # Loop through each phrase in matched_phrases\n",
    "    for phrase in matched_phrases:\n",
    "        if ',' in phrase or ' và ' in phrase:\n",
    "            check_phrases.append(phrase)\n",
    "    return check_phrases\n",
    "################\n",
    "def process_khoan_parts(text):\n",
    "    khoan_final = []\n",
    "    for match in text:\n",
    "        if ',' in match or ' và ' in match:\n",
    "            splitted = re.split(r',| và ', match)\n",
    "            for temp in splitted:\n",
    "                if re.match(r'(?i)khoản\\s+((?:[a-zA-Z0-9đĐ]+(?:\\.\\d+)?)+)', temp):\n",
    "                    khoan_final.append(temp)\n",
    "                else:\n",
    "                    khoan_final.append('khoản ' + temp)\n",
    "\n",
    "    return khoan_final\n",
    "###############\n",
    "\n",
    "def replace_khoan_in_text(text):\n",
    "    if text is None:\n",
    "        return[text]\n",
    "    split_text = split_by_khoan_new1(text)\n",
    "    matches = extract_khoan_err(text)\n",
    "    if not matches:\n",
    "        return text\n",
    "    khoan_final = process_khoan_parts(matches)\n",
    "    replaced_text = []\n",
    "    for part in split_text:\n",
    "        if re.match(r'(?i)khoản\\s+((?:[a-zA-Z0-9đĐ]+(?:\\.\\d+)?)+)', part) and \" và \" in part:\n",
    "            for khoan in khoan_final:\n",
    "                replaced_text.append(khoan)\n",
    "        else:\n",
    "            replaced_text.append(part)\n",
    "\n",
    "    return ' '.join(replaced_text)\n",
    "#############\n",
    "#############\n",
    "################# Điểm 1,2,3 và .... + replace\n",
    "import re\n",
    "\n",
    "def split_by_diem_new1(text):\n",
    "    def extract_diem_positions(text):\n",
    "        return [(match.start(), match.end()) for match in re.finditer(r'điểm\\s+((?:[a-zA-Z0-9đĐ]+(?:\\.\\d+)?(?:,\\s+| và |, và |))*[a-zA-Z0-9đĐ]+(?:\\.\\d+)?)(?=\\s|,|\\.|;|$|\\s+và\\s+[a-zA-ZđĐ0-9])', text, re.IGNORECASE)]\n",
    "        \n",
    "    diem_positions = extract_diem_positions(text)\n",
    "    if not diem_positions:\n",
    "        return [text]\n",
    "\n",
    "    split_text = []\n",
    "    current_position = 0\n",
    "\n",
    "    for start, end in diem_positions:\n",
    "        if current_position < start:\n",
    "            split_text.append(text[current_position:start].strip())\n",
    "        split_text.append(text[start:end].strip())\n",
    "        current_position = end\n",
    "\n",
    "    if current_position < len(text):\n",
    "        split_text.append(text[current_position:].strip())\n",
    "\n",
    "    return split_text\n",
    "###########################\n",
    "def extract_diem_err(text):\n",
    "    # Define the pattern to match \"điểm\" followed by numbers and possibly letters or other characters\n",
    "    pattern = r'điểm\\s+((?:[a-zA-Z0-9đĐ]+(?:\\.\\d+)?(?:,\\s+| và |, và |))*[a-zA-Z0-9đĐ]+(?:\\.\\d+)?)(?=\\s|,|\\.|;|$|\\s+và\\s+[a-zA-ZđĐ0-9])'\n",
    "\n",
    "    # Find all matches of the pattern in the text\n",
    "    matches = list(re.finditer(pattern, text, re.IGNORECASE))\n",
    "    matched_phrases = [match.group(0) for match in matches]\n",
    "    check_phrases = []\n",
    "\n",
    "    # Loop through each phrase in matched_phrases\n",
    "    for phrase in matched_phrases:\n",
    "        if ',' in phrase or ' và ' in phrase:\n",
    "            check_phrases.append(phrase)\n",
    "    return check_phrases\n",
    "#########################\n",
    "def process_diem_parts(text):\n",
    "    diem_final = []\n",
    "    for match in text:\n",
    "        if ',' in match or ' và ' in match:\n",
    "            splitted = re.split(r',| và ', match)\n",
    "            for temp in splitted:\n",
    "                if re.match(r'(?i)điểm\\s+((?:[a-zA-Z0-9đĐ]+(?:\\.\\d+)?)+)', temp):\n",
    "                    diem_final.append(temp)\n",
    "                else:\n",
    "                    diem_final.append('điểm ' + temp)\n",
    "\n",
    "    return diem_final\n",
    "#########################\n",
    "def replace_diem_in_text(text):\n",
    "    if text is None:\n",
    "        return[text]\n",
    "    split_text = split_by_diem_new1(text)\n",
    "    matches = extract_diem_err(text)\n",
    "    if not matches:\n",
    "        return text\n",
    "    diem_final = process_diem_parts(matches)\n",
    "    replaced_text = []\n",
    "    for part in split_text:\n",
    "        if re.match(r'(?i)điểm\\s+((?:[a-zA-Z0-9đĐ]+(?:\\.\\d+)?)+)', part) and \" và \" in part:\n",
    "            for diem in diem_final:\n",
    "                replaced_text.append(diem)\n",
    "        else:\n",
    "            replaced_text.append(part)\n",
    "\n",
    "    return ' '.join(replaced_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_and_transform_dieu_ranges(text):\n",
    "    if text is None:\n",
    "        return [text]\n",
    "\n",
    "    dieu_positions = [(match.start(), match.end()) for match in re.finditer(r'Điều\\s+\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*', text, re.IGNORECASE)]\n",
    "\n",
    "    if not dieu_positions:\n",
    "        return [text]\n",
    "\n",
    "    new_split_text = []\n",
    "    current_position = 0\n",
    "    start_dieu = None\n",
    "\n",
    "    for i, (start, end) in enumerate(dieu_positions):\n",
    "        if current_position < start:\n",
    "            split_text_part = text[current_position:end].strip()\n",
    "            if \"từ Điều\" in split_text_part:\n",
    "                start_dieu_match = re.search(r'Điều\\s+(\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*)', split_text_part, re.IGNORECASE)\n",
    "                if start_dieu_match:\n",
    "                    start_dieu = start_dieu_match.group(1)\n",
    "            elif \"đến Điều\" in split_text_part or \"tới Điều\" in split_text_part:\n",
    "                end_dieu_match = re.search(r'Điều\\s+(\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*)', split_text_part, re.IGNORECASE)\n",
    "                if end_dieu_match:\n",
    "                    end_dieu = end_dieu_match.group(1)\n",
    "                    start_index = int(re.sub(\"[a-zA-ZđĐ]\", \"\", start_dieu)) if start_dieu else None\n",
    "                    end_index = int(re.sub(\"[a-zA-ZđĐ]\", \"\", end_dieu)) if end_dieu else None\n",
    "                    if start_index is not None and end_index is not None:\n",
    "                        for d in range(start_index, end_index + 1):\n",
    "                            new_split_text.append(f\"Điều {d}\")\n",
    "            else:\n",
    "                new_split_text.append(split_text_part)\n",
    "\n",
    "        current_position = end\n",
    "\n",
    "    if current_position < len(text):\n",
    "        split_text_part = text[current_position:].strip()\n",
    "        if \"từ Điều\" in split_text_part:\n",
    "            start_dieu_match = re.search(r'Điều\\s+(\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*)', split_text_part, re.IGNORECASE)\n",
    "            if start_dieu_match:\n",
    "                start_dieu = start_dieu_match.group(1)\n",
    "        elif \"đến Điều\" in split_text_part or \"tới Điều\" in split_text_part:\n",
    "            end_dieu_match = re.search(r'Điều\\s+(\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*)', split_text_part, re.IGNORECASE)\n",
    "            if end_dieu_match:\n",
    "                end_dieu = end_dieu_match.group(1)\n",
    "                start_index = int(re.sub(\"[a-zA-ZđĐ]\", \"\", start_dieu)) if start_dieu else None\n",
    "                end_index = int(re.sub(\"[a-zA-ZđĐ]\", \"\", end_dieu)) if end_dieu else None\n",
    "                if start_index is not None and end_index is not None:\n",
    "                    for d in range(start_index, end_index + 1):\n",
    "                        new_split_text.append(f\"Điều {d}\")\n",
    "        else:\n",
    "            new_split_text.append(split_text_part)\n",
    "\n",
    "    return new_split_text\n",
    "\n",
    "def replace_dieu_ranges(text):\n",
    "    # Tìm khoảng \"từ Điều X đến Điều Y\" hoặc \"từ Điều X tới Điều Y\"\n",
    "    if text is None:\n",
    "        return[text]\n",
    "    dieu_range_match = re.search(r'từ Điều \\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)* (?:đến|tới) Điều \\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*', text, re.IGNORECASE)\n",
    "    if not dieu_range_match:\n",
    "        return text\n",
    "\n",
    "    dieu_range_text = dieu_range_match.group(0)\n",
    "    dieu_list = split_and_transform_dieu_ranges(dieu_range_text)\n",
    "    \n",
    "    # Tạo chuỗi mới từ danh sách các Điều\n",
    "    dieu_list_str = ', '.join(dieu_list)\n",
    "\n",
    "    # Thay thế đoạn văn bản \"từ Điều X đến Điều Y\" hoặc \"từ Điều X tới Điều Y\" bằng danh sách các Điều\n",
    "    new_text = text.replace(dieu_range_text, dieu_list_str)\n",
    "\n",
    "    return new_text\n",
    "\n",
    "def split_and_transform_khoan_ranges(text):\n",
    "    if text is None:\n",
    "        return [text]\n",
    "        \n",
    "\n",
    "    khoan_positions = [(match.start(), match.end()) for match in re.finditer(r'khoản\\s+\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*', text, re.IGNORECASE)]\n",
    "    if not khoan_positions:\n",
    "        return [text]\n",
    "\n",
    "    new_split_text = []\n",
    "    current_position = 0\n",
    "    start_khoan = None\n",
    "\n",
    "    for i, (start, end) in enumerate(khoan_positions):\n",
    "        if current_position < start:\n",
    "            split_text_part = text[current_position:end].strip()\n",
    "            if \"từ khoản\" in split_text_part.lower():\n",
    "                start_khoan = re.search(r'khoản\\s+(\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*)', split_text_part, re.IGNORECASE).group(1)\n",
    "            elif \"đến khoản\" in split_text_part.lower() or \"tới khoản\" in split_text_part.lower():\n",
    "                end_khoan = re.search(r'khoản\\s+(\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*)', split_text_part, re.IGNORECASE).group(1)\n",
    "                start_index = int(re.sub(\"[a-zA-ZđĐ]\", \"\", start_khoan))\n",
    "                end_index = int(re.sub(\"[a-zA-ZđĐ]\", \"\", end_khoan))\n",
    "                for d in range(start_index, end_index + 1):\n",
    "                    new_split_text.append(f\"khoản {d}\")\n",
    "            else:\n",
    "                new_split_text.append(split_text_part)\n",
    "        current_position = end\n",
    "\n",
    "    if current_position < len(text):\n",
    "        split_text_part = text[current_position:].strip()\n",
    "        if \"từ khoản\" in split_text_part.lower():\n",
    "            start_khoan = re.search(r'khoản\\s+(\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*)', split_text_part, re.IGNORECASE).group(1)\n",
    "        elif \"đến khoản\" in split_text_part.lower() or \"tới khoản\" in split_text_part.lower():\n",
    "            end_khoan = re.search(r'khoản\\s+(\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*)', split_text_part, re.IGNORECASE).group(1)\n",
    "            start_index = int(re.sub(\"[a-zA-ZđĐ]\", \"\", start_khoan))\n",
    "            end_index = int(re.sub(\"[a-zA-ZđĐ]\", \"\", end_khoan))\n",
    "            for d in range(start_index, end_index + 1):\n",
    "                new_split_text.append(f\"khoản {d}\")\n",
    "        else:\n",
    "            new_split_text.append(split_text_part)\n",
    "\n",
    "    return new_split_text\n",
    "\n",
    "def replace_khoan_ranges(text):\n",
    "    # Tìm khoảng \"từ khoản X đến khoản Y\" hoặc \"từ khoản X tới khoản Y\"\n",
    "    if text is None:\n",
    "        return[text]\n",
    "    khoan_range_matches = re.finditer(r'từ khoản \\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)* (?:đến|tới) khoản \\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*', text, re.IGNORECASE)\n",
    "    \n",
    "    new_text = text\n",
    "    offset = 0\n",
    "    \n",
    "    for match in khoan_range_matches:\n",
    "        khoan_range_text = match.group(0)\n",
    "        khoan_list = split_and_transform_khoan_ranges(khoan_range_text)\n",
    "        \n",
    "        # Tạo chuỗi mới từ danh sách các khoản\n",
    "        khoan_list_str = ', '.join(khoan_list)\n",
    "\n",
    "        # Tính toán vị trí mới sau khi thay thế\n",
    "        start, end = match.start() + offset, match.end() + offset\n",
    "        new_text = new_text[:start] + khoan_list_str + new_text[end:]\n",
    "        \n",
    "        # Cập nhật offset\n",
    "        offset += len(khoan_list_str) - len(khoan_range_text)\n",
    "\n",
    "    return new_text\n",
    "\n",
    "def split_and_transform_diem_ranges(text):\n",
    "    if text is None:\n",
    "        return [text]\n",
    "        \n",
    "\n",
    "    diem_positions = [(match.start(), match.end()) for match in re.finditer(r'điểm\\s+\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*', text, re.IGNORECASE)]\n",
    "    if not diem_positions:\n",
    "        return [text]\n",
    "\n",
    "    new_split_text = []\n",
    "    current_position = 0\n",
    "    start_diem = None\n",
    "\n",
    "    for i, (start, end) in enumerate(diem_positions):\n",
    "        if current_position < start:\n",
    "            split_text_part = text[current_position:end].strip()\n",
    "            if \"từ điểm\" in split_text_part.lower():\n",
    "                start_diem = re.search(r'điểm\\s+(\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*)', split_text_part, re.IGNORECASE).group(1)\n",
    "            elif \"đến điểm\" in split_text_part.lower() or \"tới điểm\" in split_text_part.lower():\n",
    "                end_diem = re.search(r'điểm\\s+(\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*)', split_text_part, re.IGNORECASE).group(1)\n",
    "                start_index = int(re.sub(\"[a-zA-ZđĐ]\", \"\", start_diem))\n",
    "                end_index = int(re.sub(\"[a-zA-ZđĐ]\", \"\", end_diem))\n",
    "                for d in range(start_index, end_index + 1):\n",
    "                    new_split_text.append(f\"điểm {d}\")\n",
    "            else:\n",
    "                new_split_text.append(split_text_part)\n",
    "        current_position = end\n",
    "\n",
    "    if current_position < len(text):\n",
    "        split_text_part = text[current_position:].strip()\n",
    "        if \"từ điểm\" in split_text_part.lower():\n",
    "            start_diem = re.search(r'điểm\\s+(\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*)', split_text_part, re.IGNORECASE).group(1)\n",
    "        elif \"đến điểm\" in split_text_part.lower() or \"tới điểm\" in split_text_part.lower():\n",
    "            end_diem = re.search(r'điểm\\s+(\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*)', split_text_part, re.IGNORECASE).group(1)\n",
    "            start_index = int(re.sub(\"[a-zA-ZđĐ]\", \"\", start_diem))\n",
    "            end_index = int(re.sub(\"[a-zA-ZđĐ]\", \"\", end_diem))\n",
    "            for d in range(start_index, end_index + 1):\n",
    "                new_split_text.append(f\"điểm {d}\")\n",
    "        else:\n",
    "            new_split_text.append(split_text_part)\n",
    "\n",
    "    return new_split_text\n",
    "\n",
    "def replace_diem_ranges(text):\n",
    "    # Tìm khoảng \"từ điểm X đến điểm Y\" hoặc \"từ điểm X tới điểm Y\"\n",
    "    if text is None:\n",
    "        return[text]\n",
    "    diem_range_matches = re.finditer(r'từ điểm \\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)* (?:đến|tới) điểm \\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*', text, re.IGNORECASE)\n",
    "    \n",
    "    new_text = text\n",
    "    offset = 0\n",
    "    \n",
    "    for match in diem_range_matches:\n",
    "        diem_range_text = match.group(0)\n",
    "        diem_list = split_and_transform_diem_ranges(diem_range_text)\n",
    "        \n",
    "        # Tạo chuỗi mới từ danh sách các điểm\n",
    "        diem_list_str = ', '.join(diem_list)\n",
    "\n",
    "        # Tính toán vị trí mới sau khi thay thế\n",
    "        start, end = match.start() + offset, match.end() + offset\n",
    "        new_text = new_text[:start] + diem_list_str + new_text[end:]\n",
    "        \n",
    "        # Cập nhật offset\n",
    "        offset += len(diem_list_str) - len(diem_range_text)\n",
    "\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## dạng ..... 1, 2, 3 và .....,......\n",
    "def processing_ccpl_full(text):\n",
    "    if text is None:\n",
    "        return [text]  \n",
    "    check = replace_dieu_ranges(replace_khoan_ranges(replace_diem_ranges(text)))\n",
    "    check2 = replace_dieu_in_text(replace_khoan_in_text(replace_diem_in_text(check)))\n",
    "\n",
    "    return check2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Cắt từ điều này tới điều kia\n",
    "\n",
    "\n",
    "def split_by_dieu(text):\n",
    "    if text is None: \n",
    "        return[text]\n",
    "    \n",
    "    text = ' ' + text\n",
    "    dieu_positions = [(match.start(), match.end()) for match in re.finditer(r'Điều\\s+\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*', text, re.IGNORECASE)]\n",
    "    if not dieu_positions:\n",
    "        return [text]\n",
    "\n",
    "    split_text = []\n",
    "    current_position = 0\n",
    "    \n",
    "    for i, (start, end) in enumerate(dieu_positions):\n",
    "        if current_position < start:\n",
    "            split_text.append(text[current_position:end].strip())\n",
    "        current_position = end\n",
    "\n",
    "    if current_position < len(text):\n",
    "        split_text.append(text[current_position:].strip())\n",
    "\n",
    "    return split_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Cắt từ khoản này tới khoản kia\n",
    "\n",
    "def split_by_khoan(text):\n",
    "    if text is None:\n",
    "            return[text]\n",
    "\n",
    "    text = ' ' + text\n",
    "    khoan_positions = [(match.start(), match.end()) for match in re.finditer(r'khoản\\s+((?:[a-zA-Z0-9đĐ]+(?:\\.\\d+)?(?:,\\s*| và |, và |))*[a-zA-Z0-9đĐ]+(?:\\.\\d+)?)(?=\\s|,|\\.|;|$)', text, re.IGNORECASE)]\n",
    "    if not khoan_positions:\n",
    "        return [text]\n",
    "\n",
    "    split_text = []\n",
    "    current_position = 0\n",
    "\n",
    "    first_khoan_start = khoan_positions[0][0]\n",
    "    if first_khoan_start > 0:\n",
    "        split_text.append(text[:first_khoan_start].strip())\n",
    "\n",
    "    for i, (start, end) in enumerate(khoan_positions):\n",
    "        if current_position < start:\n",
    "            split_text.append(text[current_position:end].strip())\n",
    "        current_position = end\n",
    "\n",
    "    if current_position < len(text):\n",
    "        split_text.append(text[current_position:].strip())\n",
    "\n",
    "    return split_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hàm trích xuất điều\n",
    "def extract_dieu(text):\n",
    "    # Find individual \"Điều\" references\n",
    "    dieu_list = re.findall(r'Điều\\s+(\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*)', text, re.IGNORECASE)\n",
    "\n",
    "    # Find joined \"Điều\" lists\n",
    "    joined_dieu_list = re.findall(r'Điều\\s+\\d+[a-zA-Z0-9đĐ]*(?:,\\s*\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*)*(?:\\s+và\\s+\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*)?', text, re.IGNORECASE)\n",
    "\n",
    "    # Process joined \"Điều\" lists\n",
    "    for item in joined_dieu_list:\n",
    "        # Find all \"Điều\" references in the joined list\n",
    "        joined_dieu_refs = re.findall(r'\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*', item)\n",
    "        # Add only the unique \"Điều\" references to the main list\n",
    "        for ref in joined_dieu_refs:\n",
    "            if ref not in dieu_list:\n",
    "                dieu_list.append(ref)\n",
    "    return dieu_list\n",
    "\n",
    "# Hàm trích xuất khoản\n",
    "def extract_khoan(text):\n",
    "    pattern = r'khoản\\s+((?:[a-zA-Z0-9đĐ]+(?:\\.\\d+)?(?:,\\s*| và |, và |))*[a-zA-Z0-9đĐ]+(?:\\.\\d+)?)(?=\\s|,|\\.|;|$)'\n",
    "    matches = re.findall(pattern, text, re.IGNORECASE)\n",
    "    final_khoan_list = []\n",
    "    for match in matches:\n",
    "        parts = re.split(r',\\s*|\\s+và\\s+', match)\n",
    "        for part in parts:\n",
    "            part = part.strip().strip('.').lower()\n",
    "            if '-' in part:\n",
    "                start, end = part.split('-')\n",
    "                for idx in range(int(start), int(end) + 1):\n",
    "                    final_khoan_list.append(str(idx))\n",
    "            elif len(part) == 1 and part.isalpha() or part[0].isdigit():\n",
    "                final_khoan_list.append(part)\n",
    "    return final_khoan_list\n",
    "\n",
    "# Hàm trích xuất điểm\n",
    "def extract_diem(text):\n",
    "    pattern = r'điểm\\s+((?:[a-zA-Z0-9đĐ]+(?:\\.\\d+)?(?:,\\s+| và |, và |))*[a-zA-Z0-9đĐ]+(?:\\.\\d+)?)(?=\\s|,|\\.|;|$|\\s+và\\s+[a-zA-ZđĐ0-9])'\n",
    "    matches = re.findall(pattern, text, re.IGNORECASE)\n",
    "    final_diem_list = []\n",
    "    for match in matches:\n",
    "        parts = re.split(r',\\s*|\\s+và\\s+', match)\n",
    "        for part in parts:\n",
    "            part = part.strip().strip('.').lower()\n",
    "            if re.match(r'^[a-zA-ZđĐ]\\d*(\\.\\d+)*$', part) or re.match(r'^\\d+(\\.\\d+)*$', part):\n",
    "                final_diem_list.append(part)\n",
    "    return final_diem_list\n",
    "\n",
    "# hàm xử lý đầu ra ccpl\n",
    "def processing_output_ccpl(input_data):\n",
    "    output_array = []\n",
    "    for dieu in input_data:\n",
    "        dieu_value = \", \".join(dieu[\"Dieu\"])\n",
    "        if dieu[\"Khoan\"] == [[]]:\n",
    "            output_array.extend([\n",
    "                    dieu_value,\n",
    "                    \"0\",\n",
    "                    \"0\"\n",
    "                ])\n",
    "        elif dieu[\"Khoan\"]:\n",
    "            for khoan in dieu[\"Khoan\"]:\n",
    "                for khoan_item in khoan[\"Khoan\"]:\n",
    "                    if khoan[\"Diem\"] == [[]]:\n",
    "                        output_array.extend([\n",
    "                            dieu_value,\n",
    "                            khoan_item,\n",
    "                            \"0\"\n",
    "                        ])\n",
    "                    elif khoan[\"Diem\"]:\n",
    "                        for diem in khoan[\"Diem\"]:\n",
    "                            for diem_item in diem:\n",
    "                                output_array.extend([\n",
    "                                    dieu_value,\n",
    "                                    khoan_item,\n",
    "                                    diem_item\n",
    "                                ])\n",
    "                                # print(output_array)\n",
    "                    else:\n",
    "                        output_array.extend([\n",
    "                            dieu_value,\n",
    "                            khoan_item,\n",
    "                            \"0\"\n",
    "                        ])\n",
    "        else:\n",
    "            output_array.extend([\n",
    "                dieu_value,\n",
    "                \"0\",\n",
    "                \"0\"\n",
    "            ])\n",
    "    if output_array:\n",
    "        return output_array\n",
    "\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "# Function to remove duplicate ccpl entries\n",
    "def remove_duplicates_ccpls(data):\n",
    "    for item in data:\n",
    "        seen = set()\n",
    "        unique_ccpls = []\n",
    "        for ccpl in item['ccpls']:\n",
    "            if ccpl['Dieu']:  # Kiểm tra nếu 'Dieu' không phải là rỗng\n",
    "                ccpl_tuple = (ccpl['LawID'], ccpl['LawTitle'], ccpl['Dieu'], ccpl['Khoan'], ccpl['Diem'])\n",
    "                if ccpl_tuple not in seen:\n",
    "                    seen.add(ccpl_tuple)\n",
    "                    unique_ccpls.append(ccpl)\n",
    "        item['ccpls'] = unique_ccpls\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processing_redundant_data(data):\n",
    "    soup = BeautifulSoup(data, 'html.parser')\n",
    "\n",
    "    # Xóa dẫn link dạng bảng có chữ tải về\n",
    "    for p_tag in soup.find_all('table'):\n",
    "        try:\n",
    "            a_tag = p_tag.find('a')\n",
    "            # Tìm thẻ <img> bên trong thẻ <a>\n",
    "            img_tag = a_tag.find('img')\n",
    "            # Kiểm tra href và src có tồn tại không trước khi kiểm tra endswith\n",
    "            if (a_tag.get('href') and not a_tag.get('href').endswith(('.doc', '.docx', '.xls', '.xlsx', '.zip', '.rar', '.jpg', '.pdf', '.png', '.jpeg'))) and (img_tag and img_tag.get('src') and img_tag.get('src').endswith(('png', 'jpg', 'jpeg'))): \n",
    "                # Xử lý thẻ <p> nếu điều kiện thỏa mãn\n",
    "                # print(p_tag.text)\n",
    "                p_tag.decompose()\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing tag: {e}\")\n",
    "    \n",
    "    # Tìm tất cả các thẻ <p>\n",
    "    p_tags = soup.find_all('p')\n",
    "    # Kiểm tra từng thẻ <p> để xem có chứa chuỗi \"lưu ý: khi thực hiện cải cách tiền lương...\" không\n",
    "    for p_tag in p_tags:\n",
    "        if \"lưu ý: khi thực hiện cải cách tiền lương từ ngày 1/7/2024 sẽ bãi bỏ mức lương cơ sở và hệ số lương hiện nay, xây dựng bảng lương mới, theo mục 2 nghị quyết 27-nq/tw năm 2018\" in p_tag.get_text().lower():\n",
    "            p_tag.decompose()\n",
    "        if \"theo điều 3, điều 4 thông tư 29/2012/tt- bgdđt đã hết hiệu lực thì chỉ có 02 hình thức tài trợ đó là tiếp nhận tài trợ bằng tiền mặt hoặc tiếp nhận tài trợ bằng hiện vật. còn hiện nay tại khoản 3 điều 4 thông tư 16/2018/tt-bgdđt có thêm hình thức tài trợ phi vật chất\" in p_tag.get_text().lower():\n",
    "            p_tag.decompose()\n",
    "        if p_tag.get_text().lower().endswith('.html') or p_tag.get_text().lower().endswith('.htm'):\n",
    "            p_tag.decompose()\n",
    "        img = p_tag.find('img')\n",
    "        # Kiểm tra nếu thẻ img có thuộc tính style đúng như yêu cầu\n",
    "        if img and img.get('style') == 'height:400px; width:600px':\n",
    "            p_tag.decompose()  # Xóa thẻ <p> nếu điều kiện đúng\n",
    "            \n",
    "    # Tìm và xóa thẻ <img> có đuôi .gif\n",
    "    for img_tag in soup.select('p img'):\n",
    "        try:\n",
    "            if re.search(r'\\.gif$', img_tag['src']):\n",
    "                img_tag.decompose()  # Xóa thẻ <img> khỏi cây HTML\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing tag: {e}\")   \n",
    "    # for p_tag in soup.find_all('p', style=\"text-align:center\"):\n",
    "    #     for em_tag in p_tag.find_all('em'):\n",
    "    #         p_tag.decompose()\n",
    "                    \n",
    "    # xóa ảnh thumb và title ảnh\n",
    "    # Lặp qua tất cả các thẻ <p> để kiểm tra điều kiện\n",
    "    for i, p in enumerate(p_tags):\n",
    "        # Kiểm tra nếu thẻ <p> chứa <img> và thẻ <p> tiếp theo có style \"text-align: center\"\n",
    "        try:\n",
    "            if p.find('img') and i + 1 < len(p_tags) and ('text-align: center' in p_tags[i + 1].get('style', '') or p_tags[i + 1].get_text().lower().endswith('internet)')):\n",
    "                # or p_tags[i + 1].find('em')\n",
    "                # Xóa cả hai thẻ <p> hiện tại và thẻ <p> tiếp theo\n",
    "                p.decompose()\n",
    "                p_tags[i + 1].decompose()\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing tag: {e}\")\n",
    "            \n",
    "    # xóa nội dung bảng hết hiệu lực\n",
    "    for table_tag in soup.find_all('s'):\n",
    "        # print(table_tag.text)\n",
    "        table_tag.decompose()\n",
    "    \n",
    "    # bỏ mấy bảng tham khảo màu đỏ\n",
    "    for p_tag in soup.find_all('p'):\n",
    "        for strong_tag in p_tag.find_all('strong', style=\"color: rgb(250, 10, 10);\"):\n",
    "            p_tag.decompose()\n",
    "    \n",
    "    # Xóa chữ \"Xem thêm\"\n",
    "    for p_tag in soup.find_all('p'):\n",
    "        if p_tag.get_text(strip=True).lower() == '>>xem thêm mẫu:' or p_tag.get_text(strip=True).lower() == \"xem thêm:\" or p_tag.get_text(strip=True).lower() == \">> xem thêm:\":\n",
    "            p_tag.decompose()\n",
    "                   \n",
    "    # Xóa chữ \"Xem thêm\"\n",
    "    for p_tag in soup.find_all('p'):\n",
    "        if p_tag.get_text(strip=True).lower() == '>>xem thêm mẫu' or p_tag.get_text(strip=True).lower() == \"xem thêm\" or p_tag.get_text(strip=True).lower() == \">> xem thêm\":\n",
    "            p_tag.decompose()\n",
    "    \n",
    "    # Xóa chữ \"Về vấn đề này <span style=\"color:#FF0000;\"><strong>THƯ VIỆN PHÁP LUẬT</strong></span> giải đáp như sau:</p>\"\n",
    "    for p_tag in soup.find_all('p'):\n",
    "        for span_tag in p_tag.find_all('span'):\n",
    "            if span_tag.get_text(strip=True).lower() == \"thư viện pháp luật\" or span_tag.get_text(strip=True).lower() == \"pháp lý khởi nghiệp\" or span_tag.get_text(strip=True).lower() == \"pháp luật doanh nghiệp\":\n",
    "                p_tag.decompose()\n",
    "                \n",
    "    # Xóa chữ \"trân trọng!\"\n",
    "    for p_tag in soup.find_all('p'):\n",
    "        if p_tag.get_text(strip=True).lower() == \"trân trọng!\":\n",
    "            p_tag.decompose()\n",
    "        # Xóa chữ \"trân trọng!\"\n",
    "    for p_tag in soup.find_all('p'):\n",
    "        if p_tag.get_text(strip=True).lower() == \"trân trọng\":\n",
    "            p_tag.decompose()\n",
    "     # Xóa chữ \"trân trọng!\"\n",
    "    for p_tag in soup.find_all('p'):\n",
    "        if p_tag.get_text(strip=True).lower() == 'chúng tôi phản hồi thông tin đến bạn.':\n",
    "            p_tag.decompose()\n",
    "    # Xóa chữ \"Trên đây là một số thông tin chúng tôi cung cấp gửi tới bạn. Trân trọng!\"        \n",
    "    for p_tag in soup.find_all('p'):\n",
    "        if p_tag.get_text(strip=True).lower() == \"trên đây là một số thông tin chúng tôi cung cấp gửi tới bạn. trân trọng!\":\n",
    "            p_tag.decompose()\n",
    "    \n",
    "    # xóa tên tác giả\n",
    "    # <p style=\"text-align: right;\"><strong>Như Mai</strong></p>\n",
    "    for p_tag in soup.find_all('p', style=\"text-align: right;\"):\n",
    "        try:\n",
    "            strong_tag = p_tag.find('strong')\n",
    "            if strong_tag:  # Nếu tìm thấy thẻ <strong>\n",
    "                p_tag.decompose()  # Xóa toàn bộ thẻ <p>\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing tag: {e}\")\n",
    "            \n",
    "    for p_tag in soup.find_all('p', style=\"text-align: right;\"):\n",
    "        try:\n",
    "            strong_tag = p_tag.find('em')\n",
    "            if strong_tag:  # Nếu tìm thấy thẻ <strong>\n",
    "                p_tag.decompose()  # Xóa toàn bộ thẻ <p>\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing tag: {e}\")\n",
    "    for p_tag in soup.find_all('p', align=\"right\"):\n",
    "        try:\n",
    "            strong_tag = p_tag.find('strong')\n",
    "            if strong_tag:  # Nếu tìm thấy thẻ <strong>\n",
    "                p_tag.decompose()  # Xóa toàn bộ thẻ <p>\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing tag: {e}\")\n",
    "    for p_tag in soup.find_all('p', align=\"right\"):\n",
    "        try:\n",
    "            strong_tag = p_tag.find('em')\n",
    "            if strong_tag:  # Nếu tìm thấy thẻ <strong>\n",
    "                p_tag.decompose()  # Xóa toàn bộ thẻ <p>\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing tag: {e}\")\n",
    "    \n",
    "    # # chỉ duy nhất trang KN_FAQ và KN_JOB\n",
    "    # # Tìm tất cả các thẻ table\n",
    "    # tables = soup.find_all('table')\n",
    "\n",
    "    # # Duyệt qua từng thẻ table\n",
    "    # for table in tables:\n",
    "    #     # Tìm tất cả các thẻ td trong table\n",
    "    #     tds = table.find_all('td')\n",
    "        \n",
    "    #     # Nếu chỉ có duy nhất 1 thẻ td, thì xóa cả table\n",
    "    #     if len(tds) == 1:\n",
    "    #         table.decompose()\n",
    "    # Xử lý phần tình huống pháp lý chứa trong nội dung\n",
    "    # Tìm tất cả các thẻ <p> có chứa \"Trả lời:\"\n",
    "    answer_paragraphs = soup.find_all('p', string=lambda text: text == \"Trả lời:\")\n",
    "    # Lặp qua các thẻ <p> đã tìm được\n",
    "    for answer_p in answer_paragraphs:\n",
    "        # Tìm thẻ <h2> phía trên thẻ <p>\n",
    "        h2_above = answer_p.find_previous('h2')\n",
    "        \n",
    "        if h2_above:\n",
    "            # Lấy tất cả các phần tử từ <h2> tới <p> và xóa chúng (nhưng không xóa <h2>)\n",
    "            current_element = h2_above.find_next_sibling()\n",
    "            while current_element and current_element != answer_p:\n",
    "                next_element = current_element.find_next_sibling()\n",
    "                current_element.decompose()\n",
    "                current_element = next_element\n",
    "            # Xóa luôn thẻ <p> có chứa \"Trả lời:\"\n",
    "            answer_p.decompose()\n",
    "            \n",
    "    # xóa các link dẫn\n",
    "    for p_tag in soup.find_all('p'):\n",
    "        # Lặp qua tất cả các thẻ <a> bên trong mỗi thẻ <p>\n",
    "        for a_tag in p_tag.find_all('a', href=True):\n",
    "            if a_tag is not None:  # Đảm bảo a_tag không phải là None\n",
    "                try:\n",
    "                    href_value = a_tag.get('href')  # Sử dụng get để tránh lỗi\n",
    "                    if href_value and (href_value.endswith(('.html', '.htm')) or 'html?' in href_value):\n",
    "                        # print(p_tag.text)\n",
    "                        p_tag.decompose()  # Xóa thẻ <p> chứa thẻ <a> này\n",
    "                    # extensions_end = ('.doc', '.docx', '.xls', '.xlsx', '.zip', '.rar', '.jpg', '.pdf', '.png')\n",
    "                    extensions_start = ('>> xem chi tiết', 'xem chi tiết', '>> xem toàn bộ', 'xem toàn bộ', 'xem thêm', '>> xem thêm', 'xem chi tiết tại', '>> tham khảo', 'tham khảo', 'tải đầy đủ', '>>>')\n",
    "                    if href_value and href_value.startswith('http') and (not '.aspx' in href_value) and p_tag.get_text(strip=True).lower().startswith(extensions_start):\n",
    "                        p_tag.decompose()  # Xóa thẻ <p> chứa thẻ <a> này\n",
    "                except AttributeError as e:\n",
    "                    print(f\"Đã gặp lỗi AttributeError: {e}\")\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import uuid\n",
    "import base64\n",
    "\n",
    "def save_base64_image(base64_string, file_extension=\"png\", folder_name=\"img_base64_convert\"):\n",
    "    # Tạo thư mục nếu chưa tồn tại\n",
    "    if not os.path.exists(folder_name):\n",
    "        os.makedirs(folder_name)\n",
    "    \n",
    "    # Giải mã chuỗi base64\n",
    "    image_data = base64_string.split(',')[1]  # Tách phần dữ liệu base64\n",
    "    image_bytes = base64.b64decode(image_data)\n",
    "\n",
    "    # Tạo tên file ngẫu nhiên với phần mở rộng được chỉ định\n",
    "    file_name = f\"{uuid.uuid4()}.{file_extension}\"\n",
    "\n",
    "    # Đường dẫn đầy đủ để lưu file\n",
    "    file_path = os.path.join(folder_name, file_name)\n",
    "\n",
    "    # Lưu ảnh vào thư mục\n",
    "    with open(file_path, 'wb') as f:\n",
    "        f.write(image_bytes)\n",
    "\n",
    "    return file_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def processing_1(soup):\n",
    "    questions_and_answers1 = []  # Danh sách để lưu trữ các link và anchor cho từng đoạn h2\n",
    "    # Tìm tất cả các thẻ <h2>\n",
    "    h2_tags = []\n",
    "    for temp in soup.find_all('h2'):\n",
    "        # Kiểm tra nếu thẻ <h2> có chứa thẻ <strong> và không phải là thẻ trống hoặc chỉ chứa <br>\n",
    "        # if temp.find('strong') and temp.get_text(strip=True):\n",
    "        if temp.get_text(strip=True):\n",
    "            # and len(temp.get_text(strip=True)) >= 9:\n",
    "            h2_tags.append(temp)\n",
    "    # Lặp qua từng cặp thẻ <h2>\n",
    "    # print(h2_tags)\n",
    "    for i in range(len(h2_tags)):\n",
    "        h2_current = h2_tags[i]\n",
    "        if i + 1 < len(h2_tags):\n",
    "            h2_next = h2_tags[i + 1]\n",
    "        else:\n",
    "            h2_next = 'Xuan Vinh Gay'\n",
    "\n",
    "        patterns = r\"^\\d+\\.\\s*\"\n",
    "        # Tìm và thay thế phần (1) và phần 1. bằng chuỗi rỗng\n",
    "        second_part = re.sub(patterns, \"\", h2_current.text).strip()\n",
    "        second_part = unicodedata.normalize(\"NFC\", second_part)\n",
    "        # Lấy văn bản cho câu trả lời\n",
    "        answer_text = \"\"\n",
    "        next_element = h2_current.next_sibling\n",
    "        while next_element and (not isinstance(next_element, Tag) or (next_element.name != 'h2')):\n",
    "            if next_element and next_element.name == 'table':\n",
    "                a_tag = next_element.find('a')\n",
    "                # span_tag = next_element.find_all('span')\n",
    "                if a_tag:\n",
    "                    try:\n",
    "                        href = a_tag['href']\n",
    "                        # Danh sách các định dạng tài liệu\n",
    "                        file_extensions = ('.doc', '.docx', '.xls', '.xlsx', '.zip', '.rar', '.jpg', '.pdf', '.png', '.jpeg')\n",
    "                        # Kiểm tra nếu href kết thúc bằng một trong các định dạng tài liệu\n",
    "                        if href.endswith(file_extensions):\n",
    "                            # Tìm thẻ td chứa thẻ a\n",
    "                            td_tag = a_tag.find_parent('td')\n",
    "                            if td_tag:\n",
    "                                # Tìm thẻ td kế tiếp (ngang hàng)\n",
    "                                next_td = td_tag.find_next_sibling('td')\n",
    "                                if next_td:\n",
    "                                    link_text = next_td.get_text(strip=True)\n",
    "                                    # Thay thế thẻ a bằng văn bản Markdown\n",
    "                                    a_tag.replace_with(f\"[{link_text}]({href})\\n\")\n",
    "                                    answer_text += next_element.get_text() + \"\\n\"\n",
    "                        else:\n",
    "                            # Nếu href không phải tài liệu\n",
    "                            answer = unicodedata.normalize(\"NFC\", next_element.get_text())\n",
    "                            answer_text += answer + \"\\n\"\n",
    "                    except KeyError as e:\n",
    "                        print(f\"Thuộc tính 'href' không tồn tại trong thẻ <a>. Lỗi: {e}\")\n",
    "                else:\n",
    "                    # Nếu không tìm thấy thẻ a\n",
    "                    # print(table_to_markdown(next_element))\n",
    "                    answer_text += table_to_markdown(next_element) + \"\\n\"\n",
    "            \n",
    "            elif next_element and next_element.name == 'blockquote':\n",
    "                if next_element.find('img'):\n",
    "                    try:\n",
    "                        img_tag = next_element.find('img')\n",
    "                        src = img_tag['src']  \n",
    "                        # Kiểm tra xem ảnh có phải là base64\n",
    "                        if src.startswith('data:image'):\n",
    "                            alt_text = img_tag['alt'] if img_tag.has_attr('alt') else ''\n",
    "                            \n",
    "                            # Lưu ảnh base64 vào file và chọn định dạng file (ví dụ: 'png')\n",
    "                            file_name = save_base64_image(src, file_extension=\"png\", folder_name=\"img_base64_convert\")\n",
    "                            \n",
    "                            # Sử dụng tên file thay vì chuỗi base64\n",
    "                            answer_text += f\"![{alt_text}]({file_name})\\n\"\n",
    "                        else:\n",
    "                            alt_text = img_tag['alt'] if img_tag.has_attr('alt') else ''\n",
    "                            answer_text += f\"![{alt_text}]({src})\\n\"\n",
    "                    except KeyError as e:\n",
    "                        print(f\"Thuộc tính 'src' không tồn tại. Lỗi: {e}\")\n",
    "                elif isinstance(next_element, NavigableString):\n",
    "                    answer = unicodedata.normalize(\"NFC\", next_element)\n",
    "                    answer_text += answer.strip() + \"\\n\"\n",
    "                elif next_element and next_element.name:\n",
    "                    answer = unicodedata.normalize(\"NFC\", next_element.get_text())\n",
    "                    answer_text += answer.strip() + \"\\n\"\n",
    "            elif next_element and next_element.name == 'p':\n",
    "                if next_element.find('img'):\n",
    "                    img_tag = next_element.find('img')\n",
    "                    if img_tag:\n",
    "                        try:\n",
    "                            src = img_tag['src']\n",
    "\n",
    "                            # Kiểm tra chuỗi base64\n",
    "                            if src.startswith('data:image'):\n",
    "                                alt_text = img_tag['alt'] if img_tag.has_attr('alt') else ''\n",
    "                                \n",
    "                                # Lưu ảnh base64 với định dạng file cố định (ví dụ: 'png')\n",
    "                                file_name = save_base64_image(src, file_extension=\"png\", folder_name=\"img_base64_convert\")\n",
    "                                \n",
    "                                # Sử dụng tên file thay vì chuỗi base64\n",
    "                                answer_text += f\"![{alt_text}]({file_name})\\n\"\n",
    "                            else:\n",
    "                                alt_text = img_tag['alt'] if img_tag.has_attr('alt') else ''\n",
    "                                answer_text += f\"![{alt_text}]({src})\\n\"\n",
    "                        except KeyError as e:\n",
    "                            print(f\"Thuộc tính 'src' không tồn tại trong thẻ <img>. Lỗi: {e}\")            \n",
    "                elif next_element.find('a'):\n",
    "                    a_tag = next_element.find('a')\n",
    "                    if a_tag:\n",
    "                        try:\n",
    "                            href = a_tag['href']\n",
    "                            # Danh sách các định dạng tài liệu\n",
    "                            file_extensions = ('.doc', '.docx', '.xls', '.xlsx', '.zip', '.rar', '.jpg', '.pdf', '.png', '.jpeg')\n",
    "                            # Kiểm tra nếu href kết thúc bằng một trong các định dạng tài liệu\n",
    "                            if href.endswith(file_extensions):\n",
    "                                link_text = a_tag.text\n",
    "                                for drop_a_tag in next_element.find('a'):\n",
    "                                    drop_a_tag.replace_with(f\"[{link_text}]({href})\\n\")\n",
    "                                answer_text += next_element.get_text() + \"\\n\"\n",
    "                            else:\n",
    "                                answer = unicodedata.normalize(\"NFC\", next_element.get_text())\n",
    "                                answer_text += answer.strip() + \"\\n\"\n",
    "                        except KeyError as e:\n",
    "                            print(f\"Thuộc tính 'href' không tồn tại trong <link>. Lỗi: {e}\")\n",
    "                # Kiểm tra chuỗi văn bản thuần\n",
    "                elif isinstance(next_element, NavigableString):\n",
    "                    answer = unicodedata.normalize(\"NFC\", next_element)\n",
    "                    answer_text += answer.strip() + \"\\n\"\n",
    "                \n",
    "                # Kiểm tra thẻ khác và lấy văn bản\n",
    "                elif next_element and next_element.name:\n",
    "                    answer = unicodedata.normalize(\"NFC\", next_element.get_text())\n",
    "                    answer_text += answer.strip() + \"\\n\"\n",
    "            elif next_element and next_element.name == 'h3':\n",
    "                patterns = r'^\\d+(\\.\\d+)*\\.\\s*'\n",
    "                if isinstance(next_element, NavigableString):\n",
    "                    # Tìm và thay thế phần (1) và phần 1. bằng chuỗi rỗng\n",
    "                    text_strip = re.sub(patterns, \"\", next_element).strip()\n",
    "                    answer = unicodedata.normalize(\"NFC\", text_strip)\n",
    "                    answer_text += answer.strip() + \"\\n\"\n",
    "                elif next_element and next_element.name:\n",
    "                    text_strip = re.sub(patterns, \"\", next_element.get_text()).strip()\n",
    "                    answer = unicodedata.normalize(\"NFC\", text_strip)\n",
    "                    answer_text += answer.strip() + \"\\n\"\n",
    "            elif isinstance(next_element, NavigableString):\n",
    "                    answer = unicodedata.normalize(\"NFC\", next_element)\n",
    "                    answer_text += answer.strip() + \"\\n\"\n",
    "            elif next_element and next_element.name:\n",
    "                    answer = unicodedata.normalize(\"NFC\", next_element.get_text())\n",
    "                    answer_text += answer.strip() + \"\\n\"\n",
    "            next_element = next_element.next_sibling\n",
    "        \n",
    "        # Tìm tất cả các thẻ giữa h2_current và h2_next\n",
    "        siblings = []\n",
    "        next_tag = h2_current.find_next_sibling()\n",
    "        while next_tag and next_tag != h2_next:\n",
    "            siblings.append(next_tag)\n",
    "            next_tag = next_tag.find_next_sibling()\n",
    "        # Lặp qua các thẻ từ dưới lên trên trong các thẻ anh chị em và lưu kết quả tạm thời\n",
    "        temp_results = []  # Danh sách tạm thời để lưu trữ các kết quả\n",
    "        skip_next_p_check = True  # Biến cờ để kiểm tra thẻ <p> tiếp theo\n",
    "        for tag in reversed(siblings):\n",
    "            if tag.name == 'p':  # Kiểm tra nếu tag là thẻ <p>\n",
    "                p_tag = tag.text\n",
    "                # Kiểm tra xem nội dung của tag có bắt đầu bằng '(' và kết thúc bằng ')'\n",
    "                if tag and tag.find('a') and p_tag.strip().startswith('Căn cứ pháp lý:'):\n",
    "                    if skip_next_p_check:\n",
    "                        for a_tag in tag.find_all('a'):\n",
    "                            href = a_tag.get('href')\n",
    "                            if href and '.aspx' in href and 'thuvienphapluat.vn' in href:\n",
    "                                final = processing_ccpl_full(a_tag.text)\n",
    "                                positions = split_by_dieu(final)\n",
    "                                # test = p_tag.split(positions)\n",
    "                                # print(positions)\n",
    "                                if positions:\n",
    "                                    lawtitle = positions[-1]\n",
    "                                else:\n",
    "                                    lawtitle = \"default_value\"  # Hoặc một giá trị mặc định nếu positions rỗng\n",
    "\n",
    "                                # Kiểm tra nếu lawtitle không rỗng trước khi gọi split()\n",
    "                                if lawtitle:\n",
    "                                    test = p_tag.split(lawtitle)\n",
    "                                else:\n",
    "                                    # Xử lý khi lawtitle rỗng (nếu cần thiết)\n",
    "                                    test = []\n",
    "                                anchor_match = re.search(r'anchor=(\\w+)', href)\n",
    "                                anchor = anchor_match.group(1) if anchor_match else \"0\"\n",
    "                                if test and \"điều\" in test[0].lower():\n",
    "                                    p_tag = p_tag.replace(test[0], '', 1)\n",
    "                                    final = processing_ccpl_full(test[0])\n",
    "                                    positions = split_by_dieu(final)\n",
    "                                    data_ccpl = []\n",
    "                                    for dieu in positions:\n",
    "                                            if \"điều\" in dieu.lower():\n",
    "                                                dieu_dict = {\"Dieu\": extract_dieu(dieu), \"Khoan\": []}\n",
    "                                                # Tìm anchor trong href\n",
    "                                                khoan_list = split_by_khoan(dieu)\n",
    "                                                for khoan in khoan_list:\n",
    "                                                    if \"khoản\" in khoan.lower():\n",
    "                                                        khoan_dict = {\"Khoan\": extract_khoan(khoan), \"Diem\": []}\n",
    "                                                        if \"điểm\" in khoan.lower():\n",
    "                                                            diem_dict = extract_diem(khoan)\n",
    "                                                            khoan_dict[\"Diem\"].append(diem_dict)\n",
    "                                                        dieu_dict[\"Khoan\"].append(khoan_dict)\n",
    "                                                data_ccpl.append(dieu_dict)\n",
    "                                                # print(data_ccpl)\n",
    "                                                # Tạo một từ điển mới và gán giá trị từ mảng\n",
    "                                                ccpl_temp = processing_output_ccpl(data_ccpl)\n",
    "                                                if len(ccpl_temp) > 3:\n",
    "                                                    for i in range(0, len(ccpl_temp), 3):\n",
    "                                                        # Lấy 3 phần tử từ chỉ mục i đến i+3 (không bị lỗi nếu kích thước không đủ)\n",
    "                                                        chunk = ccpl_temp[i:i+3]\n",
    "                                                        anchor_match = re.search(r'anchor=(\\w+)', href)\n",
    "                                                        law_id_match = re.search(r'-(\\d+)\\.aspx', href)\n",
    "                                                        anchor = anchor_match.group(1) if anchor_match else \"0\"\n",
    "                                                        law_id = law_id_match.group(1) if law_id_match else \"0\"\n",
    "                                                        # print(ccpl_temp)\n",
    "                                                        if len(chunk) > 0:\n",
    "                                                            dictionary = {\n",
    "                                                                \"url\": href,\n",
    "                                                                \"anchor\": anchor,\n",
    "                                                                \"LawID\": law_id,\n",
    "                                                                \"LawTitle\": lawtitle,\n",
    "                                                                \"Dieu\": chunk[0] or \"0\",\n",
    "                                                                \"Khoan\": chunk[1] or \"0\",\n",
    "                                                                \"Diem\": chunk[2] or \"0\"\n",
    "                                                            }\n",
    "                                                            temp_results.append(dictionary)\n",
    "                                                else:\n",
    "                                                    anchor_match = re.search(r'anchor=(\\w+)', href)\n",
    "                                                    law_id_match = re.search(r'-(\\d+)\\.aspx', href)\n",
    "                                                    anchor = anchor_match.group(1) if anchor_match else \"0\"\n",
    "                                                    law_id = law_id_match.group(1) if law_id_match else \"0\"\n",
    "                                                    if len(ccpl_temp) > 0:\n",
    "                                                        dictionary = {\n",
    "                                                            \"url\": href,\n",
    "                                                            \"anchor\": anchor,\n",
    "                                                            \"LawID\": law_id,\n",
    "                                                            \"LawTitle\": lawtitle,\n",
    "                                                            \"Dieu\": ccpl_temp[0] or \"0\",\n",
    "                                                            \"Khoan\": ccpl_temp[1] or \"0\",\n",
    "                                                            \"Diem\": ccpl_temp[2] or \"0\"\n",
    "                                                        }\n",
    "                                                        temp_results.append(dictionary)\n",
    "\n",
    "                                else:\n",
    "                                    temp_else = {}\n",
    "                                    anchor_match = re.search(r'anchor=(\\w+)', href)\n",
    "                                    law_id_match = re.search(r'-(\\d+)\\.aspx', href)\n",
    "                                    anchor = anchor_match.group(1) if anchor_match else \"0\"\n",
    "                                    law_id = law_id_match.group(1) if law_id_match else \"0\"\n",
    "                                    # law_title = a_tag.get_text()\n",
    "                                    temp_else[\"url\"] = href\n",
    "                                    temp_else[\"anchor\"] = anchor\n",
    "                                    temp_else[\"LawID\"] = law_id\n",
    "                                    temp_else[\"LawTitle\"] = lawtitle\n",
    "                                    temp_else[\"Dieu\"] = \"0\"\n",
    "                                    temp_else[\"Khoan\"] = \"0\"\n",
    "                                    temp_else[\"Diem\"] = \"0\"\n",
    "                                    temp_results.append(temp_else)\n",
    "                    skip_next_p_check = False\n",
    "        ccpls = []\n",
    "        ccpls.extend(temp_results)\n",
    "        \n",
    "        questions_and_answers1.append({\n",
    "            \"question\": second_part,\n",
    "            \"answer\": answer_text.strip(),\n",
    "            \"ccpls\": ccpls\n",
    "        })\n",
    "    patterns = [    \n",
    "        re.compile(r\"Chọn lĩnh vực để xem văn bản liên quan.*\", re.MULTILINE),\n",
    "        re.compile(r\">> Quý khách hàng xem thêm.*\", re.MULTILINE),\n",
    "        re.compile(r\"Quý khách hàng xem thêm.*\", re.MULTILINE),\n",
    "        re.compile(r\"Quý khách hành xem thêm.*\", re.MULTILINE),    \n",
    "        re.compile(r\"Quý khách hàng có thể.*\", re.MULTILINE),\n",
    "    ]\n",
    "\n",
    "    for item in questions_and_answers1:\n",
    "        answer_text = item[\"answer\"]\n",
    "        # Khởi tạo start_index với giá trị -1 (không tìm thấy)\n",
    "        start_index = -1\n",
    "        # Kiểm tra từng mẫu regex và cập nhật start_index nếu tìm thấy\n",
    "        for pattern in patterns:\n",
    "            match = pattern.search(answer_text)\n",
    "            if match:\n",
    "                index = match.start()\n",
    "                # Nếu start_index chưa được cập nhật hoặc tìm thấy index nhỏ hơn (gần đầu chuỗi hơn)\n",
    "                if start_index == -1 or index < start_index:\n",
    "                    start_index = index\n",
    "\n",
    "        # Nếu tìm thấy bất kỳ mẫu nào, xóa từ đoạn đó đến hết câu trả lời\n",
    "        if start_index != -1:\n",
    "            cleaned_answer = answer_text[:start_index].strip()\n",
    "            item[\"answer\"] = cleaned_answer\n",
    "    # print(json.dumps(questions_and_answers1, ensure_ascii=False, indent=5))        \n",
    "    # # In ra danh sách câu hỏi và câu trả lời đã được làm sạch\n",
    "    return json.dumps(questions_and_answers1, ensure_ascii=False, indent=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processing_5(soup):\n",
    "    h2_tags = []\n",
    "    for temp in soup.find_all('h2'):\n",
    "        # Kiểm tra nếu thẻ <h2> có chứa thẻ <strong> và không phải là thẻ trống hoặc chỉ chứa <br>\n",
    "        # if temp.find('strong') and temp.get_text(strip=True):\n",
    "        if temp.get_text(strip=True):\n",
    "        # and len(temp.get_text(strip=True)) >= 9:\n",
    "            h2_tags.append(temp)\n",
    "\n",
    "    questions_and_answers5 = []  # Danh sách để lưu trữ các link và anchor cho từng đoạn h2\n",
    "\n",
    "\n",
    "    for h2_tag in h2_tags:\n",
    "        temp_results = []\n",
    "        next_tag = h2_tag.find_next_sibling()\n",
    "        gay_flag = True\n",
    "        while next_tag and next_tag.name != 'h2':\n",
    "            if next_tag.name == 'p':\n",
    "                p_tag = next_tag.text\n",
    "                if (re.search(r\"thư\\s+viện\\s+pháp\\s+luật\", next_tag.text.lower())):\n",
    "                    gay_flag = False\n",
    "                if next_tag.find('a') and not next_tag.text.startswith('>>') and gay_flag == True:\n",
    "                    for a_tag in next_tag.find_all('a'):\n",
    "                        href = a_tag.get('href')\n",
    "                        if href and '.aspx' in href and 'thuvienphapluat.vn' in href:\n",
    "                            final = processing_ccpl_full(a_tag.text)\n",
    "                            positions = split_by_dieu(final)\n",
    "                            # test = p_tag.split(positions)\n",
    "                            # print(positions)\n",
    "                            if positions:\n",
    "                                lawtitle = positions[-1]\n",
    "                            else:\n",
    "                                lawtitle = \"default_value\"  # Hoặc một giá trị mặc định nếu positions rỗng\n",
    "\n",
    "                            # Kiểm tra nếu lawtitle không rỗng trước khi gọi split()\n",
    "                            if lawtitle:\n",
    "                                test = p_tag.split(lawtitle)\n",
    "                            else:\n",
    "                                # Xử lý khi lawtitle rỗng (nếu cần thiết)\n",
    "                                test = []\n",
    "                            # print(lawtitle)\n",
    "                            anchor_match = re.search(r'anchor=(\\w+)', href)\n",
    "                            anchor = anchor_match.group(1) if anchor_match else \"0\"\n",
    "                            if test and \"điều\" in test[0].lower():\n",
    "                                p_tag = p_tag.replace(test[0], '', 1)\n",
    "                                final = processing_ccpl_full(test[0])\n",
    "                                positions = split_by_dieu(final)\n",
    "                                data_ccpl = []\n",
    "                                for dieu in positions:\n",
    "                                        if \"điều\" in dieu.lower():\n",
    "                                            dieu_dict = {\"Dieu\": extract_dieu(dieu), \"Khoan\": []}\n",
    "                                            # Tìm anchor trong href\n",
    "                                            khoan_list = split_by_khoan(dieu)\n",
    "                                            for khoan in khoan_list:\n",
    "                                                if \"khoản\" in khoan.lower():\n",
    "                                                    khoan_dict = {\"Khoan\": extract_khoan(khoan), \"Diem\": []}\n",
    "                                                    if \"điểm\" in khoan.lower():\n",
    "                                                        diem_dict = extract_diem(khoan)\n",
    "                                                        khoan_dict[\"Diem\"].append(diem_dict)\n",
    "                                                    dieu_dict[\"Khoan\"].append(khoan_dict)\n",
    "                                            data_ccpl.append(dieu_dict)\n",
    "                                            # print(data_ccpl)\n",
    "                                            # Tạo một từ điển mới và gán giá trị từ mảng\n",
    "                                            ccpl_temp = processing_output_ccpl(data_ccpl)\n",
    "                                            if len(ccpl_temp) > 3:\n",
    "                                                for i in range(0, len(ccpl_temp), 3):\n",
    "                                                    # Lấy 3 phần tử từ chỉ mục i đến i+3 (không bị lỗi nếu kích thước không đủ)\n",
    "                                                    chunk = ccpl_temp[i:i+3]\n",
    "                                                    anchor_match = re.search(r'anchor=(\\w+)', href)\n",
    "                                                    law_id_match = re.search(r'-(\\d+)\\.aspx', href)\n",
    "                                                    anchor = anchor_match.group(1) if anchor_match else \"0\"\n",
    "                                                    law_id = law_id_match.group(1) if law_id_match else \"0\"\n",
    "                                                    # print(ccpl_temp)\n",
    "                                                    if len(chunk) > 0:\n",
    "                                                        dictionary = {\n",
    "                                                            \"url\": href,\n",
    "                                                            \"anchor\": anchor,\n",
    "                                                            \"LawID\": law_id,\n",
    "                                                            \"LawTitle\": lawtitle,\n",
    "                                                            \"Dieu\": chunk[0] or \"0\",\n",
    "                                                            \"Khoan\": chunk[1] or \"0\",\n",
    "                                                            \"Diem\": chunk[2] or \"0\"\n",
    "                                                        }\n",
    "                                                        temp_results.append(dictionary)\n",
    "                                            else:\n",
    "                                                anchor_match = re.search(r'anchor=(\\w+)', href)\n",
    "                                                law_id_match = re.search(r'-(\\d+)\\.aspx', href)\n",
    "                                                anchor = anchor_match.group(1) if anchor_match else \"0\"\n",
    "                                                law_id = law_id_match.group(1) if law_id_match else \"0\"\n",
    "                                                if len(ccpl_temp) > 0:\n",
    "                                                    dictionary = {\n",
    "                                                        \"url\": href,\n",
    "                                                        \"anchor\": anchor,\n",
    "                                                        \"LawID\": law_id,\n",
    "                                                        \"LawTitle\": lawtitle,\n",
    "                                                        \"Dieu\": ccpl_temp[0] or \"0\",\n",
    "                                                        \"Khoan\": ccpl_temp[1] or \"0\",\n",
    "                                                        \"Diem\": ccpl_temp[2] or \"0\"\n",
    "                                                    }\n",
    "                                                    temp_results.append(dictionary)\n",
    "\n",
    "                            else:\n",
    "                                temp_else = {}\n",
    "                                anchor_match = re.search(r'anchor=(\\w+)', href)\n",
    "                                law_id_match = re.search(r'-(\\d+)\\.aspx', href)\n",
    "                                anchor = anchor_match.group(1) if anchor_match else \"0\"\n",
    "                                law_id = law_id_match.group(1) if law_id_match else \"0\"\n",
    "                                # law_title = a_tag.get_text()\n",
    "                                temp_else[\"url\"] = href\n",
    "                                temp_else[\"anchor\"] = anchor\n",
    "                                temp_else[\"LawID\"] = law_id\n",
    "                                temp_else[\"LawTitle\"] = lawtitle\n",
    "                                temp_else[\"Dieu\"] = \"0\"\n",
    "                                temp_else[\"Khoan\"] = \"0\"\n",
    "                                temp_else[\"Diem\"] = \"0\"\n",
    "                                temp_results.append(temp_else)\n",
    "                    gay_flag = True\n",
    "            next_tag = next_tag.find_next_sibling()\n",
    "        ccpls = []\n",
    "        # ccpls.extend(temp_results)\n",
    "        ccpls.extend(temp_results)\n",
    "        questions_and_answers5.append({\n",
    "            \"ccpls\": ccpls\n",
    "        })\n",
    "    # In ra danh sách câu hỏi và câu trả lời đã được làm sạch\n",
    "    return json.dumps(questions_and_answers5, ensure_ascii=False, indent=5)\n",
    "# print(json.dumps(questions_and_answers4, ensure_ascii=False, indent=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hàm giải mã lawid\n",
    "def decrypt(s):\n",
    "    try:\n",
    "        tem = s\n",
    "        if len(tem) > 4:\n",
    "            tem = tem[-2:] + tem[2:-2] + tem[:2]\n",
    "        return base64.b64decode(base64.b64decode(tem).decode('utf-8')).decode('utf-8')\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "def decode_ccpls(questions_and_answers_final):\n",
    "    for temp in questions_and_answers_final:\n",
    "        for temp1 in temp.get('ccpls'):\n",
    "            if '.aspx?id=' in temp1.get('url'):\n",
    "                # Phân tích cú pháp URL\n",
    "                parsed_url = urlparse(temp1.get('url'))\n",
    "                # Lấy các tham số truy vấn từ URL\n",
    "                query_params = parse_qs(parsed_url.query)\n",
    "                # Lấy giá trị của tham số 'id'\n",
    "                id_value = query_params.get('id', [None])[0]\n",
    "                temp1['LawID'] = decrypt(id_value)\n",
    "    return questions_and_answers_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(data):\n",
    "    soup = processing_redundant_data(data)\n",
    "    file1_data = json.loads(processing_1(soup))\n",
    "    file5_data = json.loads(processing_5(soup))\n",
    "    \n",
    "    # ghép ccpl loại 5\n",
    "    for index, item in enumerate(file1_data):\n",
    "        if not item['ccpls']:\n",
    "            item['ccpls'] = file5_data[index]['ccpls']\n",
    "    \n",
    "    # Chỉ giữ lại các mục có câu trả lời\n",
    "    filtered_questions_and_answers = [qa for qa in file1_data if 'answer' in qa and qa['answer'].strip()]\n",
    "    filtered_questions_and_answers1 = [qa for qa in filtered_questions_and_answers if 'question' in qa and qa['question'].strip()]\n",
    "    \n",
    "    questions_and_answers_final = []\n",
    "    for qa in filtered_questions_and_answers1:\n",
    "        question_text = qa[\"question\"]\n",
    "        # Kiểm tra xem câu hỏi có chứa các từ \"dự thảo\", \"đề xuất\", \"sắp tới\" hoặc \"dự kiến\" không\n",
    "        if not re.search(r'\\b(?:dự thảo|đề xuất|sắp tới|dự kiến|đáp án|tra cứu điểm thi)\\b', question_text.lower(), flags=re.IGNORECASE) and not question_text.lower().startswith(\"đã có\"):\n",
    "            questions_and_answers_final.append(qa)\n",
    "    # Remove duplicates\n",
    "    filtered_questions_and_answers1 = remove_duplicates_ccpls(filtered_questions_and_answers1)\n",
    "    filtered_questions_and_answers1  = decode_ccpls(filtered_questions_and_answers1)\n",
    "    return filtered_questions_and_answers1\n",
    "    # return json.dumps(questions_and_answers_final, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "function html request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import html\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def post_request(url, headers):\n",
    "    response = requests.post(url, headers=headers)\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "def post_request_key(url, headers, data):\n",
    "    try:\n",
    "        data = requests.post(url, headers=headers, data=json.dumps(data))\n",
    "        data.raise_for_status()  # Raise an error for bad status codes\n",
    "        return data.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sử dụng hàm để thực hiện yêu cầu POST\n",
    "def get_api():\n",
    "    url1 = 'https://apids.thuvienphapluat.vn/auth/get-token?key=pvaG4gRG9lIiwiaWF0IjoxNTE2MjM5MDIyfQS563xADXH'\n",
    "    headers1 = {\n",
    "        'Cookie': 'Culture=vi; Culture=vi'\n",
    "    }\n",
    "\n",
    "    key = post_request(url1, headers1)\n",
    "\n",
    "    url = 'https://apids.thuvienphapluat.vn/data/get-phapluat'\n",
    "    headers = {\n",
    "        'Authorization': 'Bearer ' + key['Data']['AccessToken'],\n",
    "        'Content-Type': 'application/json',\n",
    "        'Cookie': 'Culture=vi'\n",
    "    }\n",
    "    return url, headers\n",
    "\n",
    "def send_data_to_api(data):\n",
    "    url, headers = get_api()\n",
    "    response = requests.post(url, headers=headers, json=data)\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing tag: 'NoneType' object has no attribute 'find'\n",
      "Error processing tag: 'NoneType' object has no attribute 'find'\n",
      "[{\"question\": \"Thời hạn nộp tiền thuế TNDN tạm tính quý 2 năm 2024?\", \"answer\": \"Căn cứ Khoản 1 Điều 55 Luật Quản lý Thuế 2019 quy định về thời hạn nộp thuế như sau:\\nThời hạn nộp thuế\\n1. Trường hợp người nộp thuế tính thuế, thời hạn nộp thuế chậm nhất là ngày cuối cùng của thời hạn nộp hồ sơ khai thuế. Trường hợp khai bổ sung hồ sơ khai thuế, thời hạn nộp thuế là thời hạn nộp hồ sơ khai thuế của kỳ tính thuế có sai, sót.\\nĐối với thuế thu nhập doanh nghiệp thì tạm nộp theo quý, thời hạn nộp thuế chậm nhất là ngày 30 của tháng đầu quý sau.\\n…\\nNhư vậy, thời hạn nộp thuế TNDN tạm tính quý 2 (tức tháng 4,5,6) chậm nhất là ngày 30 của tháng đầu quý 3 tức ngày 30/07/2024. Trường hợp ngày cuối cùng của thời hạn là ngày nghỉ cuối tuần hoặc ngày nghỉ lễ thì thời hạn kết thúc tại thời điểm kết thúc ngày làm việc tiếp theo ngày nghỉ đó theo quy định tại khoản 5 Điều 148 Bộ luật Dân sự 2015.\", \"ccpls\": [{\"url\": \"https://thuvienphapluat.vn/van-ban/Thue-Phi-Le-Phi/Luat-quan-ly-thue-2019-387595.aspx?anchor=dieu_55\", \"anchor\": \"dieu_55\", \"LawID\": \"387595\", \"LawTitle\": \" Luật Quản lý Thuế 2019\", \"Dieu\": \"55\", \"Khoan\": \"1\", \"Diem\": \"0\"}, {\"url\": \"https://thuvienphapluat.vn/van-ban/Thu-tuc-To-tung/Bo-luat-to-tung-dan-su-2015-296861.aspx?anchor=dieu_148\", \"anchor\": \"dieu_148\", \"LawID\": \"296861\", \"LawTitle\": \" Bộ luật Dân sự 2015\", \"Dieu\": \"148\", \"Khoan\": \"5\", \"Diem\": \"0\"}]}, {\"question\": \"Chậm nộp tiền thuế TNDN tạm tính quý 2 năm 2024 bị xử lý như thế nào?\", \"answer\": \"Căn cứ vào khoản 1 Điều 59 Luật Quản lý Thuế 2019 quy định về các trường hợp phải nộp tiền chậm nộp đối với việc chậm nộp tiền thuế như sau:\\nXử lý đối với việc chậm nộp tiền thuế\\n1. Các trường hợp phải nộp tiền chậm nộp bao gồm:\\na) Người nộp thuế chậm nộp tiền thuế so với thời hạn quy định, thời hạn gia hạn nộp thuế, thời hạn ghi trong thông báo của cơ quan quản lý thuế, thời hạn trong quyết định ấn định thuế hoặc quyết định xử lý của cơ quan quản lý thuế;\\nb) Người nộp thuế khai bổ sung hồ sơ khai thuế làm tăng số tiền thuế phải nộp hoặc cơ quan quản lý thuế, cơ quan nhà nước có thẩm quyền kiểm tra, thanh tra phát hiện khai thiếu số tiền thuế phải nộp thì phải nộp tiền chậm nộp đối với số tiền thuế phải nộp tăng thêm kể từ ngày kế tiếp ngày cuối cùng thời hạn nộp thuế của kỳ tính thuế có sai, sót hoặc kể từ ngày hết thời hạn nộp thuế của tờ khai hải quan ban đầu;\\nc) Người nộp thuế khai bổ sung hồ sơ khai thuế làm giảm số tiền thuế đã được hoàn trả hoặc cơ quan quản lý thuế, cơ quan nhà nước có thẩm quyền kiểm tra, thanh tra phát hiện số tiền thuế được hoàn nhỏ hơn số tiền thuế đã hoàn thì phải nộp tiền chậm nộp đối với số tiền thuế đã hoàn trả phải thu hồi kể từ ngày nhận được tiền hoàn trả từ ngân sách nhà nước;\\nd) Trường hợp được nộp dần tiền thuế nợ quy định tại khoản 5 Điều 124 của Luật này;\\nđ) Trường hợp không bị xử phạt vi phạm hành chính về quản lý thuế do hết thời hiệu xử phạt nhưng bị truy thu số tiền thuế thiếu quy định tại khoản 3 Điều 137 của Luật này;\\ne) Trường hợp không bị xử phạt vi phạm hành chính về quản lý thuế đối với hành vi quy định tại khoản 3 và khoản 4 Điều 142 của Luật này;\\ng) Cơ quan, tổ chức được cơ quan quản lý thuế ủy nhiệm thu thuế chậm chuyển tiền thuế, tiền chậm nộp, tiền phạt của người nộp thuế vào ngân sách nhà nước thì phải nộp tiền chậm nộp đối với số tiền chậm chuyển theo quy định.\\n…\\nĐồng thời Căn cứ vào điểm b khoản 6 Điều 8 Nghị định 126/2020/NĐ-CP được sửa đổi bởi khoản 3 Điều 1 Nghị định 91/2022/NĐ-CP quy định về các loại thuế khai theo tháng, khai theo quý, khai theo năm, khai theo từng lần phát sinh nghĩa vụ thuế và khai quyết toán thuế như sau:\\nCác loại thuế khai theo tháng, khai theo quý, khai theo năm, khai theo từng lần phát sinh nghĩa vụ thuế và khai quyết toán thuế\\n....\\n6. Các loại thuế, khoản thu khai quyết toán năm và quyết toán đến thời điểm giải thể, phá sản, chấm dứt hoạt động, chấm dứt hợp đồng hoặc tổ chức lại doanh nghiệp. Trường hợp chuyển đổi loại hình doanh nghiệp (không bao gồm doanh nghiệp nhà nước cổ phần hóa) mà doanh nghiệp chuyển đổi kế thừa toàn bộ nghĩa vụ về thuế của doanh nghiệp được chuyển đổi thì không phải khai quyết toán thuế đến thời điểm có quyết định về việc chuyển đổi doanh nghiệp, doanh nghiệp khai quyết toán khi kết thúc năm. Cụ thể như sau:\\n…\\nb) Thuế thu nhập doanh nghiệp (trừ thuế thu nhập doanh nghiệp từ chuyển nhượng vốn của nhà thầu nước ngoài; thuế thu nhập doanh nghiệp kê khai theo phương pháp tỷ lệ trên doanh thu theo từng lần phát sinh hoặc theo tháng theo quy định tại điểm đ khoản 4 Điều này). Người nộp thuế phải tự xác định số thuế thu nhập doanh nghiệp tạm nộp quý (bao gồm cả tạm phân bổ số thuế thu nhập doanh nghiệp cho địa bàn cấp tỉnh nơi có đơn vị phụ thuộc, địa điểm kinh doanh, nơi có bất động sản chuyển nhượng khác với nơi người nộp thuế đóng trụ sở chính) và được trừ số thuế đã tạm nộp với số phải nộp theo quyết toán thuế năm.\\n…\\nTổng số thuế thu nhập doanh nghiệp đã tạm nộp của 04 quý không được thấp hơn 80% số thuế thu nhập doanh nghiệp phải nộp theo quyết toán năm. Trường hợp người nộp thuế nộp thiếu so với số thuế phải tạm nộp 04 quý thì phải nộp tiền chậm nộp tính trên số thuế nộp thiếu kể từ ngày tiếp sau ngày cuối cùng của thời hạn tạm nộp thuế thu nhập doanh nghiệp quý 04 đến ngày liền kề trước ngày nộp số thuế còn thiếu vào ngân sách nhà nước.\\n…\\nTheo đó, thuế TNDN là loại thuế tạm nộp theo quý và khai quyết toán theo năm.\\nNhư vậy, chậm nộp thuế TNDN quý 2 không thuộc các trường hợp xử lý đối với việc chậm nộp tiền thuế theo quy định trên và không bắt buộc phải khai, nộp hồ sơ khai thuế TNDN tạm nộp theo quý và số thuế TNDN tạm nộp sẽ do doanh nghiệp tự xác định. Chính vì vậy, hành vi chậm nộp, không nộp, thuế TNDN tạm nộp không bị xử phạt vi phạm.\\nTuy nhiên, trường hợp người nộp thuế chậm nộp (nộp thiếu) so với số thuế phải tạm nộp 04 quý thì phải nộp tiền chậm nộp tính trên số thuế nộp thiếu kể từ ngày tiếp sau ngày cuối cùng của thời hạn tạm nộp thuế thu nhập doanh nghiệp quý 04 đến ngày liền kề trước ngày nộp số thuế còn thiếu vào ngân sách nhà nước.\\nTrong đó, số thuế TNDN đã tạm nộp của 04 quý không được thấp hơn 80% số thuế TNDN phải nộp theo quyết toán năm.\", \"ccpls\": [{\"url\": \"https://thuvienphapluat.vn/van-ban/Thue-Phi-Le-Phi/Luat-quan-ly-thue-2019-387595.aspx?anchor=dieu_59\", \"anchor\": \"dieu_59\", \"LawID\": \"387595\", \"LawTitle\": \" Luật Quản lý Thuế 2019\", \"Dieu\": \"59\", \"Khoan\": \"1\", \"Diem\": \"0\"}, {\"url\": \"https://thuvienphapluat.vn/van-ban/Thue-Phi-Le-Phi/Nghi-dinh-126-2020-ND-CP-huong-dan-Luat-Quan-ly-thue-455733.aspx?anchor=dieu_8\", \"anchor\": \"dieu_8\", \"LawID\": \"455733\", \"LawTitle\": \" Nghị định 126/2020/NĐ-CP\", \"Dieu\": \"8\", \"Khoan\": \"6\", \"Diem\": \"b\"}, {\"url\": \"https://thuvienphapluat.vn/van-ban/Thue-Phi-Le-Phi/Nghi-dinh-126-2020-ND-CP-huong-dan-Luat-Quan-ly-thue-455733.aspx\", \"anchor\": \"0\", \"LawID\": \"455733\", \"LawTitle\": \"  \", \"Dieu\": \"1\", \"Khoan\": \"3\", \"Diem\": \"0\"}, {\"url\": \"https://thuvienphapluat.vn/van-ban/Thue-Phi-Le-Phi/Nghi-dinh-91-2022-ND-CP-sua-doi-Nghi-dinh-126-2020-ND-CP-huong-dan-Luat-Quan-ly-thue-516302.aspx?anchor=dieu_1\", \"anchor\": \"dieu_1\", \"LawID\": \"516302\", \"LawTitle\": \" Nghị định 91/2022/NĐ-CP\", \"Dieu\": \"0\", \"Khoan\": \"0\", \"Diem\": \"0\"}]}, {\"question\": \"Mức tính tiền chậm nộp thuế TNDN tạm nộp năm 2024?\", \"answer\": \"Căn cứ khoản 2 Điều 59 Luật Quản lý Thuế 2019 quy định về cách tính tiền chậm nộp thuế TNDN tạm nộp như sau:\\nXử lý đối với việc chậm nộp tiền thuế\\n…\\n2. Mức tính tiền chậm nộp và thời gian tính tiền chậm nộp được quy định như sau:\\na) Mức tính tiền chậm nộp bằng 0,03%/ngày tính trên số tiền thuế chậm nộp;\\nb) Thời gian tính tiền chậm nộp được tính liên tục kể từ ngày tiếp theo ngày phát sinh tiền chậm nộp quy định tại khoản 1 Điều này đến ngày liền kề trước ngày số tiền nợ thuế, tiền thu hồi hoàn thuế, tiền thuế tăng thêm, tiền thuế ấn định, tiền thuế chậm chuyển đã nộp vào ngân sách nhà nước.\\n…\\nNhư vậy, tiền chậm nộp thuế TNDN được tính bằng 0,03%/ngày tính trên số tiền thuế chậm nộp.\\nThời gian tính tiền chậm nộp được tính liên tục kể từ ngày tiếp theo ngày phát sinh tiền chậm nộp đến ngày liền kề trước ngày số tiền nợ thuế, tiền thu hồi hoàn thuế, tiền thuế tăng thêm, tiền thuế ấn định, tiền thuế chậm chuyển đã nộp vào ngân sách nhà nước.\", \"ccpls\": [{\"url\": \"https://thuvienphapluat.vn/van-ban/Thue-Phi-Le-Phi/Luat-quan-ly-thue-2019-387595.aspx?anchor=dieu_59\", \"anchor\": \"dieu_59\", \"LawID\": \"387595\", \"LawTitle\": \"  Luật Quản lý Thuế 2019\", \"Dieu\": \"59\", \"Khoan\": \"2\", \"Diem\": \"0\"}]}]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "url = 'https://thuvienphapluat.vn/phap-luat/chung-chi-dai-ly-bao-hiem-gom-nhung-chung-chi-nao-cap-chung-chi-dai-ly-bao-hiem-khong-dung-quy-dinh-962551-169772.html#cap-chung-chi-dai-ly-bao-hiem-khong-dung-quy-dinh-thi-co-so-dao-tao-dai-ly-bi-xu-phat-the-nao-2'\n",
    "# Gửi yêu cầu HTTP để lấy nội dung của trang web\n",
    "response = requests.get(url)\n",
    "\n",
    "# Kiểm tra xem yêu cầu có thành công không (mã trạng thái 200 là thành công)\n",
    "if response.status_code == 200:\n",
    "    # Lấy nội dung của trang web\n",
    "    html_content = response.text\n",
    "    # print(html_content)\n",
    "    print(json.dumps(extract_data(html_content), ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136283\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "Error processing tag: 'NoneType' object has no attribute 'find'\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "Error processing tag: 'NoneType' object has no attribute 'find'\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "Error processing tag: 'NoneType' object has no attribute 'find'\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "Error processing tag: 'NoneType' object has no attribute 'find'\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "Error processing tag: 'NoneType' object has no attribute 'find'\n",
      "Error processing tag: 'NoneType' object has no attribute 'find'\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "Error processing tag: 'NoneType' object has no attribute 'find'\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n"
     ]
    }
   ],
   "source": [
    "data_api = {\n",
    "    \"page\": 1, #14039\n",
    "    \"num\": 1,\n",
    "    \"field\":0,\n",
    "    \"expireccpl\": 2,\n",
    "    \"type\": 2\n",
    "}\n",
    "file_note = []\n",
    "data_type1 = []\n",
    "data_type7 = []\n",
    "data_type3 = []\n",
    "url, headers = get_api()\n",
    "response = post_request_key(url, headers, data_api)\n",
    "\n",
    "if response:\n",
    "    specific_page = response.get('total')\n",
    "    print(specific_page)\n",
    "now = ((136127)//100)+1\n",
    "num = 100\n",
    "total_page = specific_page // num + ( 1 if specific_page % num > 0 else 0)\n",
    "for num_page in range(now, total_page + 1):\n",
    "# for num_page in range(total_page, 0, - 1):\n",
    "    data_api = {\n",
    "        \"page\": num_page, #14039\n",
    "        \"num\": num,\n",
    "        \"field\":0,\n",
    "        \"expireccpl\": 2,\n",
    "        \"type\": 2\n",
    "    }\n",
    "    url1 = 'https://apids.thuvienphapluat.vn/auth/get-token?key=pvaG4gRG9lIiwiaWF0IjoxNTE2MjM5MDIyfQS563xADXH'\n",
    "    headers1 = {\n",
    "        'Cookie': 'Culture=vi; Culture=vi'\n",
    "    }\n",
    "\n",
    "    key = post_request(url1, headers1)\n",
    "    url_post = \"https://apids.thuvienphapluat.vn/crud/log-extracted-news-data\"\n",
    "\n",
    "    headers_post = {\n",
    "    'Authorization': 'Bearer ' + key['Data']['AccessToken'],\n",
    "    'Content-Type': 'application/json',\n",
    "    'Cookie': 'Culture=vi'\n",
    "    }\n",
    "\n",
    "    url, headers = get_api()\n",
    "    response = post_request_key(url, headers, data_api)\n",
    "    # print(response.get('data'))\n",
    "    data_input = response.get('data')\n",
    "    for temp in data_input:\n",
    "        # print(temp)\n",
    "        # print(temp[\"content\"])\n",
    "        # print(temp['title'])\n",
    "        extracted_data = extract_data(temp[\"content\"])\n",
    "        # print(extract_data(temp[\"content\"]))\n",
    "        # ccpls_flags = True\n",
    "        if extracted_data != [] and not re.search(r'\\b(?:dự thảo|đề xuất|sắp tới|dự kiến|đáp án|tra cứu điểm thi)\\b', temp[\"title\"].lower(), flags=re.IGNORECASE):\n",
    "            # for ccpl_check in extracted_data:\n",
    "            #     if not ccpl_check.get('ccpls'):\n",
    "            #         ccpls_flags = False\n",
    "            #         break\n",
    "            # if not ccpls_flags:    \n",
    "            #     dictionary = {\n",
    "            #         \"objid\": int(temp['obj_id']),\n",
    "            #         \"source\": str(temp['obj_code']),\n",
    "            #         \"data\": json.dumps(extracted_data, ensure_ascii=False),\n",
    "            #             # extracted_data,\n",
    "            #         \"type\": 7\n",
    "            #     }\n",
    "            #     dictionary = json.dumps(dictionary, ensure_ascii=False)\n",
    "            #     data_type7.append(int(temp['obj_id']))\n",
    "            #     response = requests.request(\"POST\", url_post, headers=headers_post, data=dictionary)\n",
    "            #     print(response.text)\n",
    "            # else:\n",
    "            #     # for temp_extract_data in extracted_data:\n",
    "            #     #     if not temp_extract_data.get('ccpls'):\n",
    "            #     #         temp_extract_data['ccpls'] = result_ccpl(temp_extract_data.get('answer'), get_items())\n",
    "            dictionary = {\n",
    "                \"objid\": int(temp['obj_id']),\n",
    "                \"source\": str(temp['obj_code']),\n",
    "                \"data\": json.dumps(extracted_data, ensure_ascii=False),\n",
    "                    # extracted_data,\n",
    "                \"type\": 1\n",
    "            }\n",
    "            dictionary = json.dumps(dictionary, ensure_ascii=False)\n",
    "            data_type1.append(int(temp['obj_id']))\n",
    "            response = requests.request(\"POST\", url_post, headers=headers_post, data=dictionary)\n",
    "            print(response.text)\n",
    "        elif extracted_data != [] and re.search(r'\\b(?:dự thảo|đề xuất|sắp tới|dự kiến|đáp án|tra cứu điểm thi)\\b', temp[\"title\"].lower(), flags=re.IGNORECASE):\n",
    "            dictionary = {\n",
    "                \"objid\": int(temp['obj_id']),\n",
    "                \"source\": str(temp['obj_code']),\n",
    "                \"data\": json.dumps(extracted_data, ensure_ascii=False),\n",
    "                \"type\": 3\n",
    "            }\n",
    "            dictionary = json.dumps(dictionary, ensure_ascii=False)\n",
    "            data_type3.append(int(temp['obj_id']))\n",
    "            response = requests.request(\"POST\", url_post, headers=headers_post, data=dictionary)\n",
    "            print(response.text)\n",
    "        else:\n",
    "            file_note.append(int(temp['obj_id']))\n",
    "            # print(dictionary)\n",
    "            # time.sleep(1)\n",
    "now = specific_page\n",
    "with open('PL_type1_l3.txt', 'w') as file:\n",
    "    for item in data_type1:\n",
    "        file.write(f\"{item}\\n\")\n",
    "with open('PL_type3_l3.txt', 'w') as file:\n",
    "    for item in data_type3:\n",
    "        file.write(f\"{item}\\n\")\n",
    "with open('PL_type7_l3.txt', 'w') as file:\n",
    "    for item in data_type7:\n",
    "        file.write(f\"{item}\\n\")\n",
    "with open('PL_file_note_l3.txt', 'w') as file:\n",
    "    for item in file_note:\n",
    "        file.write(f\"{item}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('PL_type1_l3.txt', 'w') as file:\n",
    "    for item in data_type1:\n",
    "        file.write(f\"{item}\\n\")\n",
    "with open('PL_type3_l3.txt', 'w') as file:\n",
    "    for item in data_type3:\n",
    "        file.write(f\"{item}\\n\")\n",
    "with open('PL_type7_l3.txt', 'w') as file:\n",
    "    for item in data_type7:\n",
    "        file.write(f\"{item}\\n\")\n",
    "with open('PL_file_note_l3.txt', 'w') as file:\n",
    "    for item in file_note:\n",
    "        file.write(f\"{item}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Minh-AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
