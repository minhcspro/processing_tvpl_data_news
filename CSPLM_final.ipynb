{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "import json\n",
    "from bs4 import BeautifulSoup, NavigableString,Tag\n",
    "from tabulate import tabulate\n",
    "import unicodedata\n",
    "import time\n",
    "import requests\n",
    "from markdownify import markdownify as md\n",
    "import base64\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test ccpl mới"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format các dạng bảng\n",
    "def table_to_markdown(table):\n",
    "    if table is None:\n",
    "        return \"\"\n",
    "    rows = []\n",
    "    headers = []\n",
    "\n",
    "    for row in table.find_all('tr'):\n",
    "        row_data = []\n",
    "        for cell in row.find_all(['th', 'td']):\n",
    "            cell_text = cell.get_text(strip=True)\n",
    "            row_data.append(cell_text)\n",
    "        if not headers:\n",
    "            headers = row_data\n",
    "        else:\n",
    "            rows.append(row_data)\n",
    "\n",
    "    return tabulate(rows, headers, tablefmt='pipe', disable_numparse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ Điều 1, 2, 3 + replace\n",
    "import re\n",
    "\n",
    "def split_by_dieu_new1(text):\n",
    "    # def extract_dieu_positions(text):\n",
    "    if text is None:\n",
    "        return[text]\n",
    "\n",
    "        \n",
    "    dieu_positions = [(match.start(), match.end()) for match in re.finditer(r'Điều\\s+\\d+[a-zA-Z0-9đĐ]*(?:,\\s*\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*)*(?:\\s+và\\s+\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*)?', text, re.IGNORECASE)]\n",
    "    if not dieu_positions:\n",
    "        return [text]\n",
    "\n",
    "    split_text = []\n",
    "    current_position = 0\n",
    "\n",
    "    for start, end in dieu_positions:\n",
    "        if current_position < start:\n",
    "            split_text.append(text[current_position:start].strip())\n",
    "        split_text.append(text[start:end].strip())\n",
    "        current_position = end\n",
    "\n",
    "    if current_position < len(text):\n",
    "        split_text.append(text[current_position:].strip())\n",
    "\n",
    "    return split_text\n",
    "################\n",
    "def extract_dieu_err(text):\n",
    "    # Define the pattern to match \"Điều\" followed by numbers and possibly letters or other characters\n",
    "    pattern = r'Điều\\s+\\d+[a-zA-Z0-9đĐ]*(?:,\\s*\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*)*(?:\\s+và\\s+\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*)?'\n",
    "\n",
    "    # Find all matches of the pattern in the text\n",
    "    matches = list(re.finditer(pattern, text, re.IGNORECASE))\n",
    "    matched_phrases = [match.group(0) for match in matches]\n",
    "    check_phrases = []\n",
    "\n",
    "    # Loop through each phrase in matched_phrases\n",
    "    for phrase in matched_phrases:\n",
    "        if ',' in phrase or ' và ' in phrase:\n",
    "            check_phrases.append(phrase)\n",
    "    return check_phrases\n",
    "##############\n",
    "def process_dieu_parts(text):\n",
    "    dieu_final = []\n",
    "    for match in text:\n",
    "        if ',' in match or ' và ' in match:\n",
    "            splitted = re.split(r',| và ', match)\n",
    "            for temp in splitted:\n",
    "                if re.match(r'(?i)Điều\\s+\\d+', temp):\n",
    "                    dieu_final.append(temp)\n",
    "                else:\n",
    "                    dieu_final.append('Điều ' + temp)\n",
    "\n",
    "    return dieu_final\n",
    "###############\n",
    "def replace_dieu_in_text(text):\n",
    "    if text is None:\n",
    "        return[text]\n",
    "    split_text = split_by_dieu_new1(text)\n",
    "    matches = extract_dieu_err(text)\n",
    "    if not matches:\n",
    "        return text\n",
    "    dieu_final = process_dieu_parts(matches)\n",
    "    replaced_text = []\n",
    "    for part in split_text:\n",
    "        if re.match(r'(?i)Điều\\s+\\d+', part) and \" và \" in part:\n",
    "            for dieu in dieu_final:\n",
    "                replaced_text.append(dieu)\n",
    "        else:\n",
    "            replaced_text.append(part)\n",
    "\n",
    "    return ' '.join(replaced_text)\n",
    "\n",
    "#############\n",
    "#############\n",
    "########### khoản 1, 2,3  và .... + replace\n",
    "\n",
    "\n",
    "import re\n",
    "\n",
    "def split_by_khoan_new1(text):\n",
    "    if text is None:\n",
    "        return[text]       \n",
    "    khoan_positions = [(match.start(), match.end()) for match in re.finditer(r'khoản\\s+((?:[a-zA-Z0-9đĐ]+(?:\\.\\d+)?(?:,\\s*| và |, và |))*[a-zA-Z0-9đĐ]+(?:\\.\\d+)?)(?=\\s|,|\\.|;|$)', text, re.IGNORECASE)]\n",
    "    if not khoan_positions:\n",
    "        return [text]\n",
    "\n",
    "    split_text = []\n",
    "    current_position = 0\n",
    "\n",
    "    for start, end in khoan_positions:\n",
    "        if current_position < start:\n",
    "            split_text.append(text[current_position:start].strip())\n",
    "        split_text.append(text[start:end].strip())\n",
    "        current_position = end\n",
    "\n",
    "    if current_position < len(text):\n",
    "        split_text.append(text[current_position:].strip())\n",
    "\n",
    "    return split_text\n",
    "\n",
    "#####################\n",
    "def extract_khoan_err(text):\n",
    "    # Define the pattern to match \"khoản\" followed by numbers and possibly letters or other characters\n",
    "    pattern = r'khoản\\s+((?:[a-zA-Z0-9đĐ]+(?:\\.\\d+)?(?:,\\s*| và |, và |))*[a-zA-Z0-9đĐ]+(?:\\.\\d+)?)(?=\\s|,|\\.|;|$)'\n",
    "\n",
    "    # Find all matches of the pattern in the text\n",
    "    matches = list(re.finditer(pattern, text, re.IGNORECASE))\n",
    "    matched_phrases = [match.group(0) for match in matches]\n",
    "    check_phrases = []\n",
    "\n",
    "    # Loop through each phrase in matched_phrases\n",
    "    for phrase in matched_phrases:\n",
    "        if ',' in phrase or ' và ' in phrase:\n",
    "            check_phrases.append(phrase)\n",
    "    return check_phrases\n",
    "################\n",
    "def process_khoan_parts(text):\n",
    "    khoan_final = []\n",
    "    for match in text:\n",
    "        if ',' in match or ' và ' in match:\n",
    "            splitted = re.split(r',| và ', match)\n",
    "            for temp in splitted:\n",
    "                if re.match(r'(?i)khoản\\s+((?:[a-zA-Z0-9đĐ]+(?:\\.\\d+)?)+)', temp):\n",
    "                    khoan_final.append(temp)\n",
    "                else:\n",
    "                    khoan_final.append('khoản ' + temp)\n",
    "\n",
    "    return khoan_final\n",
    "###############\n",
    "\n",
    "def replace_khoan_in_text(text):\n",
    "    if text is None:\n",
    "        return[text]\n",
    "    split_text = split_by_khoan_new1(text)\n",
    "    matches = extract_khoan_err(text)\n",
    "    if not matches:\n",
    "        return text\n",
    "    khoan_final = process_khoan_parts(matches)\n",
    "    replaced_text = []\n",
    "    for part in split_text:\n",
    "        if re.match(r'(?i)khoản\\s+((?:[a-zA-Z0-9đĐ]+(?:\\.\\d+)?)+)', part) and \" và \" in part:\n",
    "            for khoan in khoan_final:\n",
    "                replaced_text.append(khoan)\n",
    "        else:\n",
    "            replaced_text.append(part)\n",
    "\n",
    "    return ' '.join(replaced_text)\n",
    "#############\n",
    "#############\n",
    "################# Điểm 1,2,3 và .... + replace\n",
    "import re\n",
    "\n",
    "def split_by_diem_new1(text):\n",
    "    def extract_diem_positions(text):\n",
    "        return [(match.start(), match.end()) for match in re.finditer(r'điểm\\s+((?:[a-zA-Z0-9đĐ]+(?:\\.\\d+)?(?:,\\s+| và |, và |))*[a-zA-Z0-9đĐ]+(?:\\.\\d+)?)(?=\\s|,|\\.|;|$|\\s+và\\s+[a-zA-ZđĐ0-9])', text, re.IGNORECASE)]\n",
    "        \n",
    "    diem_positions = extract_diem_positions(text)\n",
    "    if not diem_positions:\n",
    "        return [text]\n",
    "\n",
    "    split_text = []\n",
    "    current_position = 0\n",
    "\n",
    "    for start, end in diem_positions:\n",
    "        if current_position < start:\n",
    "            split_text.append(text[current_position:start].strip())\n",
    "        split_text.append(text[start:end].strip())\n",
    "        current_position = end\n",
    "\n",
    "    if current_position < len(text):\n",
    "        split_text.append(text[current_position:].strip())\n",
    "\n",
    "    return split_text\n",
    "###########################\n",
    "def extract_diem_err(text):\n",
    "    # Define the pattern to match \"điểm\" followed by numbers and possibly letters or other characters\n",
    "    pattern = r'điểm\\s+((?:[a-zA-Z0-9đĐ]+(?:\\.\\d+)?(?:,\\s+| và |, và |))*[a-zA-Z0-9đĐ]+(?:\\.\\d+)?)(?=\\s|,|\\.|;|$|\\s+và\\s+[a-zA-ZđĐ0-9])'\n",
    "\n",
    "    # Find all matches of the pattern in the text\n",
    "    matches = list(re.finditer(pattern, text, re.IGNORECASE))\n",
    "    matched_phrases = [match.group(0) for match in matches]\n",
    "    check_phrases = []\n",
    "\n",
    "    # Loop through each phrase in matched_phrases\n",
    "    for phrase in matched_phrases:\n",
    "        if ',' in phrase or ' và ' in phrase:\n",
    "            check_phrases.append(phrase)\n",
    "    return check_phrases\n",
    "#########################\n",
    "def process_diem_parts(text):\n",
    "    diem_final = []\n",
    "    for match in text:\n",
    "        if ',' in match or ' và ' in match:\n",
    "            splitted = re.split(r',| và ', match)\n",
    "            for temp in splitted:\n",
    "                if re.match(r'(?i)điểm\\s+((?:[a-zA-Z0-9đĐ]+(?:\\.\\d+)?)+)', temp):\n",
    "                    diem_final.append(temp)\n",
    "                else:\n",
    "                    diem_final.append('điểm ' + temp)\n",
    "\n",
    "    return diem_final\n",
    "#########################\n",
    "def replace_diem_in_text(text):\n",
    "    if text is None:\n",
    "        return[text]\n",
    "    split_text = split_by_diem_new1(text)\n",
    "    matches = extract_diem_err(text)\n",
    "    if not matches:\n",
    "        return text\n",
    "    diem_final = process_diem_parts(matches)\n",
    "    replaced_text = []\n",
    "    for part in split_text:\n",
    "        if re.match(r'(?i)điểm\\s+((?:[a-zA-Z0-9đĐ]+(?:\\.\\d+)?)+)', part) and \" và \" in part:\n",
    "            for diem in diem_final:\n",
    "                replaced_text.append(diem)\n",
    "        else:\n",
    "            replaced_text.append(part)\n",
    "\n",
    "    return ' '.join(replaced_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_and_transform_dieu_ranges(text):\n",
    "    if text is None:\n",
    "        return [text]\n",
    "\n",
    "    dieu_positions = [(match.start(), match.end()) for match in re.finditer(r'Điều\\s+\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*', text, re.IGNORECASE)]\n",
    "\n",
    "    if not dieu_positions:\n",
    "        return [text]\n",
    "\n",
    "    new_split_text = []\n",
    "    current_position = 0\n",
    "    start_dieu = None\n",
    "\n",
    "    for i, (start, end) in enumerate(dieu_positions):\n",
    "        if current_position < start:\n",
    "            split_text_part = text[current_position:end].strip()\n",
    "            if \"từ Điều\" in split_text_part:\n",
    "                start_dieu_match = re.search(r'Điều\\s+(\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*)', split_text_part, re.IGNORECASE)\n",
    "                if start_dieu_match:\n",
    "                    start_dieu = start_dieu_match.group(1)\n",
    "            elif \"đến Điều\" in split_text_part or \"tới Điều\" in split_text_part:\n",
    "                end_dieu_match = re.search(r'Điều\\s+(\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*)', split_text_part, re.IGNORECASE)\n",
    "                if end_dieu_match:\n",
    "                    end_dieu = end_dieu_match.group(1)\n",
    "                    start_index = int(re.sub(\"[a-zA-ZđĐ]\", \"\", start_dieu)) if start_dieu else None\n",
    "                    end_index = int(re.sub(\"[a-zA-ZđĐ]\", \"\", end_dieu)) if end_dieu else None\n",
    "                    if start_index is not None and end_index is not None:\n",
    "                        for d in range(start_index, end_index + 1):\n",
    "                            new_split_text.append(f\"Điều {d}\")\n",
    "            else:\n",
    "                new_split_text.append(split_text_part)\n",
    "\n",
    "        current_position = end\n",
    "\n",
    "    if current_position < len(text):\n",
    "        split_text_part = text[current_position:].strip()\n",
    "        if \"từ Điều\" in split_text_part:\n",
    "            start_dieu_match = re.search(r'Điều\\s+(\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*)', split_text_part, re.IGNORECASE)\n",
    "            if start_dieu_match:\n",
    "                start_dieu = start_dieu_match.group(1)\n",
    "        elif \"đến Điều\" in split_text_part or \"tới Điều\" in split_text_part:\n",
    "            end_dieu_match = re.search(r'Điều\\s+(\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*)', split_text_part, re.IGNORECASE)\n",
    "            if end_dieu_match:\n",
    "                end_dieu = end_dieu_match.group(1)\n",
    "                start_index = int(re.sub(\"[a-zA-ZđĐ]\", \"\", start_dieu)) if start_dieu else None\n",
    "                end_index = int(re.sub(\"[a-zA-ZđĐ]\", \"\", end_dieu)) if end_dieu else None\n",
    "                if start_index is not None and end_index is not None:\n",
    "                    for d in range(start_index, end_index + 1):\n",
    "                        new_split_text.append(f\"Điều {d}\")\n",
    "        else:\n",
    "            new_split_text.append(split_text_part)\n",
    "\n",
    "    return new_split_text\n",
    "\n",
    "def replace_dieu_ranges(text):\n",
    "    # Tìm khoảng \"từ Điều X đến Điều Y\" hoặc \"từ Điều X tới Điều Y\"\n",
    "    if text is None:\n",
    "        return[text]\n",
    "    dieu_range_match = re.search(r'từ Điều \\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)* (?:đến|tới) Điều \\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*', text, re.IGNORECASE)\n",
    "    if not dieu_range_match:\n",
    "        return text\n",
    "\n",
    "    dieu_range_text = dieu_range_match.group(0)\n",
    "    dieu_list = split_and_transform_dieu_ranges(dieu_range_text)\n",
    "    \n",
    "    # Tạo chuỗi mới từ danh sách các Điều\n",
    "    dieu_list_str = ', '.join(dieu_list)\n",
    "\n",
    "    # Thay thế đoạn văn bản \"từ Điều X đến Điều Y\" hoặc \"từ Điều X tới Điều Y\" bằng danh sách các Điều\n",
    "    new_text = text.replace(dieu_range_text, dieu_list_str)\n",
    "\n",
    "    return new_text\n",
    "\n",
    "def split_and_transform_khoan_ranges(text):\n",
    "    if text is None:\n",
    "        return [text]\n",
    "        \n",
    "\n",
    "    khoan_positions = [(match.start(), match.end()) for match in re.finditer(r'khoản\\s+\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*', text, re.IGNORECASE)]\n",
    "    if not khoan_positions:\n",
    "        return [text]\n",
    "\n",
    "    new_split_text = []\n",
    "    current_position = 0\n",
    "    start_khoan = None\n",
    "\n",
    "    for i, (start, end) in enumerate(khoan_positions):\n",
    "        if current_position < start:\n",
    "            split_text_part = text[current_position:end].strip()\n",
    "            if \"từ khoản\" in split_text_part.lower():\n",
    "                start_khoan = re.search(r'khoản\\s+(\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*)', split_text_part, re.IGNORECASE).group(1)\n",
    "            elif \"đến khoản\" in split_text_part.lower() or \"tới khoản\" in split_text_part.lower():\n",
    "                end_khoan = re.search(r'khoản\\s+(\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*)', split_text_part, re.IGNORECASE).group(1)\n",
    "                start_index = int(re.sub(\"[a-zA-ZđĐ]\", \"\", start_khoan))\n",
    "                end_index = int(re.sub(\"[a-zA-ZđĐ]\", \"\", end_khoan))\n",
    "                for d in range(start_index, end_index + 1):\n",
    "                    new_split_text.append(f\"khoản {d}\")\n",
    "            else:\n",
    "                new_split_text.append(split_text_part)\n",
    "        current_position = end\n",
    "\n",
    "    if current_position < len(text):\n",
    "        split_text_part = text[current_position:].strip()\n",
    "        if \"từ khoản\" in split_text_part.lower():\n",
    "            start_khoan = re.search(r'khoản\\s+(\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*)', split_text_part, re.IGNORECASE).group(1)\n",
    "        elif \"đến khoản\" in split_text_part.lower() or \"tới khoản\" in split_text_part.lower():\n",
    "            end_khoan = re.search(r'khoản\\s+(\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*)', split_text_part, re.IGNORECASE).group(1)\n",
    "            start_index = int(re.sub(\"[a-zA-ZđĐ]\", \"\", start_khoan))\n",
    "            end_index = int(re.sub(\"[a-zA-ZđĐ]\", \"\", end_khoan))\n",
    "            for d in range(start_index, end_index + 1):\n",
    "                new_split_text.append(f\"khoản {d}\")\n",
    "        else:\n",
    "            new_split_text.append(split_text_part)\n",
    "\n",
    "    return new_split_text\n",
    "\n",
    "def replace_khoan_ranges(text):\n",
    "    # Tìm khoảng \"từ khoản X đến khoản Y\" hoặc \"từ khoản X tới khoản Y\"\n",
    "    if text is None:\n",
    "        return[text]\n",
    "    khoan_range_matches = re.finditer(r'từ khoản \\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)* (?:đến|tới) khoản \\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*', text, re.IGNORECASE)\n",
    "    \n",
    "    new_text = text\n",
    "    offset = 0\n",
    "    \n",
    "    for match in khoan_range_matches:\n",
    "        khoan_range_text = match.group(0)\n",
    "        khoan_list = split_and_transform_khoan_ranges(khoan_range_text)\n",
    "        \n",
    "        # Tạo chuỗi mới từ danh sách các khoản\n",
    "        khoan_list_str = ', '.join(khoan_list)\n",
    "\n",
    "        # Tính toán vị trí mới sau khi thay thế\n",
    "        start, end = match.start() + offset, match.end() + offset\n",
    "        new_text = new_text[:start] + khoan_list_str + new_text[end:]\n",
    "        \n",
    "        # Cập nhật offset\n",
    "        offset += len(khoan_list_str) - len(khoan_range_text)\n",
    "\n",
    "    return new_text\n",
    "\n",
    "def split_and_transform_diem_ranges(text):\n",
    "    if text is None:\n",
    "        return [text]\n",
    "        \n",
    "\n",
    "    diem_positions = [(match.start(), match.end()) for match in re.finditer(r'điểm\\s+\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*', text, re.IGNORECASE)]\n",
    "    if not diem_positions:\n",
    "        return [text]\n",
    "\n",
    "    new_split_text = []\n",
    "    current_position = 0\n",
    "    start_diem = None\n",
    "\n",
    "    for i, (start, end) in enumerate(diem_positions):\n",
    "        if current_position < start:\n",
    "            split_text_part = text[current_position:end].strip()\n",
    "            if \"từ điểm\" in split_text_part.lower():\n",
    "                start_diem = re.search(r'điểm\\s+(\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*)', split_text_part, re.IGNORECASE).group(1)\n",
    "            elif \"đến điểm\" in split_text_part.lower() or \"tới điểm\" in split_text_part.lower():\n",
    "                end_diem = re.search(r'điểm\\s+(\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*)', split_text_part, re.IGNORECASE).group(1)\n",
    "                start_index = int(re.sub(\"[a-zA-ZđĐ]\", \"\", start_diem))\n",
    "                end_index = int(re.sub(\"[a-zA-ZđĐ]\", \"\", end_diem))\n",
    "                for d in range(start_index, end_index + 1):\n",
    "                    new_split_text.append(f\"điểm {d}\")\n",
    "            else:\n",
    "                new_split_text.append(split_text_part)\n",
    "        current_position = end\n",
    "\n",
    "    if current_position < len(text):\n",
    "        split_text_part = text[current_position:].strip()\n",
    "        if \"từ điểm\" in split_text_part.lower():\n",
    "            start_diem = re.search(r'điểm\\s+(\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*)', split_text_part, re.IGNORECASE).group(1)\n",
    "        elif \"đến điểm\" in split_text_part.lower() or \"tới điểm\" in split_text_part.lower():\n",
    "            end_diem = re.search(r'điểm\\s+(\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*)', split_text_part, re.IGNORECASE).group(1)\n",
    "            start_index = int(re.sub(\"[a-zA-ZđĐ]\", \"\", start_diem))\n",
    "            end_index = int(re.sub(\"[a-zA-ZđĐ]\", \"\", end_diem))\n",
    "            for d in range(start_index, end_index + 1):\n",
    "                new_split_text.append(f\"điểm {d}\")\n",
    "        else:\n",
    "            new_split_text.append(split_text_part)\n",
    "\n",
    "    return new_split_text\n",
    "\n",
    "def replace_diem_ranges(text):\n",
    "    # Tìm khoảng \"từ điểm X đến điểm Y\" hoặc \"từ điểm X tới điểm Y\"\n",
    "    if text is None:\n",
    "        return[text]\n",
    "    diem_range_matches = re.finditer(r'từ điểm \\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)* (?:đến|tới) điểm \\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*', text, re.IGNORECASE)\n",
    "    \n",
    "    new_text = text\n",
    "    offset = 0\n",
    "    \n",
    "    for match in diem_range_matches:\n",
    "        diem_range_text = match.group(0)\n",
    "        diem_list = split_and_transform_diem_ranges(diem_range_text)\n",
    "        \n",
    "        # Tạo chuỗi mới từ danh sách các điểm\n",
    "        diem_list_str = ', '.join(diem_list)\n",
    "\n",
    "        # Tính toán vị trí mới sau khi thay thế\n",
    "        start, end = match.start() + offset, match.end() + offset\n",
    "        new_text = new_text[:start] + diem_list_str + new_text[end:]\n",
    "        \n",
    "        # Cập nhật offset\n",
    "        offset += len(diem_list_str) - len(diem_range_text)\n",
    "\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## dạng ..... 1, 2, 3 và .....,......\n",
    "def processing_ccpl_full(text):\n",
    "    if text is None:\n",
    "        return [text]  \n",
    "    check = replace_dieu_ranges(replace_khoan_ranges(replace_diem_ranges(text)))\n",
    "    check2 = replace_dieu_in_text(replace_khoan_in_text(replace_diem_in_text(check)))\n",
    "\n",
    "    return check2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Cắt từ điều này tới điều kia\n",
    "\n",
    "\n",
    "def split_by_dieu(text):\n",
    "    if text is None: \n",
    "        return[text]\n",
    "    \n",
    "    text = ' ' + text\n",
    "    dieu_positions = [(match.start(), match.end()) for match in re.finditer(r'Điều\\s+\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*', text, re.IGNORECASE)]\n",
    "    if not dieu_positions:\n",
    "        return [text]\n",
    "\n",
    "    split_text = []\n",
    "    current_position = 0\n",
    "    \n",
    "    for i, (start, end) in enumerate(dieu_positions):\n",
    "        if current_position < start:\n",
    "            split_text.append(text[current_position:end].strip())\n",
    "        current_position = end\n",
    "\n",
    "    if current_position < len(text):\n",
    "        split_text.append(text[current_position:].strip())\n",
    "\n",
    "    return split_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Cắt từ khoản này tới khoản kia\n",
    "\n",
    "def split_by_khoan(text):\n",
    "    if text is None:\n",
    "            return[text]\n",
    "\n",
    "    text = ' ' + text\n",
    "    khoan_positions = [(match.start(), match.end()) for match in re.finditer(r'khoản\\s+((?:[a-zA-Z0-9đĐ]+(?:\\.\\d+)?(?:,\\s*| và |, và |))*[a-zA-Z0-9đĐ]+(?:\\.\\d+)?)(?=\\s|,|\\.|;|$)', text, re.IGNORECASE)]\n",
    "    if not khoan_positions:\n",
    "        return [text]\n",
    "\n",
    "    split_text = []\n",
    "    current_position = 0\n",
    "\n",
    "    first_khoan_start = khoan_positions[0][0]\n",
    "    if first_khoan_start > 0:\n",
    "        split_text.append(text[:first_khoan_start].strip())\n",
    "\n",
    "    for i, (start, end) in enumerate(khoan_positions):\n",
    "        if current_position < start:\n",
    "            split_text.append(text[current_position:end].strip())\n",
    "        current_position = end\n",
    "\n",
    "    if current_position < len(text):\n",
    "        split_text.append(text[current_position:].strip())\n",
    "\n",
    "    return split_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hàm trích xuất điều\n",
    "def extract_dieu(text):\n",
    "    # Find individual \"Điều\" references\n",
    "    dieu_list = re.findall(r'Điều\\s+(\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*)', text, re.IGNORECASE)\n",
    "\n",
    "    # Find joined \"Điều\" lists\n",
    "    joined_dieu_list = re.findall(r'Điều\\s+\\d+[a-zA-Z0-9đĐ]*(?:,\\s*\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*)*(?:\\s+và\\s+\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*)?', text, re.IGNORECASE)\n",
    "\n",
    "    # Process joined \"Điều\" lists\n",
    "    for item in joined_dieu_list:\n",
    "        # Find all \"Điều\" references in the joined list\n",
    "        joined_dieu_refs = re.findall(r'\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*', item)\n",
    "        # Add only the unique \"Điều\" references to the main list\n",
    "        for ref in joined_dieu_refs:\n",
    "            if ref not in dieu_list:\n",
    "                dieu_list.append(ref)\n",
    "    return dieu_list\n",
    "\n",
    "# Hàm trích xuất khoản\n",
    "def extract_khoan(text):\n",
    "    pattern = r'khoản\\s+((?:[a-zA-Z0-9đĐ]+(?:\\.\\d+)?(?:,\\s*| và |, và |))*[a-zA-Z0-9đĐ]+(?:\\.\\d+)?)(?=\\s|,|\\.|;|$)'\n",
    "    matches = re.findall(pattern, text, re.IGNORECASE)\n",
    "    final_khoan_list = []\n",
    "    for match in matches:\n",
    "        parts = re.split(r',\\s*|\\s+và\\s+', match)\n",
    "        for part in parts:\n",
    "            part = part.strip().strip('.').lower()\n",
    "            if '-' in part:\n",
    "                start, end = part.split('-')\n",
    "                for idx in range(int(start), int(end) + 1):\n",
    "                    final_khoan_list.append(str(idx))\n",
    "            elif len(part) == 1 and part.isalpha() or part[0].isdigit():\n",
    "                final_khoan_list.append(part)\n",
    "    return final_khoan_list\n",
    "\n",
    "# Hàm trích xuất điểm\n",
    "def extract_diem(text):\n",
    "    pattern = r'điểm\\s+((?:[a-zA-Z0-9đĐ]+(?:\\.\\d+)?(?:,\\s+| và |, và |))*[a-zA-Z0-9đĐ]+(?:\\.\\d+)?)(?=\\s|,|\\.|;|$|\\s+và\\s+[a-zA-ZđĐ0-9])'\n",
    "    matches = re.findall(pattern, text, re.IGNORECASE)\n",
    "    final_diem_list = []\n",
    "    for match in matches:\n",
    "        parts = re.split(r',\\s*|\\s+và\\s+', match)\n",
    "        for part in parts:\n",
    "            part = part.strip().strip('.').lower()\n",
    "            if re.match(r'^[a-zA-ZđĐ]\\d*(\\.\\d+)*$', part) or re.match(r'^\\d+(\\.\\d+)*$', part):\n",
    "                final_diem_list.append(part)\n",
    "    return final_diem_list\n",
    "\n",
    "# hàm xử lý đầu ra ccpl\n",
    "def processing_output_ccpl(input_data):\n",
    "    output_array = []\n",
    "    for dieu in input_data:\n",
    "        dieu_value = \", \".join(dieu[\"Dieu\"])\n",
    "        if dieu[\"Khoan\"] == [[]]:\n",
    "            output_array.extend([\n",
    "                    dieu_value,\n",
    "                    \"0\",\n",
    "                    \"0\"\n",
    "                ])\n",
    "        elif dieu[\"Khoan\"]:\n",
    "            for khoan in dieu[\"Khoan\"]:\n",
    "                for khoan_item in khoan[\"Khoan\"]:\n",
    "                    if khoan[\"Diem\"] == [[]]:\n",
    "                        output_array.extend([\n",
    "                            dieu_value,\n",
    "                            khoan_item,\n",
    "                            \"0\"\n",
    "                        ])\n",
    "                    elif khoan[\"Diem\"]:\n",
    "                        for diem in khoan[\"Diem\"]:\n",
    "                            for diem_item in diem:\n",
    "                                output_array.extend([\n",
    "                                    dieu_value,\n",
    "                                    khoan_item,\n",
    "                                    diem_item\n",
    "                                ])\n",
    "                                # print(output_array)\n",
    "                    else:\n",
    "                        output_array.extend([\n",
    "                            dieu_value,\n",
    "                            khoan_item,\n",
    "                            \"0\"\n",
    "                        ])\n",
    "        else:\n",
    "            output_array.extend([\n",
    "                dieu_value,\n",
    "                \"0\",\n",
    "                \"0\"\n",
    "            ])\n",
    "    if output_array:\n",
    "        return output_array\n",
    "\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "# Function to remove duplicate ccpl entries\n",
    "def remove_duplicates_ccpls(data):\n",
    "    for item in data:\n",
    "        seen = set()\n",
    "        unique_ccpls = []\n",
    "        for ccpl in item['ccpls']:\n",
    "            if ccpl['Dieu']:  # Kiểm tra nếu 'Dieu' không phải là rỗng\n",
    "                ccpl_tuple = (ccpl['LawID'], ccpl['LawTitle'], ccpl['Dieu'], ccpl['Khoan'], ccpl['Diem'])\n",
    "                if ccpl_tuple not in seen:\n",
    "                    seen.add(ccpl_tuple)\n",
    "                    unique_ccpls.append(ccpl)\n",
    "        item['ccpls'] = unique_ccpls\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processing_redundant_data(data):\n",
    "    soup = BeautifulSoup(data, 'html.parser')\n",
    "\n",
    "    # Xóa dẫn link dạng bảng có chữ tải về\n",
    "    for p_tag in soup.find_all('table'):\n",
    "        try:\n",
    "            a_tag = p_tag.find('a')\n",
    "            # Tìm thẻ <img> bên trong thẻ <a>\n",
    "            img_tag = a_tag.find('img')\n",
    "            # Kiểm tra href và src có tồn tại không trước khi kiểm tra endswith\n",
    "            if (a_tag.get('href') and not a_tag.get('href').endswith(('.doc', '.docx', '.xls', '.xlsx', '.zip', '.rar', '.jpg', '.pdf', '.png', '.jpeg'))) and (img_tag and img_tag.get('src') and img_tag.get('src').endswith(('png', 'jpg', 'jpeg'))): \n",
    "                # Xử lý thẻ <p> nếu điều kiện thỏa mãn\n",
    "                # print(p_tag.text)\n",
    "                p_tag.decompose()\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing tag: {e}\")\n",
    "    \n",
    "    # script\n",
    "    # bỏ mấy bảng tham khảo màu đỏ\n",
    "    for p_tag in soup.find_all('script'):\n",
    "        p_tag.decompose()\n",
    "        \n",
    "    for tag in soup.find_all(attrs={\"type\": \"text/css\"}):\n",
    "        tag.decompose()\n",
    "    # Tìm tất cả các thẻ <p>\n",
    "    p_tags = soup.find_all('p')\n",
    "    # Kiểm tra từng thẻ <p> để xem có chứa chuỗi \"lưu ý: khi thực hiện cải cách tiền lương...\" không\n",
    "    for p_tag in p_tags:\n",
    "        if \"lưu ý: khi thực hiện cải cách tiền lương từ ngày 1/7/2024 sẽ bãi bỏ mức lương cơ sở và hệ số lương hiện nay, xây dựng bảng lương mới, theo mục 2 nghị quyết 27-nq/tw năm 2018\" in p_tag.get_text().lower():\n",
    "            p_tag.decompose()\n",
    "        if p_tag.get_text().lower().endswith('.html') or p_tag.get_text().lower().endswith('.htm'):\n",
    "            p_tag.decompose()\n",
    "        img = p_tag.find('img')\n",
    "        # Kiểm tra nếu thẻ img có thuộc tính style đúng như yêu cầu\n",
    "        if img and img.get('style') == 'height:400px; width:600px':\n",
    "            p_tag.decompose()  # Xóa thẻ <p> nếu điều kiện đúng\n",
    "            \n",
    "    # Tìm và xóa thẻ <img> có đuôi .gif\n",
    "    for img_tag in soup.select('p img'):\n",
    "        if re.search(r'\\.gif$', img_tag['src']):\n",
    "            img_tag.decompose()  # Xóa thẻ <img> khỏi cây HTML\n",
    "            \n",
    "    # for p_tag in soup.find_all('p', style=\"text-align:center\"):\n",
    "    #     for em_tag in p_tag.find_all('em'):\n",
    "    #         p_tag.decompose()\n",
    "                    \n",
    "    # xóa ảnh thumb và title ảnh\n",
    "    # Lặp qua tất cả các thẻ <p> để kiểm tra điều kiện\n",
    "    for i, p in enumerate(p_tags):\n",
    "        # Kiểm tra nếu thẻ <p> chứa <img> và thẻ <p> tiếp theo có style \"text-align: center\"\n",
    "        try:\n",
    "            if p.find('img') and i + 1 < len(p_tags) and ('text-align: center' in p_tags[i + 1].get('style', '') or p_tags[i + 1].get_text().lower().endswith('internet)')):\n",
    "                # or p_tags[i + 1].find('em')\n",
    "                # Xóa cả hai thẻ <p> hiện tại và thẻ <p> tiếp theo\n",
    "                p.decompose()\n",
    "                p_tags[i + 1].decompose()\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing tag: {e}\")\n",
    "            \n",
    "    # xóa nội dung bảng hết hiệu lực\n",
    "    for table_tag in soup.find_all('s'):\n",
    "        # print(table_tag.text)\n",
    "        table_tag.decompose()\n",
    "    \n",
    "    # bỏ mấy bảng tham khảo màu đỏ\n",
    "    for p_tag in soup.find_all('p'):\n",
    "        for strong_tag in p_tag.find_all('strong', style=\"color: rgb(250, 10, 10);\"):\n",
    "            p_tag.decompose()\n",
    "    \n",
    "    # Xóa chữ \"Xem thêm\"\n",
    "    for p_tag in soup.find_all('p'):\n",
    "        if p_tag.get_text(strip=True).lower() == '>>xem thêm mẫu:' or p_tag.get_text(strip=True).lower() == \"xem thêm:\" or p_tag.get_text(strip=True).lower() == \">> xem thêm:\":\n",
    "            p_tag.decompose()\n",
    "                   \n",
    "    # Xóa chữ \"Xem thêm\"\n",
    "    for p_tag in soup.find_all('p'):\n",
    "        if p_tag.get_text(strip=True).lower() == '>>xem thêm mẫu' or p_tag.get_text(strip=True).lower() == \"xem thêm\" or p_tag.get_text(strip=True).lower() == \">> xem thêm\":\n",
    "            p_tag.decompose()\n",
    "    \n",
    "    # Xóa chữ \"Về vấn đề này <span style=\"color:#FF0000;\"><strong>THƯ VIỆN PHÁP LUẬT</strong></span> giải đáp như sau:</p>\"\n",
    "    for p_tag in soup.find_all('p'):\n",
    "        for span_tag in p_tag.find_all('span'):\n",
    "            if span_tag.get_text(strip=True).lower() == \"thư viện pháp luật\" or span_tag.get_text(strip=True).lower() == \"pháp lý khởi nghiệp\" or span_tag.get_text(strip=True).lower() == \"pháp luật doanh nghiệp\":\n",
    "                p_tag.decompose()\n",
    "                \n",
    "    # Xóa chữ \"trân trọng!\"\n",
    "    for p_tag in soup.find_all('p'):\n",
    "        if p_tag.get_text(strip=True).lower() == \"trân trọng!\":\n",
    "            p_tag.decompose()\n",
    "        # Xóa chữ \"trân trọng!\"\n",
    "    for p_tag in soup.find_all('p'):\n",
    "        if p_tag.get_text(strip=True).lower() == \"trân trọng\":\n",
    "            p_tag.decompose()\n",
    "     # Xóa chữ \"trân trọng!\"\n",
    "    for p_tag in soup.find_all('p'):\n",
    "        if p_tag.get_text(strip=True).lower() == 'chúng tôi phản hồi thông tin đến bạn.':\n",
    "            p_tag.decompose()\n",
    "    # Xóa chữ \"Trên đây là một số thông tin chúng tôi cung cấp gửi tới bạn. Trân trọng!\"        \n",
    "    for p_tag in soup.find_all('p'):\n",
    "        if p_tag.get_text(strip=True).lower() == \"trên đây là một số thông tin chúng tôi cung cấp gửi tới bạn. trân trọng!\":\n",
    "            p_tag.decompose()\n",
    "    \n",
    "    # xóa tên tác giả\n",
    "    # <p style=\"text-align: right;\"><strong>Như Mai</strong></p>\n",
    "    for p_tag in soup.find_all('p', style=\"text-align: right;\"):\n",
    "        try:\n",
    "            strong_tag = p_tag.find('strong')\n",
    "            if strong_tag:  # Nếu tìm thấy thẻ <strong>\n",
    "                p_tag.decompose()  # Xóa toàn bộ thẻ <p>\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing tag: {e}\")\n",
    "            \n",
    "    for p_tag in soup.find_all('p', style=\"text-align: right;\"):\n",
    "        try:\n",
    "            strong_tag = p_tag.find('em')\n",
    "            if strong_tag:  # Nếu tìm thấy thẻ <strong>\n",
    "                p_tag.decompose()  # Xóa toàn bộ thẻ <p>\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing tag: {e}\")\n",
    "    for p_tag in soup.find_all('p', align=\"right\"):\n",
    "        try:\n",
    "            strong_tag = p_tag.find('strong')\n",
    "            if strong_tag:  # Nếu tìm thấy thẻ <strong>\n",
    "                p_tag.decompose()  # Xóa toàn bộ thẻ <p>\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing tag: {e}\")\n",
    "    for p_tag in soup.find_all('p', align=\"right\"):\n",
    "        try:\n",
    "            strong_tag = p_tag.find('em')\n",
    "            if strong_tag:  # Nếu tìm thấy thẻ <strong>\n",
    "                p_tag.decompose()  # Xóa toàn bộ thẻ <p>\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing tag: {e}\")\n",
    "    \n",
    "    # # chỉ duy nhất trang KN_FAQ và KN_JOB\n",
    "    # # Tìm tất cả các thẻ table\n",
    "    # tables = soup.find_all('table')\n",
    "\n",
    "    # # Duyệt qua từng thẻ table\n",
    "    # for table in tables:\n",
    "    #     # Tìm tất cả các thẻ td trong table\n",
    "    #     tds = table.find_all('td')\n",
    "        \n",
    "    #     # Nếu chỉ có duy nhất 1 thẻ td, thì xóa cả table\n",
    "    #     if len(tds) == 1:\n",
    "    #         table.decompose()\n",
    "    # Xử lý phần tình huống pháp lý chứa trong nội dung\n",
    "    # Tìm tất cả các thẻ <p> có chứa \"Trả lời:\"\n",
    "    answer_paragraphs = soup.find_all('p', string=lambda text: text == \"Trả lời:\")\n",
    "    # Lặp qua các thẻ <p> đã tìm được\n",
    "    for answer_p in answer_paragraphs:\n",
    "        # Tìm thẻ <h2> phía trên thẻ <p>\n",
    "        h2_above = answer_p.find_previous('h2')\n",
    "        \n",
    "        if h2_above:\n",
    "            # Lấy tất cả các phần tử từ <h2> tới <p> và xóa chúng (nhưng không xóa <h2>)\n",
    "            current_element = h2_above.find_next_sibling()\n",
    "            while current_element and current_element != answer_p:\n",
    "                next_element = current_element.find_next_sibling()\n",
    "                current_element.decompose()\n",
    "                current_element = next_element\n",
    "            # Xóa luôn thẻ <p> có chứa \"Trả lời:\"\n",
    "            answer_p.decompose()\n",
    "            \n",
    "    # xóa các link dẫn\n",
    "    for p_tag in soup.find_all('p'):\n",
    "        # Lặp qua tất cả các thẻ <a> bên trong mỗi thẻ <p>\n",
    "        for a_tag in p_tag.find_all('a', href=True):\n",
    "            if a_tag is not None:  # Đảm bảo a_tag không phải là None\n",
    "                try:\n",
    "                    href_value = a_tag.get('href')  # Sử dụng get để tránh lỗi\n",
    "                    if href_value and (href_value.endswith(('.html', '.htm')) or 'html?' in href_value):\n",
    "                        # print(p_tag.text)\n",
    "                        p_tag.decompose()  # Xóa thẻ <p> chứa thẻ <a> này\n",
    "                    # extensions_end = ('.doc', '.docx', '.xls', '.xlsx', '.zip', '.rar', '.jpg', '.pdf', '.png')\n",
    "                    extensions_start = ('>> xem chi tiết', 'xem chi tiết', '>> xem toàn bộ', 'xem toàn bộ', 'xem thêm', '>> xem thêm', 'xem chi tiết tại', '>> tham khảo', 'tham khảo', 'tải đầy đủ', '>>>')\n",
    "                    if href_value and href_value.startswith('http') and (not '.aspx' in href_value) and p_tag.get_text(strip=True).lower().startswith(extensions_start):\n",
    "                        p_tag.decompose()  # Xóa thẻ <p> chứa thẻ <a> này\n",
    "                except AttributeError as e:\n",
    "                    print(f\"Đã gặp lỗi AttributeError: {e}\")\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import uuid\n",
    "import base64\n",
    "\n",
    "def save_base64_image(base64_string, file_extension=\"png\", folder_name=\"img_base64_convert\"):\n",
    "    # Tạo thư mục nếu chưa tồn tại\n",
    "    if not os.path.exists(folder_name):\n",
    "        os.makedirs(folder_name)\n",
    "    \n",
    "    # Giải mã chuỗi base64\n",
    "    image_data = base64_string.split(',')[1]  # Tách phần dữ liệu base64\n",
    "    image_bytes = base64.b64decode(image_data)\n",
    "\n",
    "    # Tạo tên file ngẫu nhiên với phần mở rộng được chỉ định\n",
    "    file_name = f\"{uuid.uuid4()}.{file_extension}\"\n",
    "\n",
    "    # Đường dẫn đầy đủ để lưu file\n",
    "    file_path = os.path.join(folder_name, file_name)\n",
    "\n",
    "    # Lưu ảnh vào thư mục\n",
    "    with open(file_path, 'wb') as f:\n",
    "        f.write(image_bytes)\n",
    "\n",
    "    return file_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def processing_1(soup):\n",
    "    questions_and_answers1 = []  # Danh sách để lưu trữ các link và anchor cho từng đoạn h2\n",
    "    # Tìm tất cả các thẻ <h2>\n",
    "    h2_tags = []\n",
    "    for temp in soup.find_all('h2'):\n",
    "        # Kiểm tra nếu thẻ <h2> có chứa thẻ <strong> và không phải là thẻ trống hoặc chỉ chứa <br>\n",
    "        # if temp.find('strong') and temp.get_text(strip=True):\n",
    "        if temp.get_text(strip=True):\n",
    "            # and len(temp.get_text(strip=True)) >= 9:\n",
    "            h2_tags.append(temp)\n",
    "    # Lặp qua từng cặp thẻ <h2>\n",
    "    for i in range(len(h2_tags)):\n",
    "        h2_current = h2_tags[i]\n",
    "        if i + 1 < len(h2_tags):\n",
    "            h2_next = h2_tags[i + 1]\n",
    "        else:\n",
    "            h2_next = 'Xuan Vinh Gay'\n",
    "\n",
    "        patterns = r\"^\\d+\\.\\s*\"\n",
    "        # Tìm và thay thế phần (1) và phần 1. bằng chuỗi rỗng\n",
    "        second_part = re.sub(patterns, \"\", h2_current.text).strip()\n",
    "        second_part = unicodedata.normalize(\"NFC\", second_part)\n",
    "        # Lấy văn bản cho câu trả lời\n",
    "        answer_text = \"\"\n",
    "        next_element = h2_current.next_sibling\n",
    "        while next_element and (not isinstance(next_element, Tag) or (next_element.name != 'h2')):\n",
    "            if next_element and next_element.name == 'table':\n",
    "                a_tag = next_element.find('a')\n",
    "                # span_tag = next_element.find_all('span')\n",
    "                if a_tag:\n",
    "                    try:\n",
    "                        href = a_tag['href']\n",
    "                        # Danh sách các định dạng tài liệu\n",
    "                        file_extensions = ('.doc', '.docx', '.xls', '.xlsx', '.zip', '.rar', '.jpg', '.pdf', '.png', '.jpeg')\n",
    "                        # Kiểm tra nếu href kết thúc bằng một trong các định dạng tài liệu\n",
    "                        if href.endswith(file_extensions):\n",
    "                            # Tìm thẻ td chứa thẻ a\n",
    "                            td_tag = a_tag.find_parent('td')\n",
    "                            if td_tag:\n",
    "                                # Tìm thẻ td kế tiếp (ngang hàng)\n",
    "                                next_td = td_tag.find_next_sibling('td')\n",
    "                                if next_td:\n",
    "                                    link_text = next_td.get_text(strip=True)\n",
    "                                    # Thay thế thẻ a bằng văn bản Markdown\n",
    "                                    a_tag.replace_with(f\"[{link_text}]({href})\\n\")\n",
    "                                    answer_text += next_element.get_text() + \"\\n\"\n",
    "                        else:\n",
    "                            # Nếu href không phải tài liệu\n",
    "                            answer = unicodedata.normalize(\"NFC\", next_element.get_text())\n",
    "                            answer_text += answer + \"\\n\"\n",
    "                    except KeyError as e:\n",
    "                        print(f\"Thuộc tính 'href' không tồn tại trong thẻ <a>. Lỗi: {e}\")\n",
    "                else:\n",
    "                    # Nếu không tìm thấy thẻ a\n",
    "                    # print(table_to_markdown(next_element))\n",
    "                    answer_text += table_to_markdown(next_element) + \"\\n\"\n",
    "            \n",
    "            elif next_element and next_element.name == 'blockquote':\n",
    "                if next_element.find('img'):\n",
    "                    try:\n",
    "                        img_tag = next_element.find('img')\n",
    "                        src = img_tag['src']  \n",
    "                        # Kiểm tra xem ảnh có phải là base64\n",
    "                        if src.startswith('data:image'):\n",
    "                            alt_text = img_tag['alt'] if img_tag.has_attr('alt') else ''\n",
    "                            \n",
    "                            # Lưu ảnh base64 vào file và chọn định dạng file (ví dụ: 'png')\n",
    "                            file_name = save_base64_image(src, file_extension=\"png\", folder_name=\"img_base64_convert\")\n",
    "                            \n",
    "                            # Sử dụng tên file thay vì chuỗi base64\n",
    "                            answer_text += f\"![{alt_text}]({file_name})\\n\"\n",
    "                        else:\n",
    "                            alt_text = img_tag['alt'] if img_tag.has_attr('alt') else ''\n",
    "                            answer_text += f\"![{alt_text}]({src})\\n\"\n",
    "                    except KeyError as e:\n",
    "                        print(f\"Thuộc tính 'src' không tồn tại. Lỗi: {e}\")\n",
    "                elif isinstance(next_element, NavigableString):\n",
    "                    answer = unicodedata.normalize(\"NFC\", next_element)\n",
    "                    answer_text += answer.strip() + \"\\n\"\n",
    "                elif next_element and next_element.name:\n",
    "                    answer = unicodedata.normalize(\"NFC\", next_element.get_text())\n",
    "                    answer_text += answer.strip() + \"\\n\"\n",
    "            elif next_element and next_element.name == 'p':\n",
    "                if next_element.find('img'):\n",
    "                    img_tag = next_element.find('img')\n",
    "                    if img_tag:\n",
    "                        try:\n",
    "                            src = img_tag['src']\n",
    "\n",
    "                            # Kiểm tra chuỗi base64\n",
    "                            if src.startswith('data:image'):\n",
    "                                alt_text = img_tag['alt'] if img_tag.has_attr('alt') else ''\n",
    "                                \n",
    "                                # Lưu ảnh base64 với định dạng file cố định (ví dụ: 'png')\n",
    "                                file_name = save_base64_image(src, file_extension=\"png\", folder_name=\"img_base64_convert\")\n",
    "                                \n",
    "                                # Sử dụng tên file thay vì chuỗi base64\n",
    "                                answer_text += f\"![{alt_text}]({file_name})\\n\"\n",
    "                            else:\n",
    "                                alt_text = img_tag['alt'] if img_tag.has_attr('alt') else ''\n",
    "                                answer_text += f\"![{alt_text}]({src})\\n\"\n",
    "                        except KeyError as e:\n",
    "                            print(f\"Thuộc tính 'src' không tồn tại trong thẻ <img>. Lỗi: {e}\")            \n",
    "                elif next_element.find('a'):\n",
    "                    a_tag = next_element.find('a')\n",
    "                    if a_tag:\n",
    "                        try:\n",
    "                            href = a_tag['href']\n",
    "                            # Danh sách các định dạng tài liệu\n",
    "                            file_extensions = ('.doc', '.docx', '.xls', '.xlsx', '.zip', '.rar', '.jpg', '.pdf', '.png', '.jpeg')\n",
    "                            # Kiểm tra nếu href kết thúc bằng một trong các định dạng tài liệu\n",
    "                            if href.endswith(file_extensions):\n",
    "                                link_text = a_tag.text\n",
    "                                for drop_a_tag in next_element.find('a'):\n",
    "                                    drop_a_tag.replace_with(f\"[{link_text}]({href})\\n\")\n",
    "                                answer_text += next_element.get_text() + \"\\n\"\n",
    "                            else:\n",
    "                                answer = unicodedata.normalize(\"NFC\", next_element.get_text())\n",
    "                                answer_text += answer.strip() + \"\\n\"\n",
    "                        except KeyError as e:\n",
    "                            print(f\"Thuộc tính 'href' không tồn tại trong <link>. Lỗi: {e}\")\n",
    "                # Kiểm tra chuỗi văn bản thuần\n",
    "                elif isinstance(next_element, NavigableString):\n",
    "                    answer = unicodedata.normalize(\"NFC\", next_element)\n",
    "                    answer_text += answer.strip() + \"\\n\"\n",
    "                \n",
    "                # Kiểm tra thẻ khác và lấy văn bản\n",
    "                elif next_element and next_element.name:\n",
    "                    answer = unicodedata.normalize(\"NFC\", next_element.get_text())\n",
    "                    answer_text += answer.strip() + \"\\n\"\n",
    "            elif next_element and next_element.name == 'h3':\n",
    "                patterns = r'^\\d+(\\.\\d+)*\\.\\s*'\n",
    "                if isinstance(next_element, NavigableString):\n",
    "                    # Tìm và thay thế phần (1) và phần 1. bằng chuỗi rỗng\n",
    "                    text_strip = re.sub(patterns, \"\", next_element).strip()\n",
    "                    answer = unicodedata.normalize(\"NFC\", text_strip)\n",
    "                    answer_text += answer.strip() + \"\\n\"\n",
    "                elif next_element and next_element.name:\n",
    "                    text_strip = re.sub(patterns, \"\", next_element.get_text()).strip()\n",
    "                    answer = unicodedata.normalize(\"NFC\", text_strip)\n",
    "                    answer_text += answer.strip() + \"\\n\"\n",
    "            elif isinstance(next_element, NavigableString):\n",
    "                    answer = unicodedata.normalize(\"NFC\", next_element)\n",
    "                    answer_text += answer.strip() + \"\\n\"\n",
    "            elif next_element and next_element.name:\n",
    "                    answer = unicodedata.normalize(\"NFC\", next_element.get_text())\n",
    "                    answer_text += answer.strip() + \"\\n\"\n",
    "            next_element = next_element.next_sibling\n",
    "        \n",
    "        # Tìm tất cả các thẻ giữa h2_current và h2_next\n",
    "        siblings = []\n",
    "        next_tag = h2_current.find_next_sibling()\n",
    "        while next_tag and next_tag != h2_next:\n",
    "            siblings.append(next_tag)\n",
    "            next_tag = next_tag.find_next_sibling()\n",
    "        # Lặp qua các thẻ từ dưới lên trên trong các thẻ anh chị em và lưu kết quả tạm thời\n",
    "        temp_results = []  # Danh sách tạm thời để lưu trữ các kết quả\n",
    "        skip_next_p_check = True  # Biến cờ để kiểm tra thẻ <p> tiếp theo\n",
    "        for tag in reversed(siblings):\n",
    "            if tag.name == 'p':  # Kiểm tra nếu tag là thẻ <p>\n",
    "                p_tag = tag.text\n",
    "                # Kiểm tra xem nội dung của tag có bắt đầu bằng '(' và kết thúc bằng ')'\n",
    "                if tag and tag.find('a') and p_tag.strip().startswith('Căn cứ pháp lý:'):\n",
    "                    if skip_next_p_check:\n",
    "                        for a_tag in tag.find_all('a'):\n",
    "                            href = a_tag.get('href')\n",
    "                            if href and '.aspx' in href and 'thuvienphapluat.vn' in href:\n",
    "                                final = processing_ccpl_full(a_tag.text)\n",
    "                                positions = split_by_dieu(final)\n",
    "                                # test = p_tag.split(positions)\n",
    "                                # print(positions)\n",
    "                                if positions:\n",
    "                                    lawtitle = positions[-1]\n",
    "                                else:\n",
    "                                    lawtitle = \"default_value\"  # Hoặc một giá trị mặc định nếu positions rỗng\n",
    "\n",
    "                                # Kiểm tra nếu lawtitle không rỗng trước khi gọi split()\n",
    "                                if lawtitle:\n",
    "                                    test = p_tag.split(lawtitle)\n",
    "                                else:\n",
    "                                    # Xử lý khi lawtitle rỗng (nếu cần thiết)\n",
    "                                    test = []\n",
    "                                anchor_match = re.search(r'anchor=(\\w+)', href)\n",
    "                                anchor = anchor_match.group(1) if anchor_match else \"0\"\n",
    "                                if test and \"điều\" in test[0].lower():\n",
    "                                    p_tag = p_tag.replace(test[0], '', 1)\n",
    "                                    final = processing_ccpl_full(test[0])\n",
    "                                    positions = split_by_dieu(final)\n",
    "                                    data_ccpl = []\n",
    "                                    for dieu in positions:\n",
    "                                            if \"điều\" in dieu.lower():\n",
    "                                                dieu_dict = {\"Dieu\": extract_dieu(dieu), \"Khoan\": []}\n",
    "                                                # Tìm anchor trong href\n",
    "                                                khoan_list = split_by_khoan(dieu)\n",
    "                                                for khoan in khoan_list:\n",
    "                                                    if \"khoản\" in khoan.lower():\n",
    "                                                        khoan_dict = {\"Khoan\": extract_khoan(khoan), \"Diem\": []}\n",
    "                                                        if \"điểm\" in khoan.lower():\n",
    "                                                            diem_dict = extract_diem(khoan)\n",
    "                                                            khoan_dict[\"Diem\"].append(diem_dict)\n",
    "                                                        dieu_dict[\"Khoan\"].append(khoan_dict)\n",
    "                                                data_ccpl.append(dieu_dict)\n",
    "                                                # print(data_ccpl)\n",
    "                                                # Tạo một từ điển mới và gán giá trị từ mảng\n",
    "                                                ccpl_temp = processing_output_ccpl(data_ccpl)\n",
    "                                                if len(ccpl_temp) > 3:\n",
    "                                                    for i in range(0, len(ccpl_temp), 3):\n",
    "                                                        # Lấy 3 phần tử từ chỉ mục i đến i+3 (không bị lỗi nếu kích thước không đủ)\n",
    "                                                        chunk = ccpl_temp[i:i+3]\n",
    "                                                        anchor_match = re.search(r'anchor=(\\w+)', href)\n",
    "                                                        law_id_match = re.search(r'-(\\d+)\\.aspx', href)\n",
    "                                                        anchor = anchor_match.group(1) if anchor_match else \"0\"\n",
    "                                                        law_id = law_id_match.group(1) if law_id_match else \"0\"\n",
    "                                                        # print(ccpl_temp)\n",
    "                                                        if len(chunk) > 0:\n",
    "                                                            dictionary = {\n",
    "                                                                \"url\": href,\n",
    "                                                                \"anchor\": anchor,\n",
    "                                                                \"LawID\": law_id,\n",
    "                                                                \"LawTitle\": lawtitle,\n",
    "                                                                \"Dieu\": chunk[0] or \"0\",\n",
    "                                                                \"Khoan\": chunk[1] or \"0\",\n",
    "                                                                \"Diem\": chunk[2] or \"0\"\n",
    "                                                            }\n",
    "                                                            temp_results.append(dictionary)\n",
    "                                                else:\n",
    "                                                    anchor_match = re.search(r'anchor=(\\w+)', href)\n",
    "                                                    law_id_match = re.search(r'-(\\d+)\\.aspx', href)\n",
    "                                                    anchor = anchor_match.group(1) if anchor_match else \"0\"\n",
    "                                                    law_id = law_id_match.group(1) if law_id_match else \"0\"\n",
    "                                                    if len(ccpl_temp) > 0:\n",
    "                                                        dictionary = {\n",
    "                                                            \"url\": href,\n",
    "                                                            \"anchor\": anchor,\n",
    "                                                            \"LawID\": law_id,\n",
    "                                                            \"LawTitle\": lawtitle,\n",
    "                                                            \"Dieu\": ccpl_temp[0] or \"0\",\n",
    "                                                            \"Khoan\": ccpl_temp[1] or \"0\",\n",
    "                                                            \"Diem\": ccpl_temp[2] or \"0\"\n",
    "                                                        }\n",
    "                                                        temp_results.append(dictionary)\n",
    "\n",
    "                                else:\n",
    "                                    temp_else = {}\n",
    "                                    anchor_match = re.search(r'anchor=(\\w+)', href)\n",
    "                                    law_id_match = re.search(r'-(\\d+)\\.aspx', href)\n",
    "                                    anchor = anchor_match.group(1) if anchor_match else \"0\"\n",
    "                                    law_id = law_id_match.group(1) if law_id_match else \"0\"\n",
    "                                    # law_title = a_tag.get_text()\n",
    "                                    temp_else[\"url\"] = href\n",
    "                                    temp_else[\"anchor\"] = anchor\n",
    "                                    temp_else[\"LawID\"] = law_id\n",
    "                                    temp_else[\"LawTitle\"] = lawtitle\n",
    "                                    temp_else[\"Dieu\"] = \"0\"\n",
    "                                    temp_else[\"Khoan\"] = \"0\"\n",
    "                                    temp_else[\"Diem\"] = \"0\"\n",
    "                                    temp_results.append(temp_else)\n",
    "                    skip_next_p_check = False\n",
    "        ccpls = []\n",
    "        ccpls.extend(temp_results)\n",
    "        \n",
    "        questions_and_answers1.append({\n",
    "            \"question\": second_part,\n",
    "            \"answer\": answer_text.strip(),\n",
    "            \"ccpls\": ccpls\n",
    "        })\n",
    "    patterns = [    \n",
    "        re.compile(r\"Chọn lĩnh vực để xem văn bản liên quan.*\", re.MULTILINE),\n",
    "        re.compile(r\">> Quý khách hàng xem thêm.*\", re.MULTILINE),\n",
    "        re.compile(r\"Quý khách hàng xem thêm.*\", re.MULTILINE),\n",
    "        re.compile(r\"Quý khách hành xem thêm.*\", re.MULTILINE),    \n",
    "        re.compile(r\"Quý khách hàng có thể.*\", re.MULTILINE),\n",
    "    ]\n",
    "\n",
    "    for item in questions_and_answers1:\n",
    "        answer_text = item[\"answer\"]\n",
    "        # Khởi tạo start_index với giá trị -1 (không tìm thấy)\n",
    "        start_index = -1\n",
    "        # Kiểm tra từng mẫu regex và cập nhật start_index nếu tìm thấy\n",
    "        for pattern in patterns:\n",
    "            match = pattern.search(answer_text)\n",
    "            if match:\n",
    "                index = match.start()\n",
    "                # Nếu start_index chưa được cập nhật hoặc tìm thấy index nhỏ hơn (gần đầu chuỗi hơn)\n",
    "                if start_index == -1 or index < start_index:\n",
    "                    start_index = index\n",
    "\n",
    "        # Nếu tìm thấy bất kỳ mẫu nào, xóa từ đoạn đó đến hết câu trả lời\n",
    "        if start_index != -1:\n",
    "            cleaned_answer = answer_text[:start_index].strip()\n",
    "            item[\"answer\"] = cleaned_answer\n",
    "            \n",
    "    # # In ra danh sách câu hỏi và câu trả lời đã được làm sạch\n",
    "    return json.dumps(questions_and_answers1, ensure_ascii=False, indent=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processing_5(soup):\n",
    "    h2_tags = []\n",
    "    for temp in soup.find_all('h2'):\n",
    "        # Kiểm tra nếu thẻ <h2> có chứa thẻ <strong> và không phải là thẻ trống hoặc chỉ chứa <br>\n",
    "        # if temp.find('strong') and temp.get_text(strip=True):\n",
    "        if temp.get_text(strip=True):\n",
    "            # and len(temp.get_text(strip=True)) >= 9:\n",
    "            h2_tags.append(temp)\n",
    "\n",
    "    questions_and_answers5 = []  # Danh sách để lưu trữ các link và anchor cho từng đoạn h2\n",
    "\n",
    "\n",
    "    for h2_tag in h2_tags:\n",
    "        temp_results = []\n",
    "        next_tag = h2_tag.find_next_sibling()\n",
    "        gay_flag = True\n",
    "        while next_tag and next_tag.name != 'h2':\n",
    "            if next_tag.name == 'p':\n",
    "                p_tag = next_tag.text\n",
    "                if (re.search(r\"thư\\s+viện\\s+pháp\\s+luật\", next_tag.text.lower())):\n",
    "                    gay_flag = False\n",
    "                if next_tag.find('a') and not next_tag.text.startswith('>>') and gay_flag == True:\n",
    "                    for a_tag in next_tag.find_all('a'):\n",
    "                        href = a_tag.get('href')\n",
    "                        if href and '.aspx' in href and 'thuvienphapluat.vn' in href:\n",
    "                            final = processing_ccpl_full(a_tag.text)\n",
    "                            positions = split_by_dieu(final)\n",
    "                            # test = p_tag.split(positions)\n",
    "                            # print(positions)\n",
    "                            if positions:\n",
    "                                lawtitle = positions[-1]\n",
    "                            else:\n",
    "                                lawtitle = \"default_value\"  # Hoặc một giá trị mặc định nếu positions rỗng\n",
    "\n",
    "                            # Kiểm tra nếu lawtitle không rỗng trước khi gọi split()\n",
    "                            if lawtitle:\n",
    "                                test = p_tag.split(lawtitle)\n",
    "                            else:\n",
    "                                # Xử lý khi lawtitle rỗng (nếu cần thiết)\n",
    "                                test = []\n",
    "                            # print(lawtitle)\n",
    "                            anchor_match = re.search(r'anchor=(\\w+)', href)\n",
    "                            anchor = anchor_match.group(1) if anchor_match else \"0\"\n",
    "                            if test and \"điều\" in test[0].lower():\n",
    "                                p_tag = p_tag.replace(test[0], '', 1)\n",
    "                                final = processing_ccpl_full(test[0])\n",
    "                                positions = split_by_dieu(final)\n",
    "                                data_ccpl = []\n",
    "                                for dieu in positions:\n",
    "                                        if \"điều\" in dieu.lower():\n",
    "                                            dieu_dict = {\"Dieu\": extract_dieu(dieu), \"Khoan\": []}\n",
    "                                            # Tìm anchor trong href\n",
    "                                            khoan_list = split_by_khoan(dieu)\n",
    "                                            for khoan in khoan_list:\n",
    "                                                if \"khoản\" in khoan.lower():\n",
    "                                                    khoan_dict = {\"Khoan\": extract_khoan(khoan), \"Diem\": []}\n",
    "                                                    if \"điểm\" in khoan.lower():\n",
    "                                                        diem_dict = extract_diem(khoan)\n",
    "                                                        khoan_dict[\"Diem\"].append(diem_dict)\n",
    "                                                    dieu_dict[\"Khoan\"].append(khoan_dict)\n",
    "                                            data_ccpl.append(dieu_dict)\n",
    "                                            # print(data_ccpl)\n",
    "                                            # Tạo một từ điển mới và gán giá trị từ mảng\n",
    "                                            ccpl_temp = processing_output_ccpl(data_ccpl)\n",
    "                                            if len(ccpl_temp) > 3:\n",
    "                                                for i in range(0, len(ccpl_temp), 3):\n",
    "                                                    # Lấy 3 phần tử từ chỉ mục i đến i+3 (không bị lỗi nếu kích thước không đủ)\n",
    "                                                    chunk = ccpl_temp[i:i+3]\n",
    "                                                    anchor_match = re.search(r'anchor=(\\w+)', href)\n",
    "                                                    law_id_match = re.search(r'-(\\d+)\\.aspx', href)\n",
    "                                                    anchor = anchor_match.group(1) if anchor_match else \"0\"\n",
    "                                                    law_id = law_id_match.group(1) if law_id_match else \"0\"\n",
    "                                                    # print(ccpl_temp)\n",
    "                                                    if len(chunk) > 0:\n",
    "                                                        dictionary = {\n",
    "                                                            \"url\": href,\n",
    "                                                            \"anchor\": anchor,\n",
    "                                                            \"LawID\": law_id,\n",
    "                                                            \"LawTitle\": lawtitle,\n",
    "                                                            \"Dieu\": chunk[0] or \"0\",\n",
    "                                                            \"Khoan\": chunk[1] or \"0\",\n",
    "                                                            \"Diem\": chunk[2] or \"0\"\n",
    "                                                        }\n",
    "                                                        temp_results.append(dictionary)\n",
    "                                            else:\n",
    "                                                anchor_match = re.search(r'anchor=(\\w+)', href)\n",
    "                                                law_id_match = re.search(r'-(\\d+)\\.aspx', href)\n",
    "                                                anchor = anchor_match.group(1) if anchor_match else \"0\"\n",
    "                                                law_id = law_id_match.group(1) if law_id_match else \"0\"\n",
    "                                                if len(ccpl_temp) > 0:\n",
    "                                                    dictionary = {\n",
    "                                                        \"url\": href,\n",
    "                                                        \"anchor\": anchor,\n",
    "                                                        \"LawID\": law_id,\n",
    "                                                        \"LawTitle\": lawtitle,\n",
    "                                                        \"Dieu\": ccpl_temp[0] or \"0\",\n",
    "                                                        \"Khoan\": ccpl_temp[1] or \"0\",\n",
    "                                                        \"Diem\": ccpl_temp[2] or \"0\"\n",
    "                                                    }\n",
    "                                                    temp_results.append(dictionary)\n",
    "\n",
    "                            else:\n",
    "                                temp_else = {}\n",
    "                                anchor_match = re.search(r'anchor=(\\w+)', href)\n",
    "                                law_id_match = re.search(r'-(\\d+)\\.aspx', href)\n",
    "                                anchor = anchor_match.group(1) if anchor_match else \"0\"\n",
    "                                law_id = law_id_match.group(1) if law_id_match else \"0\"\n",
    "                                # law_title = a_tag.get_text()\n",
    "                                temp_else[\"url\"] = href\n",
    "                                temp_else[\"anchor\"] = anchor\n",
    "                                temp_else[\"LawID\"] = law_id\n",
    "                                temp_else[\"LawTitle\"] = lawtitle\n",
    "                                temp_else[\"Dieu\"] = \"0\"\n",
    "                                temp_else[\"Khoan\"] = \"0\"\n",
    "                                temp_else[\"Diem\"] = \"0\"\n",
    "                                temp_results.append(temp_else)\n",
    "                    gay_flag = True\n",
    "            next_tag = next_tag.find_next_sibling()\n",
    "        ccpls = []\n",
    "        # ccpls.extend(temp_results)\n",
    "        ccpls.extend(temp_results)\n",
    "        questions_and_answers5.append({\n",
    "            \"ccpls\": ccpls\n",
    "        })\n",
    "    # In ra danh sách câu hỏi và câu trả lời đã được làm sạch\n",
    "    return json.dumps(questions_and_answers5, ensure_ascii=False, indent=5)\n",
    "# print(json.dumps(questions_and_answers4, ensure_ascii=False, indent=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hàm giải mã lawid\n",
    "def decrypt(s):\n",
    "    try:\n",
    "        tem = s\n",
    "        if len(tem) > 4:\n",
    "            tem = tem[-2:] + tem[2:-2] + tem[:2]\n",
    "        return base64.b64decode(base64.b64decode(tem).decode('utf-8')).decode('utf-8')\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "def decode_ccpls(questions_and_answers_final):\n",
    "    for temp in questions_and_answers_final:\n",
    "        for temp1 in temp.get('ccpls'):\n",
    "            if '.aspx?id=' in temp1.get('url'):\n",
    "                # Phân tích cú pháp URL\n",
    "                parsed_url = urlparse(temp1.get('url'))\n",
    "                # Lấy các tham số truy vấn từ URL\n",
    "                query_params = parse_qs(parsed_url.query)\n",
    "                # Lấy giá trị của tham số 'id'\n",
    "                id_value = query_params.get('id', [None])[0]\n",
    "                temp1['LawID'] = decrypt(id_value)\n",
    "    return questions_and_answers_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(data):\n",
    "    soup = processing_redundant_data(data)\n",
    "    file1_data = json.loads(processing_1(soup))\n",
    "    file5_data = json.loads(processing_5(soup))\n",
    "    \n",
    "    # ghép ccpl loại 5\n",
    "    for index, item in enumerate(file1_data):\n",
    "        if not item['ccpls']:\n",
    "            item['ccpls'] = file5_data[index]['ccpls']\n",
    "    \n",
    "    # Chỉ giữ lại các mục có câu trả lời\n",
    "    filtered_questions_and_answers = [qa for qa in file1_data if 'answer' in qa and qa['answer'].strip()]\n",
    "    filtered_questions_and_answers1 = [qa for qa in filtered_questions_and_answers if 'question' in qa and qa['question'].strip()]\n",
    "    \n",
    "    questions_and_answers_final = []\n",
    "    for qa in filtered_questions_and_answers1:\n",
    "        question_text = qa[\"question\"]\n",
    "        # Kiểm tra xem câu hỏi có chứa các từ \"dự thảo\", \"đề xuất\", \"sắp tới\" hoặc \"dự kiến\" không\n",
    "        if not re.search(r'\\b(?:dự thảo|đề xuất|sắp tới|dự kiến|đáp án|tra cứu điểm thi|sẽ ban hành)\\b', question_text.lower(), flags=re.IGNORECASE) and not question_text.lower().startswith(\"đã có\"):\n",
    "            questions_and_answers_final.append(qa)\n",
    "    # Remove duplicates\n",
    "    questions_and_answers_final = remove_duplicates_ccpls(questions_and_answers_final)\n",
    "    questions_and_answers_final  = decode_ccpls(questions_and_answers_final)\n",
    "    return questions_and_answers_final\n",
    "    # return json.dumps(questions_and_answers_final, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "function html request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import html\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def post_request(url, headers):\n",
    "    response = requests.post(url, headers=headers)\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "def post_request_key(url, headers, data):\n",
    "    try:\n",
    "        data = requests.post(url, headers=headers, data=json.dumps(data))\n",
    "        data.raise_for_status()  # Raise an error for bad status codes\n",
    "        return data.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sử dụng hàm để thực hiện yêu cầu POST\n",
    "def get_api():\n",
    "    url1 = 'https://apids.thuvienphapluat.vn/auth/get-token?key=pvaG4gRG9lIiwiaWF0IjoxNTE2MjM5MDIyfQS563xADXH'\n",
    "    headers1 = {\n",
    "        'Cookie': 'Culture=vi; Culture=vi'\n",
    "    }\n",
    "\n",
    "    key = post_request(url1, headers1)\n",
    "\n",
    "    url = 'https://apids.thuvienphapluat.vn/data/get-chinhsachphapluat'\n",
    "    headers = {\n",
    "        'Authorization': 'Bearer ' + key['Data']['AccessToken'],\n",
    "        'Content-Type': 'application/json',\n",
    "        'Cookie': 'Culture=vi'\n",
    "    }\n",
    "    return url, headers\n",
    "\n",
    "def send_data_to_api(data):\n",
    "    url, headers = get_api()\n",
    "    response = requests.post(url, headers=headers, json=data)\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing tag: 'NoneType' object has no attribute 'find'\n",
      "Error processing tag: 'NoneType' object has no attribute 'find'\n",
      "[{\"question\": \"Sáng ngày 19/9, áp thấp nhiệt đới đã mạnh lên thành cơn bão số 4 trong năm 2024 của Việt Nam, sau đây là diễn biến bão số 4 theo cập nhật mới nhất ngày 19/9.\", \"answer\": \"Mục\\r\\n                                    lục bài viết\\n\\nDiễn biến bão số 4 theo cập nhật mới nhất ngày 19/9\\nTheo tin bão khẩn cấp (cơn bão số 4) lúc 8h00 ngày 19/9 của Trung tâm Dự báo khí tượng thủy văn quốc gia.\\n\\n\\n\\n\\nTin bão khẩn cấp (cơn bão số 4)\\n\\n\\n\\nHồi 07 giờ ngày 19/9, vị trí tâm bão ở vào khoảng 17,5 độ Vĩ Bắc; 108,7 độ Kinh Đông, cách Quảng Bình-Quảng Trị khoảng 190km về phía Đông. Sức gió mạnh nhất vùng gần tâm bão mạnh cấp 8 (62-74km/h), giật cấp 10; di chuyển chủ yếu theo hướng Tây với tốc độ khoảng 20km/h.\\nTại Cồn Cỏ (Quảng Trị) đã có gió mạnh cấp 6, giật cấp 8; Hoành Sơn (Hà Tĩnh) gió mạnh cấp 6, giật cấp 8; Đồng Hới (Quảng Bình) có gió giật mạnh cấp 7.\\n Đêm qua đến sáng nay ở khu vực Bắc và Trung Trung Bộ đã có mưa vừa, mưa to, có nơi mưa rất to: Hòa Bắc (Đà Nẵng) 152mm; Bạch Mã (Thừa Thiên Huế) 270mm, Hương Phú (Thừa Thiên Huế) 249mm; Đrăkrông (Quảng Trị) 112 mm...\\n* Dự báo diễn biến bão (trong 24 giờ tới) như sau:\\n\\n\\n\\n\\nThời điểm dự báo\\n\\n\\nHướng, tốc độ\\n\\n\\nVị trí\\n\\n\\nCường độ\\n\\n\\nVùng nguy hiểm\\n\\n\\nCấp độ rủi ro thiên tai (Khu vực chịu ảnh hưởng)\\n\\n\\n\\n\\n13h/19/9\\n\\n\\nTây, khoảng 20-25km/h\\n\\n\\n17,3 N-107,4E; trên vùng biển ven bờ từ Quảng Quảng Bình đến Thừa Thiên Huế\\n\\n\\nCấp 8, giật cấp 10\\n\\n\\n15,0N-19,5N; phía Tây kinh tuyến 112.0E\\n\\n\\nCấp 3: Vùng biển phía Tây của khu vực Bắc Biển Đông (bao gồm khu vực quần đảo Hoàng Sa); vùng biển từ Nghệ An đến Quảng Ngãi (bao gồm đảo Lý Sơn, Cù Lao Chàm, Cồn Cỏ, Hòn Ngư); khu vực đất liền ven biển từ Hà Tĩnh đến Quảng Nam\\n\\n\\n\\n\\n19h/19/9\\n\\n\\nTây, 20km/h\\n\\n\\n17,5 N-106,3E; trên đất liền khu vực từ Quảng Bình đến Quảng Trị\\n\\n\\nCấp 6, giật cấp 10\\n\\n\\n15,5N-19,5N; phía Tây kinh tuyến 110.0E\\n\\n\\nCấp 3: vùng biển từ Nghệ An đến Quảng Nam (bao gồm đảo  Cù Lao Chàm, Cồn Cỏ, Hòn Ngư); khu vực đất liền từ Hà Tĩnh đến Quảng Nam\\n\\n\\n\\n\\n07h/20/9\\n\\n\\nTây Tây Bắc, khoảng 15-20km/h, suy yếu dần thành vùng áp thấp\\n\\n\\n18,0 N-104,4E; trên khu vực trung Lào\\n\\n\\n< Cấp 6\\n\\n\\n15,0N-19,5N; phía Tây kinh tuyến 110.0E\\n\\n\\nCấp 3: vùng biển từ Nghệ An đến Đà Nẵng(bao gồm đảo Cồn Cỏ, Hòn Ngư); khu vực đất liền từ Hà Tĩnh đến Đà Nẵng\\n\\n\\n\\n\\n\\n* Dự báo tác động của bão số 4\\nTheo Trung tâm Dự báo khí tượng thủy văn quốc gia, trong sáng ngày 19/9, vùng biển phía Tây khu vực Bắc Biển Đông (bao gồm khu vực quần đảo Hoàng Sa) có gió mạnh cấp 6-7 (39-61km/h), giật cấp 9 (75-88km/h), sóng biển cao 2,0-4,0m, biển động mạnh.\\nVùng biển từ Nghệ An đến Quảng Ngãi (bao gồm cả huyện đảo Lý Sơn, Cù Lao Chàm, Cồn Cỏ, Hòn Ngư) có gió mạnh cấp 6-7 (39-61km/h), sóng biển cao 2,0-4,0m, vùng gần tâm bão đi qua cấp 8 (62-74km/h), giật cấp 10 (89-102km/h), sóng biển cao 3,0-5,0m, biển động mạnh.\\nVen biển các tỉnh từ Hà Tĩnh tới Đà Nẵng cần đề phòng nước dâng do bão cao từ 0,3-0,5m, kết hợp với triều cao và sóng lớn gây sạt lở đê, kè biển, ngập úng tại khu vực trũng, thấp vào chiều tối ngày 19/9.\\nTàu thuyền hoạt động trong các vùng nguy hiểm nói trên đều có khả năng chịu tác động của dông, lốc, gió mạnh, sóng lớn, triều cao và nước dâng do bão.\\nĐối với trên đất liền, từ sáng ngày 19/9, vùng đất liền ven biển từ Hà Tĩnh đến Quảng Nam có gió mạnh dần lên cấp 6-7, vùng gần tâm bão đi qua cấp 8 (62-74km/h), giật cấp 10 (89-102km/h); sâu trong đất liền có gió giật cấp 6-7.\\n Từ ngày 19/9 đến ngày 20/9, ở khu vực Bắc Trung Bộ và Trung Trung Bộ có mưa to đến rất to với lượng mưa phổ biến từ 100-300mm, cục bộ có nơi trên 500mm. Đề phòng mưa cường suất lớn (>150mm/6 giờ) ở khu vực từ Quảng Trị-Đà Nẵng trong ngày 19/9.\\n- Ngày 19/9, Tây Nguyên và Nam Bộ có mưa vừa, mưa to và dông, cục bộ có nơi mưa rất to với lượng mưa phổ biến từ 20-40mm, có nơi trên 70mm.\\n- Mưa lớn dẫn đến tình trạng ngập lụt cho các khu vực đô thị, nơi tập trung đông dân cư do nước không kịp thoát.\\nCông điện của Thủ tướng về chủ động ứng phó với bão số 4\\nTheo đó, tại  Công điện 97/CĐ-TTg năm 2024  để chủ động ứng phó với áp thấp nhiệt đới có khả năng mạnh lên thành bão, đặc biệt là nguy cơ mưa lớn, ngập lụt, sạt lở đất, lũ ống, lũ quét có thể xảy ra, Thủ tướng Chính phủ yêu cầu:\\n- Bộ trưởng Bộ Tài nguyên và Môi trường chỉ đạo cơ quan dự báo khí tượng thủy văn tiếp tục tổ chức theo dõi chặt chẽ, dự báo, cung cấp thông tin đầy đủ, kịp thời về diễn biến của áp thấp nhiệt đới cho cơ quan chức năng và người dân biết để chủ động triển khai công tác ứng phó theo quy định.\\n- Bộ trưởng các Bộ, Chủ tịch Ủy ban nhân dân các tỉnh, thành phố nêu trên theo chức năng, nhiệm vụ được giao tổ chức theo dõi, cập nhật thường xuyên, liên tục thông tin diễn biến áp thấp nhiệt đới, bão, mưa lũ để chủ động chỉ đạo triển khai công tác ứng phó phù hợp với diễn biến thiên tai có thể ảnh hưởng đến phạm vi quản lý của ngành, địa phương, trong đó:\\n+ Tập trung triển khai ngay các biện pháp bảo đảm an toàn cho tàu thuyền, phương tiện và các hoạt động trên biển, ven biển.\\n+ Rà soát, hoàn thiện kịch bản ứng phó với áp thấp nhiệt đới, bão, ngập lụt, sạt lở đất, lũ ống, lũ quét, tập trung bảo đảm an toàn tính mạng hạn chế thiệt hại về tài sản của Nhân dân, vận hành khoa học, an toàn hồ đập thủy điện, thủy lợi.\\n+ Chủ động bố trí lực lượng, vật tư, phương tiện, nhất là tại các địa phương dự kiến chịu ảnh hưởng trực tiếp của bão, mưa lũ, địa bàn trọng điểm để sẵn sàng triển khai ứng phó áp thấp nhiệt đới, bão, mưa lũ, cứu hộ, cứu nạn khi có yêu cầu.\\n- Đài Truyền hình Việt Nam, Đài Tiếng nói Việt Nam và các cơ quan truyền thông tăng thời lượng phát sóng, đưa tin để người dân nắm được thông tin về diễn biến áp thấp nhiệt đới, bão, mưa lũ, chỉ đạo của các cơ quan có thẩm quyền, biết được kỹ năng ứng phó khi xảy ra thiên tai, nhất là sạt lở đất, lũ quét, ngập lụt nhằm hạn chế thiệt hại.\\n- Bộ trưởng Bộ Nông nghiệp và Phát triển nông thôn tổ chức trực ban 24/7 theo dõi chặt chẽ tình hình, chủ động chỉ đạo, đôn đốc các địa phương triển khai công tác ứng phó phù hợp với diễn biến thiên tai thực tế, kịp thời báo cáo, đề xuất Thủ tướng Chính phủ, Phó Thủ tướng chỉ đạo những vấn đề vượt thẩm quyền.\\n- Văn phòng Chính phủ theo dõi, đôn đốc các Bộ, địa phương thực hiện nghiêm túc Công điện này; kịp thời báo cáo Thủ tướng Chính phủ, Phó Thủ tướng phụ trách những vấn đề đột xuất, phát sinh.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\r\\n                                    Nội dung nêu trên là phần giải đáp, tư vấn của chúng tôi dành cho khách hàng của\\r\\n                                    THƯ VIỆN PHÁP LUẬT.\\r\\n                                    Nếu quý khách còn vướng mắc, vui lòng gửi về Email info@thuvienphapluat.vn.\\n\\n\\nBão số 4 năm 2024,\\n\\n\\n\\nHỎI ĐÁP PHÁP LUẬT LIÊN QUAN\\n\\n\\n.relation-article .hd h3, .relation-article .hd h3 a {\\r\\n    display: inline;\\r\\n    text-transform: uppercase;\\r\\n    font-weight: bold;\\r\\n    font-size: 100%;\\r\\n}\\r\\n.ulnhch {margin:0;padding:0;}\\r\\n.ulnhch li {list-style-type:none;padding:5px 0;} \\r\\n.link-othernews {color: #337ab7;}\", \"ccpls\": []}, {\"question\": \"Diễn biến bão số 4 theo cập nhật mới nhất ngày 19/9\", \"answer\": \"Theo tin bão khẩn cấp (cơn bão số 4) lúc 8h00 ngày 19/9 của Trung tâm Dự báo khí tượng thủy văn quốc gia.\\n\\n\\n\\n\\n[Tin bão khẩn cấp (cơn bão số 4)](https://cdn.thuvienphapluat.vn/uploads/tintuc/2024/09/19/TIN%20B%C3%83O%20KH%E1%BA%A8N%20C%E1%BA%A4P%20(B%C3%83O%20S%E1%BB%90%204).docx)\\n\\nTin bão khẩn cấp (cơn bão số 4)\\n\\n\\n\\n\\nHồi 07 giờ ngày 19/9, vị trí tâm bão ở vào khoảng 17,5 độ Vĩ Bắc; 108,7 độ Kinh Đông, cách Quảng Bình-Quảng Trị khoảng 190km về phía Đông. Sức gió mạnh nhất vùng gần tâm bão mạnh cấp 8 (62-74km/h), giật cấp 10; di chuyển chủ yếu theo hướng Tây với tốc độ khoảng 20km/h.\\n\\nTại Cồn Cỏ (Quảng Trị) đã có gió mạnh cấp 6, giật cấp 8; Hoành Sơn (Hà Tĩnh) gió mạnh cấp 6, giật cấp 8; Đồng Hới (Quảng Bình) có gió giật mạnh cấp 7.\\n\\nĐêm qua đến sáng nay ở khu vực Bắc và Trung Trung Bộ đã có mưa vừa, mưa to, có nơi mưa rất to: Hòa Bắc (Đà Nẵng) 152mm; Bạch Mã (Thừa Thiên Huế) 270mm, Hương Phú (Thừa Thiên Huế) 249mm; Đrăkrông (Quảng Trị) 112 mm...\\n\\n* Dự báo diễn biến bão (trong 24 giờ tới) như sau:\\n\\n| Thời điểm dự báo   | Hướng, tốc độ                                                 | Vị trí                                                                      | Cường độ           | Vùng nguy hiểm                          | Cấp độ rủi ro thiên tai (Khu vực chịu ảnh hưởng)                                                                                                                                                                                    |\\n|:-------------------|:--------------------------------------------------------------|:----------------------------------------------------------------------------|:-------------------|:----------------------------------------|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\\n| 13h/19/9           | Tây, khoảng 20-25km/h                                         | 17,3 N-107,4E; trên vùng biển ven bờ từ Quảng Quảng Bình đến Thừa Thiên Huế | Cấp 8, giật cấp 10 | 15,0N-19,5N; phía Tây kinh tuyến 112.0E | Cấp 3: Vùng biển phía Tây của khu vực Bắc Biển Đông (bao gồm khu vực quần đảo Hoàng Sa); vùng biển từ Nghệ An đến Quảng Ngãi (bao gồm đảo Lý Sơn, Cù Lao Chàm, Cồn Cỏ, Hòn Ngư); khu vực đất liền ven biển từ Hà Tĩnh đến Quảng Nam |\\n| 19h/19/9           | Tây, 20km/h                                                   | 17,5 N-106,3E;trên đất liền khu vực từ Quảng Bình đến Quảng Trị             | Cấp 6, giật cấp 10 | 15,5N-19,5N; phía Tây kinh tuyến 110.0E | Cấp 3: vùng biển từ Nghệ An đến Quảng Nam (bao gồm đảo  Cù Lao Chàm, Cồn Cỏ, Hòn Ngư); khu vực đất liền từ Hà Tĩnh đến Quảng Nam                                                                                                    |\\n| 07h/20/9           | Tây Tây Bắc, khoảng 15-20km/h, suy yếu dần thành vùng áp thấp | 18,0 N-104,4E; trên khu vực trung Lào                                       | < Cấp 6            | 15,0N-19,5N; phía Tây kinh tuyến 110.0E | Cấp 3: vùng biển từ Nghệ An đến Đà Nẵng(bao gồm đảo Cồn Cỏ, Hòn Ngư); khu vực đất liền từ Hà Tĩnh đến Đà Nẵng                                                                                                                       |\\n\\n\\n* Dự báo tác động của bão số 4\\n\\nTheo Trung tâm Dự báo khí tượng thủy văn quốc gia, trong sáng ngày 19/9, vùng biển phía Tây khu vực Bắc Biển Đông (bao gồm khu vực quần đảo Hoàng Sa) có gió mạnh cấp 6-7 (39-61km/h), giật cấp 9 (75-88km/h), sóng biển cao 2,0-4,0m, biển động mạnh.\\n\\nVùng biển từ Nghệ An đến Quảng Ngãi (bao gồm cả huyện đảo Lý Sơn, Cù Lao Chàm, Cồn Cỏ, Hòn Ngư) có gió mạnh cấp 6-7 (39-61km/h), sóng biển cao 2,0-4,0m, vùng gần tâm bão đi qua cấp 8 (62-74km/h), giật cấp 10 (89-102km/h), sóng biển cao 3,0-5,0m, biển động mạnh.\\n\\nVen biển các tỉnh từ Hà Tĩnh tới Đà Nẵng cần đề phòng nước dâng do bão cao từ 0,3-0,5m, kết hợp với triều cao và sóng lớn gây sạt lở đê, kè biển, ngập úng tại khu vực trũng, thấp vào chiều tối ngày 19/9.\\n\\nTàu thuyền hoạt động trong các vùng nguy hiểm nói trên đều có khả năng chịu tác động của dông, lốc, gió mạnh, sóng lớn, triều cao và nước dâng do bão.\\n\\nĐối với trên đất liền, từ sáng ngày 19/9, vùng đất liền ven biển từ Hà Tĩnh đến Quảng Nam có gió mạnh dần lên cấp 6-7, vùng gần tâm bão đi qua cấp 8 (62-74km/h), giật cấp 10 (89-102km/h); sâu trong đất liền có gió giật cấp 6-7.\\n\\nTừ ngày 19/9 đến ngày 20/9, ở khu vực Bắc Trung Bộ và Trung Trung Bộ có mưa to đến rất to với lượng mưa phổ biến từ 100-300mm, cục bộ có nơi trên 500mm. Đề phòng mưa cường suất lớn (>150mm/6 giờ) ở khu vực từ Quảng Trị-Đà Nẵng trong ngày 19/9.\\n\\n- Ngày 19/9, Tây Nguyên và Nam Bộ có mưa vừa, mưa to và dông, cục bộ có nơi mưa rất to với lượng mưa phổ biến từ 20-40mm, có nơi trên 70mm.\\n\\n- Mưa lớn dẫn đến tình trạng ngập lụt cho các khu vực đô thị, nơi tập trung đông dân cư do nước không kịp thoát.\", \"ccpls\": []}, {\"question\": \"Công điện của Thủ tướng về chủ động ứng phó với bão số 4\", \"answer\": \"Theo đó, tại  Công điện 97/CĐ-TTg năm 2024  để chủ động ứng phó với áp thấp nhiệt đới có khả năng mạnh lên thành bão, đặc biệt là nguy cơ mưa lớn, ngập lụt, sạt lở đất, lũ ống, lũ quét có thể xảy ra, Thủ tướng Chính phủ yêu cầu:\\n\\n- Bộ trưởng Bộ Tài nguyên và Môi trường chỉ đạo cơ quan dự báo khí tượng thủy văn tiếp tục tổ chức theo dõi chặt chẽ, dự báo, cung cấp thông tin đầy đủ, kịp thời về diễn biến của áp thấp nhiệt đới cho cơ quan chức năng và người dân biết để chủ động triển khai công tác ứng phó theo quy định.\\n\\n- Bộ trưởng các Bộ, Chủ tịch Ủy ban nhân dân các tỉnh, thành phố nêu trên theo chức năng, nhiệm vụ được giao tổ chức theo dõi, cập nhật thường xuyên, liên tục thông tin diễn biến áp thấp nhiệt đới, bão, mưa lũ để chủ động chỉ đạo triển khai công tác ứng phó phù hợp với diễn biến thiên tai có thể ảnh hưởng đến phạm vi quản lý của ngành, địa phương, trong đó:\\n\\n+ Tập trung triển khai ngay các biện pháp bảo đảm an toàn cho tàu thuyền, phương tiện và các hoạt động trên biển, ven biển.\\n\\n+ Rà soát, hoàn thiện kịch bản ứng phó với áp thấp nhiệt đới, bão, ngập lụt, sạt lở đất, lũ ống, lũ quét, tập trung bảo đảm an toàn tính mạng hạn chế thiệt hại về tài sản của Nhân dân, vận hành khoa học, an toàn hồ đập thủy điện, thủy lợi.\\n\\n+ Chủ động bố trí lực lượng, vật tư, phương tiện, nhất là tại các địa phương dự kiến chịu ảnh hưởng trực tiếp của bão, mưa lũ, địa bàn trọng điểm để sẵn sàng triển khai ứng phó áp thấp nhiệt đới, bão, mưa lũ, cứu hộ, cứu nạn khi có yêu cầu.\\n\\n- Đài Truyền hình Việt Nam, Đài Tiếng nói Việt Nam và các cơ quan truyền thông tăng thời lượng phát sóng, đưa tin để người dân nắm được thông tin về diễn biến áp thấp nhiệt đới, bão, mưa lũ, chỉ đạo của các cơ quan có thẩm quyền, biết được kỹ năng ứng phó khi xảy ra thiên tai, nhất là sạt lở đất, lũ quét, ngập lụt nhằm hạn chế thiệt hại.\\n\\n- Bộ trưởng Bộ Nông nghiệp và Phát triển nông thôn tổ chức trực ban 24/7 theo dõi chặt chẽ tình hình, chủ động chỉ đạo, đôn đốc các địa phương triển khai công tác ứng phó phù hợp với diễn biến thiên tai thực tế, kịp thời báo cáo, đề xuất Thủ tướng Chính phủ, Phó Thủ tướng chỉ đạo những vấn đề vượt thẩm quyền.\\n\\n- Văn phòng Chính phủ theo dõi, đôn đốc các Bộ, địa phương thực hiện nghiêm túc Công điện này; kịp thời báo cáo Thủ tướng Chính phủ, Phó Thủ tướng phụ trách những vấn đề đột xuất, phát sinh.\", \"ccpls\": [{\"url\": \"https://thuvienphapluat.vn/van-ban/Tai-nguyen-Moi-truong/Cong-dien-97-CD-TTg-2024-chu-dong-ung-pho-ap-thap-nhiet-doi-co-kha-nang-manh-len-thanh-bao-624689.aspx\", \"anchor\": \"0\", \"LawID\": \"624689\", \"LawTitle\": \" Công điện 97/CĐ-TTg năm 2024\", \"Dieu\": \"0\", \"Khoan\": \"0\", \"Diem\": \"0\"}]}]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "url = 'https://thuvienphapluat.vn/chinh-sach-phap-luat-moi/vn/ho-tro-phap-luat/tu-van-phap-luat/71597/dien-bien-bao-so-4-theo-cap-nhat-moi-nhat-ngay-19-9'\n",
    "# Gửi yêu cầu HTTP để lấy nội dung của trang web\n",
    "response = requests.get(url)\n",
    "\n",
    "# Kiểm tra xem yêu cầu có thành công không (mã trạng thái 200 là thành công)\n",
    "if response.status_code == 200:\n",
    "    # Lấy nội dung của trang web\n",
    "    html_content = response.text\n",
    "    # print(html_content)\n",
    "    print(json.dumps(extract_data(html_content), ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = json.dumps(extract_data(html_content), ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# import json\n",
    "# file_note = []\n",
    "# data_type1 = []\n",
    "# data_type7 = []\n",
    "# data_type3 = []\n",
    "# data_check = []\n",
    "# url = \"https://apids.thuvienphapluat.vn/data/get-by-ids\"\n",
    "\n",
    "# payload = json.dumps({\n",
    "#   \"source\": \"CSPLM\",\n",
    "#   \"ids\": [\n",
    "#     41460\n",
    "#   ]\n",
    "# })\n",
    "# headers = {\n",
    "#   'authorization': 'Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiJRdeG6o24gbMO9IHThuqV0IGPhuqMgcXV54buBbiIsImp0aSI6IjU4ODk1N2I4LTA5ZDgtNDk5Ny1hMWQ3LTcwYTUzMzViZTdmZCIsImh0dHA6Ly9zY2hlbWFzLm1pY3Jvc29mdC5jb20vd3MvMjAwOC8wNi9pZGVudGl0eS9jbGFpbXMvcm9sZSI6IkFkbWluaXN0cmF0b3IiLCJuYmYiOjE3MjY4ODkzNjAsImV4cCI6MTcyNjg5MDI2MCwiaXNzIjoiVFZQTCMjJCRhcG9zM29aaWFNU2sxMjNrMVNES1BST0JBU0lDRlJFRTMyMUxPTkdEQUlDQSIsImF1ZCI6IlRWUEwjIyQkYXBvczNvWmlhTVNrMTIzazFTREtQUk9CQVNJQ0ZSRUUzMjFMT05HREFJQ0EifQ.-QPqQT8D2-qOibM6fyY70e8sYpK8dh1OI-P_BV2DgV0',\n",
    "#   'Content-Type': 'application/json',\n",
    "#   'Cookie': 'Culture=vi; Culture=vi; Culture=vi; Culture=vi'\n",
    "# }\n",
    "\n",
    "# response = requests.request(\"POST\", url, headers=headers, data=payload)\n",
    "\n",
    "# print(response.text)\n",
    "# data = response.text\n",
    "# response = json.loads(data)\n",
    "# data_input = response.get('data')\n",
    "# for temp in data_input:\n",
    "#     url1 = 'https://apids.thuvienphapluat.vn/auth/get-token?key=pvaG4gRG9lIiwiaWF0IjoxNTE2MjM5MDIyfQS563xADXH'\n",
    "#     headers1 = {\n",
    "#         'Cookie': 'Culture=vi; Culture=vi'\n",
    "#     }\n",
    "\n",
    "#     key = post_request(url1, headers1)\n",
    "#     url_post = \"https://apids.thuvienphapluat.vn/crud/log-extracted-news-data\"\n",
    "\n",
    "#     headers_post = {\n",
    "#     'Authorization': 'Bearer ' + key['Data']['AccessToken'],\n",
    "#     'Content-Type': 'application/json',\n",
    "#     'Cookie': 'Culture=vi'\n",
    "#     }\n",
    "#     # print(temp)\n",
    "#     # print(temp[\"content\"])\n",
    "#     # print(temp['title'])\n",
    "#     extracted_data = extract_data(temp[\"content\"])\n",
    "#     # print(extract_data(temp[\"content\"]))\n",
    "#     # ccpls_flags = True\n",
    "#     if extracted_data != [] and not re.search(r'\\b(?:dự thảo|đề xuất|sắp tới|dự kiến|đáp án|tra cứu điểm thi)\\b', temp[\"title\"].lower(), flags=re.IGNORECASE):\n",
    "#         # for ccpl_check in extracted_data:\n",
    "#         #     if not ccpl_check.get('ccpls'):\n",
    "#         #         ccpls_flags = False\n",
    "#         #         break\n",
    "#         # if not ccpls_flags:    \n",
    "#         #     dictionary = {\n",
    "#         #         \"objid\": int(temp['obj_id']),\n",
    "#         #         \"source\": str(temp['obj_code']),\n",
    "#         #         \"data\": json.dumps(extracted_data, ensure_ascii=False),\n",
    "#         #             # extracted_data,\n",
    "#         #         \"type\": 7\n",
    "#         #     }\n",
    "#         #     dictionary = json.dumps(dictionary, ensure_ascii=False)\n",
    "#         #     data_type7.append(int(temp['obj_id']))\n",
    "#         #     response = requests.request(\"POST\", url_post, headers=headers_post, data=dictionary)\n",
    "#         #     print(response.text)\n",
    "#         # else:\n",
    "#         #     # for temp_extract_data in extracted_data:\n",
    "#         #     #     if not temp_extract_data.get('ccpls'):\n",
    "#         #     #         temp_extract_data['ccpls'] = result_ccpl(temp_extract_data.get('answer'), get_items())\n",
    "#         dictionary = {\n",
    "#             \"objid\": int(temp['obj_id']),\n",
    "#             \"source\": str(temp['obj_code']),\n",
    "#             \"data\": json.dumps(extracted_data, ensure_ascii=False),\n",
    "#                 # extracted_data,\n",
    "#             \"type\": 1\n",
    "#         }\n",
    "#         dictionary = json.dumps(dictionary, ensure_ascii=False)\n",
    "#         data_type1.append(int(temp['obj_id']))\n",
    "#         data_check.append(dictionary)\n",
    "#         response = requests.request(\"POST\", url_post, headers=headers_post, data=dictionary)\n",
    "#         print(response.text)\n",
    "#         print(dictionary)\n",
    "#     elif extracted_data != [] and re.search(r'\\b(?:dự thảo|đề xuất|sắp tới|dự kiến|đáp án|tra cứu điểm thi|sẽ ban hành)\\b', temp[\"title\"].lower(), flags=re.IGNORECASE):\n",
    "#         dictionary = {\n",
    "#             \"objid\": int(temp['obj_id']),\n",
    "#             \"source\": str(temp['obj_code']),\n",
    "#             \"data\": json.dumps(extracted_data, ensure_ascii=False),\n",
    "#             \"type\": 3\n",
    "#         }\n",
    "#         dictionary = json.dumps(dictionary, ensure_ascii=False)\n",
    "#         data_type3.append(int(temp['obj_id']))\n",
    "#         response = requests.request(\"POST\", url_post, headers=headers_post, data=dictionary)\n",
    "#         print(response.text)\n",
    "#     else:\n",
    "#         file_note.append(int(temp['obj_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "def check_old_data(obj_id, source):\n",
    "    # URL của API\n",
    "    url = f\"https://aiadmin.thuvienphapluat.vn/duyet-mau/Home/GetItem?ObjId={obj_id}&Source={source}\"\n",
    "\n",
    "    # Thực hiện request GET\n",
    "    response = requests.get(url)\n",
    "    # Kiểm tra nếu request thành công (mã trạng thái HTTP 200)\n",
    "    if response.status_code == 200:\n",
    "        # Lấy dữ liệu từ API (thường là dạng JSON)\n",
    "        data_temp = response.json()\n",
    "        b = (json.dumps(data_temp, ensure_ascii=False))\n",
    "        return b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54300\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "Error processing tag: 'NoneType' object has no attribute 'find'\n",
      "Error processing tag: 'NoneType' object has no attribute 'find'\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "Error processing tag: 'NoneType' object has no attribute 'find'\n",
      "Error processing tag: 'NoneType' object has no attribute 'find'\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "Error processing tag: 'NoneType' object has no attribute 'find'\n",
      "{\"messages\":\"success\"}\n",
      "Error processing tag: 'NoneType' object has no attribute 'find'\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "Error processing tag: 'NoneType' object has no attribute 'find'\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "Error processing tag: 'NoneType' object has no attribute 'find'\n",
      "{\"messages\":\"success\"}\n",
      "Error processing tag: 'NoneType' object has no attribute 'find'\n",
      "{\"messages\":\"success\"}\n",
      "Error processing tag: 'NoneType' object has no attribute 'find'\n",
      "{\"messages\":\"success\"}\n",
      "Error processing tag: 'NoneType' object has no attribute 'find'\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "Error processing tag: 'NoneType' object has no attribute 'find'\n",
      "Error processing tag: 'NoneType' object has no attribute 'find'\n",
      "Error processing tag: 'NoneType' object has no attribute 'find'\n",
      "{\"messages\":\"success\"}\n",
      "Error processing tag: 'NoneType' object has no attribute 'find'\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "Error processing tag: 'NoneType' object has no attribute 'find'\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "Error processing tag: 'NoneType' object has no attribute 'find'\n",
      "{\"messages\":\"success\"}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data_api = {\n",
    "    \"page\": 1,\n",
    "    \"num\": 1\n",
    "}\n",
    "file_note = []\n",
    "data_type1 = []\n",
    "data_type7 = []\n",
    "data_type3 = []\n",
    "data_check = []\n",
    "url, headers = get_api()\n",
    "response = post_request_key(url, headers, data_api)\n",
    "\n",
    "if response:\n",
    "    specific_page = response.get('total')\n",
    "    print(specific_page)\n",
    "now = (54238//100)+1\n",
    "num = 100\n",
    "total_page = (specific_page) // num + ( 1 if specific_page % num > 0 else 0)\n",
    "# for num_page in range(total_page, 0, -1):\n",
    "for num_page in range(now, total_page + 1):\n",
    "    data_api = {\n",
    "        \"page\": num_page, #14039\n",
    "        \"num\": num\n",
    "    }\n",
    "    url1 = 'https://apids.thuvienphapluat.vn/auth/get-token?key=pvaG4gRG9lIiwiaWF0IjoxNTE2MjM5MDIyfQS563xADXH'\n",
    "    headers1 = {\n",
    "        'Cookie': 'Culture=vi; Culture=vi'\n",
    "    }\n",
    "\n",
    "    key = post_request(url1, headers1)\n",
    "    url_post = \"https://apids.thuvienphapluat.vn/crud/log-extracted-news-data\"\n",
    "\n",
    "    headers_post = {\n",
    "    'Authorization': 'Bearer ' + key['Data']['AccessToken'],\n",
    "    'Content-Type': 'application/json',\n",
    "    'Cookie': 'Culture=vi'\n",
    "    }\n",
    "\n",
    "    url, headers = get_api()\n",
    "    response = post_request_key(url, headers, data_api)\n",
    "    # print(response.get('data'))\n",
    "    data_input = response.get('data')\n",
    "    for temp in data_input:\n",
    "        # print(temp)\n",
    "        # print(temp[\"content\"])\n",
    "        # print(temp['title'])\n",
    "        extracted_data = extract_data(temp[\"content\"])\n",
    "        # print(extract_data(temp[\"content\"]))\n",
    "        # ccpls_flags = True\n",
    "        dictionary = {\n",
    "            \"objid\": int(temp['obj_id']),\n",
    "            \"source\": str(temp['obj_code']),\n",
    "            \"data\": json.dumps(extracted_data, ensure_ascii=False),\n",
    "            # data,\n",
    "        }\n",
    "        a = (json.dumps(dictionary, ensure_ascii=False))\n",
    "        b = check_old_data(int(temp['obj_id']), str(temp['obj_code']))\n",
    "        if not(a == b):\n",
    "            if extracted_data != [] and not re.search(r'\\b(?:dự thảo|đề xuất|sắp tới|dự kiến|đáp án|tra cứu điểm thi)\\b', temp[\"title\"].lower(), flags=re.IGNORECASE):\n",
    "                # for ccpl_check in extracted_data:\n",
    "                #     if not ccpl_check.get('ccpls'):\n",
    "                #         ccpls_flags = False\n",
    "                #         break\n",
    "                # if not ccpls_flags:    \n",
    "                #     dictionary = {\n",
    "                #         \"objid\": int(temp['obj_id']),\n",
    "                #         \"source\": str(temp['obj_code']),\n",
    "                #         \"data\": json.dumps(extracted_data, ensure_ascii=False),\n",
    "                #             # extracted_data,\n",
    "                #         \"type\": 7\n",
    "                #     }\n",
    "                #     dictionary = json.dumps(dictionary, ensure_ascii=False)\n",
    "                #     data_type7.append(int(temp['obj_id']))\n",
    "                #     response = requests.request(\"POST\", url_post, headers=headers_post, data=dictionary)\n",
    "                #     print(response.text)\n",
    "                # else:\n",
    "                #     # for temp_extract_data in extracted_data:\n",
    "                #     #     if not temp_extract_data.get('ccpls'):\n",
    "                #     #         temp_extract_data['ccpls'] = result_ccpl(temp_extract_data.get('answer'), get_items())\n",
    "                dictionary = {\n",
    "                    \"objid\": int(temp['obj_id']),\n",
    "                    \"source\": str(temp['obj_code']),\n",
    "                    \"data\": json.dumps(extracted_data, ensure_ascii=False),\n",
    "                        # extracted_data,\n",
    "                    \"type\": 1\n",
    "                }\n",
    "                dictionary = json.dumps(dictionary, ensure_ascii=False)\n",
    "                data_type1.append(int(temp['obj_id']))\n",
    "                data_check.append(dictionary)\n",
    "                response = requests.request(\"POST\", url_post, headers=headers_post, data=dictionary)\n",
    "                print(response.text)\n",
    "            elif extracted_data != [] and re.search(r'\\b(?:dự thảo|đề xuất|sắp tới|dự kiến|đáp án|tra cứu điểm thi|sẽ ban hành)\\b', temp[\"title\"].lower(), flags=re.IGNORECASE):\n",
    "                dictionary = {\n",
    "                    \"objid\": int(temp['obj_id']),\n",
    "                    \"source\": str(temp['obj_code']),\n",
    "                    \"data\": json.dumps(extracted_data, ensure_ascii=False),\n",
    "                    \"type\": 3\n",
    "                }\n",
    "                dictionary = json.dumps(dictionary, ensure_ascii=False)\n",
    "                data_type3.append(int(temp['obj_id']))\n",
    "                response = requests.request(\"POST\", url_post, headers=headers_post, data=dictionary)\n",
    "                print(response.text)\n",
    "            else:\n",
    "                file_note.append(int(temp['obj_id']))\n",
    "            # print(dictionary)\n",
    "            # time.sleep(1)\n",
    "now = specific_page\n",
    "with open('CSPLM_type1.txt', 'w') as file:\n",
    "    for item in data_type1:\n",
    "        file.write(f\"{item}\\n\")\n",
    "with open('CSPLM_type3.txt', 'w') as file:\n",
    "    for item in data_type3:\n",
    "        file.write(f\"{item}\\n\")\n",
    "with open('CSPLM_type7.txt', 'w') as file:\n",
    "    for item in data_type7:\n",
    "        file.write(f\"{item}\\n\")\n",
    "with open('CSPLM_file_note.txt', 'w') as file:\n",
    "    for item in file_note:\n",
    "        file.write(f\"{item}\\n\")\n",
    "with open('CSPLM_data_check.txt', 'w') as file:\n",
    "    for item in data_check:\n",
    "        file.write(f\"{item}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('CSPLM_type1.txt', 'w') as file:\n",
    "    for item in data_type1:\n",
    "        file.write(f\"{item}\\n\")\n",
    "with open('CSPLM_type3.txt', 'w') as file:\n",
    "    for item in data_type3:\n",
    "        file.write(f\"{item}\\n\")\n",
    "with open('CSPLM_type7.txt', 'w') as file:\n",
    "    for item in data_type7:\n",
    "        file.write(f\"{item}\\n\")\n",
    "with open('CSPLM_file_note.txt', 'w') as file:\n",
    "    for item in file_note:\n",
    "        file.write(f\"{item}\\n\")\n",
    "with open('CSPLM_data_check.txt', 'w') as file:\n",
    "    for item in data_check:\n",
    "        file.write(f\"{item}\\n\")       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('haizz.txt', 'r', encoding='utf-8') as file:\n",
    "#     content = file.read()\n",
    "#     items = content.split('\\n')\n",
    "#     # Chuyển tất cả phần tử về chữ thường và lọc những phần tử rỗng\n",
    "#     items = [int(item.lower()) for item in items if item.strip()]\n",
    "# items = list(set(items))\n",
    "\n",
    "# def split_list(items, chunk_size=100):\n",
    "#     return [items[i:i + chunk_size] for i in range(0, len(items), chunk_size)]\n",
    "# chunks = split_list(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# import json\n",
    "# for temp_chunk in chunks:\n",
    "#     url1 = 'https://apids.thuvienphapluat.vn/auth/get-token?key=pvaG4gRG9lIiwiaWF0IjoxNTE2MjM5MDIyfQS563xADXH'\n",
    "#     headers1 = {\n",
    "#         'Cookie': 'Culture=vi; Culture=vi'\n",
    "#     }\n",
    "\n",
    "#     key = post_request(url1, headers1)\n",
    "\n",
    "#     url = \"https://apids.thuvienphapluat.vn/data/get-by-ids\"\n",
    "\n",
    "#     payload = json.dumps({\n",
    "#     \"source\": \"PL\",\n",
    "#     \"ids\": temp_chunk\n",
    "#     })\n",
    "#     headers = {\n",
    "#     'authorization': 'Bearer ' + key['Data']['AccessToken'],\n",
    "#     'Content-Type': 'application/json',\n",
    "#     'Cookie': 'Culture=vi; Culture=vi; Culture=vi; Culture=vi'\n",
    "#     }\n",
    "\n",
    "#     response = requests.request(\"POST\", url, headers=headers, data=payload)\n",
    "#     data = (response.text)\n",
    "#     data_dict = json.loads(data)\n",
    "#     # value = data_dict.get('data')\n",
    "#     file_note = []\n",
    "#     data_type1 = []\n",
    "#     data_type7 = []\n",
    "#     data_type3 = []\n",
    "#     url1 = 'https://apids.thuvienphapluat.vn/auth/get-token?key=pvaG4gRG9lIiwiaWF0IjoxNTE2MjM5MDIyfQS563xADXH'\n",
    "#     headers1 = {\n",
    "#         'Cookie': 'Culture=vi; Culture=vi'\n",
    "#     }\n",
    "\n",
    "#     key = post_request(url1, headers1)\n",
    "#     url_post = \"https://apids.thuvienphapluat.vn/crud/log-extracted-news-data\"\n",
    "\n",
    "#     headers_post = {\n",
    "#     'Authorization': 'Bearer ' + key['Data']['AccessToken'],\n",
    "#     'Content-Type': 'application/json',\n",
    "#     'Cookie': 'Culture=vi'\n",
    "#     }\n",
    "\n",
    "#     data_input = data_dict.get('data')\n",
    "#     for temp in data_input:\n",
    "#         # print(temp)\n",
    "#         # print(temp[\"content\"])\n",
    "#         # print(temp['title'])\n",
    "#         extracted_data = extract_data(temp[\"content\"])\n",
    "#         # print(extracted_data)\n",
    "#         # print(extract_data(temp[\"content\"]))\n",
    "#         ccpls_flags = True\n",
    "#         if extracted_data != [] and not re.search(r'\\b(?:dự thảo|đề xuất|sắp tới|dự kiến|đáp án|tra cứu điểm thi)\\b', temp[\"title\"].lower(), flags=re.IGNORECASE):\n",
    "#             # for ccpl_check in extracted_data:\n",
    "#             #     if not ccpl_check.get('ccpls'):\n",
    "#             #         ccpls_flags = False\n",
    "#             #         break\n",
    "#             # if not ccpls_flags:    \n",
    "#             #     dictionary = {\n",
    "#             #         \"objid\": int(temp['obj_id']),\n",
    "#             #         \"source\": str(temp['obj_code']),\n",
    "#             #         \"data\": json.dumps(extracted_data, ensure_ascii=False),\n",
    "#             #             # extracted_data,\n",
    "#             #         \"type\": 7\n",
    "#             #     }\n",
    "#             #     dictionary = json.dumps(dictionary, ensure_ascii=False)\n",
    "#             #     data_type7.append(int(temp['obj_id']))\n",
    "#             #     response = requests.request(\"POST\", url_post, headers=headers_post, data=dictionary)\n",
    "#             #     print(response.text)\n",
    "#             # else:\n",
    "#             #     # for temp_extract_data in extracted_data:\n",
    "#             #     #     if not temp_extract_data.get('ccpls'):\n",
    "#             #     #         temp_extract_data['ccpls'] = result_ccpl(temp_extract_data.get('answer'), get_items())\n",
    "#                 dictionary = {\n",
    "#                     \"objid\": int(temp['obj_id']),\n",
    "#                     \"source\": str(temp['obj_code']),\n",
    "#                     \"data\": json.dumps(extracted_data, ensure_ascii=False),\n",
    "#                         # extracted_data,\n",
    "#                     \"type\": 1\n",
    "#                 }\n",
    "#                 dictionary = json.dumps(dictionary, ensure_ascii=False)\n",
    "#                 data_type1.append(int(temp['obj_id']))\n",
    "#                 # print(dictionary)\n",
    "#                 response = requests.request(\"POST\", url_post, headers=headers_post, data=dictionary)\n",
    "#                 print(response.text)\n",
    "#         elif extracted_data != [] and re.search(r'\\b(?:dự thảo|đề xuất|sắp tới|dự kiến|đáp án|tra cứu điểm thi)\\b', temp[\"title\"].lower(), flags=re.IGNORECASE):\n",
    "#             dictionary = {\n",
    "#                 \"objid\": int(temp['obj_id']),\n",
    "#                 \"source\": str(temp['obj_code']),\n",
    "#                 \"data\": json.dumps(extracted_data, ensure_ascii=False),\n",
    "#                 \"type\": 3\n",
    "#             }\n",
    "#             dictionary = json.dumps(dictionary, ensure_ascii=False)\n",
    "#             data_type3.append(int(temp['obj_id']))\n",
    "#             # print(dictionary)\n",
    "#             response = requests.request(\"POST\", url_post, headers=headers_post, data=dictionary)\n",
    "#             print(response.text)\n",
    "#         else:\n",
    "#             file_note.append(int(temp['obj_id']))\n",
    "#             # print('hehe')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Minh-AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
